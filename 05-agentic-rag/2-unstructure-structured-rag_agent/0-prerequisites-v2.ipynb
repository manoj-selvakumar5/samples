{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Structured RAG using Amazon Bedrock Knowledge Bases: End-to-End Example using Amazon Redshift\n",
        "\n",
        "Structured RAG allows Amazon Bedrock Knowledge Bases customers to query structured data in Redshift using natural language, and receive natural language responses summarizing the data thereby providing an answer to the user question.\n",
        "\n",
        "Using advanced natural language processing, Amazon Bedrock Knowledge Bases can transform natural language queries into SQL queries, allowing users to retrieve data directly from the source without the need to move or preprocess the data. To generate accurate SQL queries, Bedrock Knowledge Base leverages database schema, previous query history, and other contextual information that are provided about the data sources. For more details, please see the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-structured.html).\n",
        "\n",
        "This notebook provides sample code for building a Structured RAG using Amazon Bedrock Knowledge Bases with Redshift.\n",
        "\n",
        "## Steps:\n",
        "1. Create Knowledge Base execution role with necessary policies for accessing data from Amazon Redshift\n",
        "2. Create a knowledge base with Structured database (Redshift database)\n",
        "3. Create data source(s) within knowledge base\n",
        "4. Start ingestion jobs using KB APIs which will read metadata about structured database\n",
        "5. Once the metadata is extracted and ingested, then user can interact with Structured databases via Amazon Bedrock Knowledge Base APIs using Natural language query\n",
        "\n",
        "## Prerequisites\n",
        "This notebook requires:\n",
        "- A Redshift serverless cluster with a workgroup [OR] Redshift provisioned cluster\n",
        "- Your workgroup or cluster is already setup with your structured data ingested\n",
        "- You've set-up the IAM Role [OR] Secrets manager with User Credentials [OR] the DB User\n",
        "\n",
        "To read more details about prerequisites, see the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-structured.html).\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 0 - Setup\n",
        "\n",
        "Before running the rest of this notebook, you'll need to run the cells below to ensure necessary libraries are installed and connect to Bedrock.\n",
        "\n",
        "Please ignore any pip dependency error (if you see any while installing libraries)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Structured RAG using Amazon Bedrock Knowledge Bases: End-to-End Example using Amazon Redshift\n",
        "\n",
        "Structured RAG allows Amazon Bedrock Knowledge Bases customers to query structured data in Redshift using natural language, and receive natural language responses summarizing the data thereby providing an answer to the user question.\n",
        "\n",
        "Using advanced natural language processing, Amazon Bedrock Knowledge Bases can transform natural language queries into SQL queries, allowing users to retrieve data directly from the source without the need to move or preprocess the data. To generate accurate SQL queries, Bedrock Knowledge Base leverages database schema, previous query history, and other contextual information that are provided about the data sources. For more details, please see the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-structured.html).\n",
        "\n",
        "This notebook provides sample code for building a Structured RAG using Amazon Bedrock Knowledge Bases with Redshift.\n",
        "\n",
        "## Steps:\n",
        "1. Create Knowledge Base execution role with necessary policies for accessing data from Amazon Redshift\n",
        "2. Create a knowledge base with Structured database (Redshift database)\n",
        "3. Create data source(s) within knowledge base\n",
        "4. Start ingestion jobs using KB APIs which will read metadata about structured database\n",
        "5. Once the metadata is extracted and ingested, then user can interact with Structured databases via Amazon Bedrock Knowledge Base APIs using Natural language query\n",
        "\n",
        "## Prerequisites\n",
        "This notebook requires:\n",
        "- A Redshift serverless cluster with a workgroup [OR] Redshift provisioned cluster\n",
        "- Your workgroup or cluster is already setup with your structured data ingested\n",
        "- You've set-up the IAM Role [OR] Secrets manager with User Credentials [OR] the DB User\n",
        "\n",
        "To read more details about prerequisites, see the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-structured.html).\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 0 - Setup\n",
        "\n",
        "Before running the rest of this notebook, you'll need to run the cells below to ensure necessary libraries are installed and connect to Bedrock.\n",
        "\n",
        "Please ignore any pip dependency error (if you see any while installing libraries)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.38.36\n"
          ]
        }
      ],
      "source": [
        "# %pip install --upgrade pip --quiet\n",
        "# %pip install -r ../requirements.txt --no-deps --quiet\n",
        "# %pip install -r ../requirements.txt --upgrade --quiet\n",
        "# %pip install --upgrade boto3\n",
        "import boto3\n",
        "print(boto3.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<script>Jupyter.notebook.kernel.restart()</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# restart kernel\n",
        "from IPython.core.display import HTML\n",
        "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "This code is part of the setup and used to:\n",
        "- Add the parent directory to the python system path\n",
        "- Imports a custom module (BedrockStructuredKnowledgeBase) from utils necessary for later executions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/Users/manojs/Documents/Code/samples/05-agentic-rag/2-unstructure-structured-rag_agent', '/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python312.zip', '/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12', '/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/lib-dynload', '', '/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages', '/Users/manojs/Documents/Code/samples/05-agentic-rag']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "current_path = Path().resolve()\n",
        "current_path = current_path.parent\n",
        "\n",
        "if str(current_path) not in sys.path:\n",
        "    sys.path.append(str(current_path))\n",
        "\n",
        "# Print sys.path to verify\n",
        "print(sys.path)\n",
        "\n",
        "from utils.structured_knowledge_base import BedrockStructuredKnowledgeBase\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup and initialize boto3 clients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('us-west-2', '533267284022')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "s3_client = boto3.client('s3')\n",
        "sts_client = boto3.client('sts')\n",
        "session = boto3.session.Session(region_name='us-west-2')\n",
        "region = session.region_name\n",
        "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
        "bedrock_agent_client = boto3.client('bedrock-agent')\n",
        "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime')\n",
        "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "region, account_id\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "Initialize and configure the knowledge base name and the foundational model. This foundational model will be used to generate the natural language response based on the records received from the structured data store.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Get the current timestamp\n",
        "current_time = time.time()\n",
        "\n",
        "# Format the timestamp as a string\n",
        "timestamp_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(current_time))[-7:]\n",
        "# Create the suffix using the timestamp\n",
        "suffix = f\"{timestamp_str}\"\n",
        "\n",
        "knowledge_base_name = f\"bedrock-sample-structured-kb-{suffix}\"\n",
        "knowledge_base_description = \"Sample Structured KB\"\n",
        "\n",
        "foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "Amazon Bedrock Knowledge Bases uses a service role to connect knowledge bases to structured data stores, retrieve data from these data stores, and generate SQL queries based on user queries and the structure of the data stores. \n",
        "\n",
        "This notebook demonstrates the **IAM Role + Redshift Serverless WorkGroup** access pattern, which provides secure access to your Redshift Serverless workgroup using IAM authentication.\n",
        "\n",
        "**Required Configuration Variables:**\n",
        "- workgroup_id: Your Redshift Serverless workgroup identifier\n",
        "- redshiftDBName: Your database name within the workgroup\n",
        "\n",
        "The knowledge base configuration will use these parameters to establish the connection and perform necessary authentication using IAM roles.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 1 - Set up Redshift Serverless Infrastructure\n",
        "\n",
        "This section will create the necessary Redshift Serverless components: namespace and workgroup. This infrastructure will host our structured data that the Knowledge Base will query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Redshift Namespace: sds-ecommerce-0205647\n",
            "Redshift Workgroup: sds-ecommerce-wg-0205647\n",
            "Database: sds-ecommerce\n",
            "S3 Bucket: sds-ecommerce-redshift-0205647\n"
          ]
        }
      ],
      "source": [
        "# Configuration for Redshift resources\n",
        "REDSHIFT_NAMESPACE = f'sds-ecommerce-{suffix}'\n",
        "REDSHIFT_WORKGROUP = f'sds-ecommerce-wg-{suffix}'\n",
        "REDSHIFT_DATABASE = f'sds-ecommerce'\n",
        "S3_BUCKET = f'sds-ecommerce-redshift-{suffix}'\n",
        "\n",
        "print(f\"Redshift Namespace: {REDSHIFT_NAMESPACE}\")\n",
        "print(f\"Redshift Workgroup: {REDSHIFT_WORKGROUP}\")\n",
        "print(f\"Database: {REDSHIFT_DATABASE}\")\n",
        "print(f\"S3 Bucket: {S3_BUCKET}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created role RedshiftS3AccessRole-0205647\n",
            "Redshift IAM Role ARN: arn:aws:iam::533267284022:role/RedshiftS3AccessRole-0205647\n",
            "Role RedshiftS3AccessRole-0205647 already exists\n",
            "Redshift IAM Role ARN: arn:aws:iam::533267284022:role/RedshiftS3AccessRole-0205647\n"
          ]
        }
      ],
      "source": [
        "def create_iam_role_for_redshift():\n",
        "    \"\"\"Create IAM role for Redshift to access S3\"\"\"\n",
        "    try:\n",
        "        # Get account ID\n",
        "        account_id = sts_client.get_caller_identity()['Account']\n",
        "        \n",
        "        # Create IAM role if it doesn't exist\n",
        "        role_name = f'RedshiftS3AccessRole-{suffix}'\n",
        "        try:\n",
        "            role_response = iam_client.get_role(RoleName=role_name)\n",
        "            print(f'Role {role_name} already exists')\n",
        "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
        "        except iam_client.exceptions.NoSuchEntityException:\n",
        "            trust_policy = {\n",
        "                \"Version\": \"2012-10-17\",\n",
        "                \"Statement\": [\n",
        "                    {\n",
        "                        \"Effect\": \"Allow\",\n",
        "                        \"Principal\": {\n",
        "                            \"Service\": \"redshift.amazonaws.com\"\n",
        "                        },\n",
        "                        \"Action\": \"sts:AssumeRole\"\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "            \n",
        "            iam_client.create_role(\n",
        "                RoleName=role_name,\n",
        "                AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
        "            )\n",
        "            \n",
        "            # Attach necessary policies\n",
        "            iam_client.attach_role_policy(\n",
        "                RoleName=role_name,\n",
        "                PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n",
        "            )\n",
        "            \n",
        "            print(f'Created role {role_name}')\n",
        "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f'Error creating IAM role: {str(e)}')\n",
        "        raise\n",
        "\n",
        "# Initialize additional clients\n",
        "import json\n",
        "import os\n",
        "iam_client = boto3.client('iam')\n",
        "redshift_client = boto3.client('redshift-serverless', region_name=region)\n",
        "redshift_data_client = boto3.client('redshift-data', region_name=region)\n",
        "\n",
        "redshift_role_arn = create_iam_role_for_redshift()\n",
        "print(f\"Redshift IAM Role ARN: {redshift_role_arn}\")\n",
        "\n",
        "redshift_role_arn = create_iam_role_for_redshift()\n",
        "print(f\"Redshift IAM Role ARN: {redshift_role_arn}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating namespace sds-ecommerce-0205647...\n",
            "Created namespace sds-ecommerce-0205647\n",
            "Waiting for namespace to be available...\n",
            "Namespace sds-ecommerce-0205647 is now available\n"
          ]
        }
      ],
      "source": [
        "def create_redshift_namespace():\n",
        "    \"\"\"Create Redshift Serverless namespace\"\"\"\n",
        "    try:\n",
        "        # Check if namespace already exists\n",
        "        try:\n",
        "            response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
        "            print(f'Namespace {REDSHIFT_NAMESPACE} already exists')\n",
        "            return response['namespace']\n",
        "        except redshift_client.exceptions.ResourceNotFoundException:\n",
        "            print(f'Creating namespace {REDSHIFT_NAMESPACE}...')\n",
        "        \n",
        "        # Create the namespace\n",
        "        response = redshift_client.create_namespace(\n",
        "            namespaceName=REDSHIFT_NAMESPACE,\n",
        "            adminUsername='admin',\n",
        "            adminUserPassword='TempPassword123!',  # Change this in production\n",
        "            dbName=REDSHIFT_DATABASE,\n",
        "            defaultIamRoleArn=redshift_role_arn,\n",
        "            iamRoles=[redshift_role_arn]\n",
        "        )\n",
        "        \n",
        "        print(f'Created namespace {REDSHIFT_NAMESPACE}')\n",
        "        \n",
        "        # Wait for namespace to be available\n",
        "        print('Waiting for namespace to be available...')\n",
        "        max_attempts = 30\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                namespace_response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
        "                status = namespace_response['namespace']['status']\n",
        "                if status == 'AVAILABLE':\n",
        "                    print(f'Namespace {REDSHIFT_NAMESPACE} is now available')\n",
        "                    return namespace_response['namespace']\n",
        "                else:\n",
        "                    print(f'Namespace status: {status}, waiting...')\n",
        "                    time.sleep(10)\n",
        "            except Exception as e:\n",
        "                print(f'Error checking namespace status: {str(e)}, retrying...')\n",
        "                time.sleep(10)\n",
        "        \n",
        "        print('Timeout waiting for namespace, but proceeding...')\n",
        "        return response['namespace']\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f'Error creating namespace: {str(e)}')\n",
        "        raise\n",
        "\n",
        "# Create namespace\n",
        "namespace = create_redshift_namespace()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating workgroup sds-ecommerce-wg-0205647...\n",
            "Created workgroup sds-ecommerce-wg-0205647\n",
            "Waiting for workgroup to be available...\n",
            "Workgroup status: CREATING, waiting...\n",
            "Workgroup status: CREATING, waiting...\n",
            "Workgroup status: CREATING, waiting...\n",
            "Workgroup status: CREATING, waiting...\n",
            "Workgroup sds-ecommerce-wg-0205647 is now available\n",
            "Workgroup ARN: arn:aws:redshift-serverless:us-west-2:533267284022:workgroup/a68f79bf-ace3-46d7-abc3-3ee32e4998aa\n"
          ]
        }
      ],
      "source": [
        "def create_redshift_workgroup():\n",
        "    \"\"\"Create Redshift Serverless workgroup\"\"\"\n",
        "    try:\n",
        "        # Check if workgroup already exists\n",
        "        try:\n",
        "            response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
        "            print(f'Workgroup {REDSHIFT_WORKGROUP} already exists')\n",
        "            return response['workgroup']\n",
        "        except redshift_client.exceptions.ResourceNotFoundException:\n",
        "            print(f'Creating workgroup {REDSHIFT_WORKGROUP}...')\n",
        "        \n",
        "        # Create the workgroup\n",
        "        response = redshift_client.create_workgroup(\n",
        "            workgroupName=REDSHIFT_WORKGROUP,\n",
        "            namespaceName=REDSHIFT_NAMESPACE,\n",
        "            baseCapacity=8,  # Minimum base capacity\n",
        "            enhancedVpcRouting=False,\n",
        "            publiclyAccessible=True,\n",
        "            configParameters=[\n",
        "                {\n",
        "                    'parameterKey': 'enable_user_activity_logging',\n",
        "                    'parameterValue': 'true'\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        print(f'Created workgroup {REDSHIFT_WORKGROUP}')\n",
        "        \n",
        "        # Wait for workgroup to be available\n",
        "        print('Waiting for workgroup to be available...')\n",
        "        max_attempts = 30\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                workgroup_response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
        "                status = workgroup_response['workgroup']['status']\n",
        "                if status == 'AVAILABLE':\n",
        "                    print(f'Workgroup {REDSHIFT_WORKGROUP} is now available')\n",
        "                    return workgroup_response['workgroup']\n",
        "                else:\n",
        "                    print(f'Workgroup status: {status}, waiting...')\n",
        "                    time.sleep(10)\n",
        "            except Exception as e:\n",
        "                print(f'Error checking workgroup status: {str(e)}, retrying...')\n",
        "                time.sleep(10)\n",
        "        \n",
        "        print('Timeout waiting for workgroup, but proceeding...')\n",
        "        return response['workgroup']\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f'Error creating workgroup: {str(e)}')\n",
        "        raise\n",
        "\n",
        "# Create workgroup\n",
        "workgroup = create_redshift_workgroup()\n",
        "workgroup_arn = workgroup['workgroupArn']\n",
        "print(f\"Workgroup ARN: {workgroup_arn}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 - Create S3 Bucket and Load Sample Data\n",
        "\n",
        "We will create an S3 bucket to stage our sample e-commerce data before loading it into Redshift tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created bucket sds-ecommerce-redshift-0205647\n"
          ]
        }
      ],
      "source": [
        "def create_s3_bucket():\n",
        "    \"\"\"Create S3 bucket for data staging\"\"\"\n",
        "    try:\n",
        "        s3_client.head_bucket(Bucket=S3_BUCKET)\n",
        "        print(f'Bucket {S3_BUCKET} already exists')\n",
        "    except:\n",
        "        try:\n",
        "            if region == 'us-east-1':\n",
        "                s3_client.create_bucket(Bucket=S3_BUCKET)\n",
        "            else:\n",
        "                s3_client.create_bucket(\n",
        "                    Bucket=S3_BUCKET,\n",
        "                    CreateBucketConfiguration={'LocationConstraint': region}\n",
        "                )\n",
        "            print(f'Created bucket {S3_BUCKET}')\n",
        "        except Exception as e:\n",
        "            print(f'Error creating bucket: {str(e)}')\n",
        "            raise\n",
        "\n",
        "# Create S3 bucket\n",
        "create_s3_bucket()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading sample data files to S3...\n",
            "Uploaded orders.csv (1.8 MB) to S3\n",
            "Uploaded order_items.csv (1.3 MB) to S3\n",
            "Uploaded payments.csv (0.8 MB) to S3\n",
            "Uploaded reviews.csv (0.5 MB) to S3\n",
            "Successfully uploaded all 4 data files to S3\n"
          ]
        }
      ],
      "source": [
        "def upload_sample_data():\n",
        "    \"\"\"Upload sample CSV files to S3\"\"\"\n",
        "    data_files = ['orders.csv', 'order_items.csv', 'payments.csv', 'reviews.csv']\n",
        "    sds_directory = 'sample_data'\n",
        "    \n",
        "    print(\"Uploading sample data files to S3...\")\n",
        "    files_found = 0\n",
        "    \n",
        "    for file_name in data_files:\n",
        "        local_path = os.path.join(sds_directory, file_name)\n",
        "        if os.path.exists(local_path):\n",
        "            # Get file size for informational purposes\n",
        "            file_size = os.path.getsize(local_path)\n",
        "            file_size_mb = file_size / (1024 * 1024)\n",
        "            \n",
        "            s3_client.upload_file(local_path, S3_BUCKET, file_name)\n",
        "            print(f'Uploaded {file_name} ({file_size_mb:.1f} MB) to S3')\n",
        "            files_found += 1\n",
        "        else:\n",
        "            print(f'Warning: {local_path} not found')\n",
        "    \n",
        "    if files_found == len(data_files):\n",
        "        print(f\"Successfully uploaded all {files_found} data files to S3\")\n",
        "    else:\n",
        "        print(f\"Only {files_found} out of {len(data_files)} files were found and uploaded\")\n",
        "\n",
        "# Upload sample data\n",
        "upload_sample_data()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 3 - Create Redshift Tables and Load Data\n",
        "\n",
        "Now we will create the database tables in Redshift and load our sample e-commerce data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Executing SQL commands to grant database access to Bedrock execution role...\n",
            "Creating user: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "Executing statement: cb83ac9a-aa81-41ad-8bde-91ceb92cfc0c\n",
            "Statement status: SUBMITTED, waiting...\n",
            "Statement status: STARTED, waiting...\n",
            "Statement status: STARTED, waiting...\n",
            "Error executing statement: Statement failed: ERROR: user \"IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\" already exists\n",
            "‚ÑπÔ∏è User already exists, continuing...\n",
            "Granting USAGE on schema public to: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "Executing statement: c948acbc-0201-440d-b2e1-fee3521a65ac\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "‚úÖ USAGE permission granted successfully!\n",
            "Granting SELECT on all tables to: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "Executing statement: 6c413ca2-d25d-41bb-ae99-d28f3506ee19\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "‚úÖ SELECT permissions granted successfully!\n",
            "\\nüéâ All database permissions have been granted to the Bedrock execution role!\n",
            "The Knowledge Base should now be able to access the database tables.\n"
          ]
        }
      ],
      "source": [
        "# Execute the required SQL commands to grant database access to Bedrock execution role\n",
        "print(\"üîß Executing SQL commands to grant database access to Bedrock execution role...\")\n",
        "\n",
        "bedrock_role_name = knowledge_base.bedrock_kb_execution_role_name\n",
        "\n",
        "# Step 1: Create the IAM user in Redshift database\n",
        "create_user_sql = f'CREATE USER \"IAMR:{bedrock_role_name}\" WITH PASSWORD DISABLE;'\n",
        "\n",
        "try:\n",
        "    print(f\"Creating user: IAMR:{bedrock_role_name}\")\n",
        "    run_redshift_statement(create_user_sql)\n",
        "    print(\"‚úÖ IAM user created successfully!\")\n",
        "except Exception as e:\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        print(\"‚ÑπÔ∏è User already exists, continuing...\")\n",
        "    else:\n",
        "        print(f\"‚ùå Error creating user: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Step 2: Grant USAGE permission on public schema\n",
        "grant_usage_sql = f'GRANT USAGE ON SCHEMA public TO \"IAMR:{bedrock_role_name}\";'\n",
        "\n",
        "try:\n",
        "    print(f\"Granting USAGE on schema public to: IAMR:{bedrock_role_name}\")\n",
        "    run_redshift_statement(grant_usage_sql)\n",
        "    print(\"‚úÖ USAGE permission granted successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error granting USAGE permission: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Step 3: Grant SELECT permission on all tables in public schema\n",
        "grant_select_sql = f'GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"IAMR:{bedrock_role_name}\";'\n",
        "\n",
        "try:\n",
        "    print(f\"Granting SELECT on all tables to: IAMR:{bedrock_role_name}\")\n",
        "    run_redshift_statement(grant_select_sql)\n",
        "    print(\"‚úÖ SELECT permissions granted successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error granting SELECT permissions: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "print(\"\\\\nüéâ All database permissions have been granted to the Bedrock execution role!\")\n",
        "print(\"The Knowledge Base should now be able to access the database tables.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Retrying the ingestion job with proper permissions...\n",
            "job  started successfully\n",
            "\n",
            "{ 'dataSourceId': 'EOZFBUZD6F',\n",
            "  'ingestionJobId': 'YBQXYQU4WL',\n",
            "  'knowledgeBaseId': 'WCNCTKFCKY',\n",
            "  'startedAt': datetime.datetime(2025, 6, 21, 7, 12, 41, 597571, tzinfo=tzutc()),\n",
            "  'status': 'FAILED',\n",
            "  'updatedAt': datetime.datetime(2025, 6, 21, 7, 12, 44, 860633, tzinfo=tzutc())}\n",
            "‚úÖ New ingestion job started successfully!\n",
            "Ingestion Job Response: None\n",
            "‚è≥ Waiting for ingestion job to complete...\n",
            "‚ùå Error starting ingestion job: 'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[25], line 16\u001b[0m\n",
            "\u001b[1;32m     13\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m30\u001b[39m)\n",
            "\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Check the ingestion job status\u001b[39;00m\n",
            "\u001b[0;32m---> 16\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mingestionJobId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
            "\u001b[1;32m     17\u001b[0m job_status \u001b[38;5;241m=\u001b[39m bedrock_agent_client\u001b[38;5;241m.\u001b[39mget_ingestion_job(\n",
            "\u001b[1;32m     18\u001b[0m     knowledgeBaseId\u001b[38;5;241m=\u001b[39mkb_id,\n",
            "\u001b[1;32m     19\u001b[0m     dataSourceId\u001b[38;5;241m=\u001b[39mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataSourceId\u001b[39m\u001b[38;5;124m'\u001b[39m],\n",
            "\u001b[1;32m     20\u001b[0m     ingestionJobId\u001b[38;5;241m=\u001b[39mjob_id\n",
            "\u001b[1;32m     21\u001b[0m )\n",
            "\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnüìä Ingestion Job Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_status[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mingestionJob\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# Retry the ingestion job now that permissions are fixed\n",
        "print(\"üîÑ Retrying the ingestion job with proper permissions...\")\n",
        "\n",
        "try:\n",
        "    # Start a new ingestion job\n",
        "    response = knowledge_base.start_ingestion_job()\n",
        "    print(\"‚úÖ New ingestion job started successfully!\")\n",
        "    print(f\"Ingestion Job Response: {response}\")\n",
        "    \n",
        "    # Wait a bit for the job to process\n",
        "    import time\n",
        "    print(\"‚è≥ Waiting for ingestion job to complete...\")\n",
        "    time.sleep(30)\n",
        "    \n",
        "    # Check the ingestion job status\n",
        "    job_id = response['ingestionJobId']\n",
        "    job_status = bedrock_agent_client.get_ingestion_job(\n",
        "        knowledgeBaseId=kb_id,\n",
        "        dataSourceId=response['dataSourceId'],\n",
        "        ingestionJobId=job_id\n",
        "    )\n",
        "    \n",
        "    print(f\"\\\\nüìä Ingestion Job Status: {job_status['ingestionJob']['status']}\")\n",
        "    if job_status['ingestionJob']['status'] == 'COMPLETE':\n",
        "        print(\"üéâ Ingestion job completed successfully!\")\n",
        "    elif job_status['ingestionJob']['status'] == 'IN_PROGRESS':\n",
        "        print(\"‚è≥ Ingestion job is still in progress. Wait a few more minutes.\")\n",
        "    else:\n",
        "        print(f\"‚ùå Ingestion job status: {job_status['ingestionJob']['status']}\")\n",
        "        if 'failureReasons' in job_status['ingestionJob']:\n",
        "            print(f\"Failure reasons: {job_status['ingestionJob']['failureReasons']}\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error starting ingestion job: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to check ingestion job status\n",
        "def check_ingestion_status(kb_id, data_source_id=None, job_id=None):\n",
        "    \"\"\"Check the status of ingestion jobs for a knowledge base\"\"\"\n",
        "    try:\n",
        "        # If no data_source_id provided, get the first one\n",
        "        if not data_source_id:\n",
        "            data_sources = bedrock_agent_client.list_data_sources(\n",
        "                knowledgeBaseId=kb_id,\n",
        "                maxResults=10\n",
        "            )\n",
        "            if data_sources['dataSourceSummaries']:\n",
        "                data_source_id = data_sources['dataSourceSummaries'][0]['dataSourceId']\n",
        "            else:\n",
        "                print(\"‚ùå No data sources found for this knowledge base\")\n",
        "                return\n",
        "        \n",
        "        # List ingestion jobs\n",
        "        ingestion_jobs = bedrock_agent_client.list_ingestion_jobs(\n",
        "            knowledgeBaseId=kb_id,\n",
        "            dataSourceId=data_source_id,\n",
        "            maxResults=5\n",
        "        )\n",
        "        \n",
        "        print(f\"üìã Recent ingestion jobs for Knowledge Base {kb_id}:\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        for i, job in enumerate(ingestion_jobs['ingestionJobSummaries'][:3]):\n",
        "            status_emoji = {\n",
        "                'COMPLETE': '‚úÖ',\n",
        "                'IN_PROGRESS': '‚è≥', \n",
        "                'FAILED': '‚ùå',\n",
        "                'STARTING': 'üîÑ'\n",
        "            }.get(job['status'], '‚ùì')\n",
        "            \n",
        "            print(f\"{i+1}. Job ID: {job['ingestionJobId']}\")\n",
        "            print(f\"   Status: {status_emoji} {job['status']}\")\n",
        "            print(f\"   Started: {job['startedAt']}\")\n",
        "            print(f\"   Updated: {job['updatedAt']}\")\n",
        "            print()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error checking ingestion status: {str(e)}\")\n",
        "\n",
        "# Check current ingestion status\n",
        "print(\"üîç Checking current ingestion job status...\")\n",
        "check_ingestion_status(kb_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Checking and fixing IAM service role permissions...\n",
            "Service Role: AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "Service Role ARN: arn:aws:iam::533267284022:role/AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "‚úÖ Created enhanced policy: EnhancedBedrockRedshiftPolicy-0205647\n",
            "‚úÖ Attached enhanced policy to role: AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "\\nüîÑ Waiting 30 seconds for IAM changes to propagate...\n"
          ]
        }
      ],
      "source": [
        "# Fix the IAM service role permissions and retry ingestion\n",
        "print(\"üîß Checking and fixing IAM service role permissions...\")\n",
        "\n",
        "# Get the current service role\n",
        "bedrock_role_name = knowledge_base.bedrock_kb_execution_role_name\n",
        "role_arn = f\"arn:aws:iam::{account_id}:role/{bedrock_role_name}\"\n",
        "\n",
        "print(f\"Service Role: {bedrock_role_name}\")\n",
        "print(f\"Service Role ARN: {role_arn}\")\n",
        "\n",
        "# Create a comprehensive policy that matches AWS documentation requirements\n",
        "enhanced_policy_document = {\n",
        "    \"Version\": \"2012-10-17\",\n",
        "    \"Statement\": [\n",
        "        {\n",
        "            \"Sid\": \"RedshiftDataAPIStatementPermissions\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"redshift-data:GetStatementResult\",\n",
        "                \"redshift-data:DescribeStatement\", \n",
        "                \"redshift-data:CancelStatement\"\n",
        "            ],\n",
        "            \"Resource\": \"*\",\n",
        "            \"Condition\": {\n",
        "                \"StringEquals\": {\n",
        "                    \"redshift-data:statement-owner-iam-userid\": \"${aws:userid}\"\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"RedshiftDataAPIExecutePermissions\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"redshift-data:ExecuteStatement\"\n",
        "            ],\n",
        "            \"Resource\": [\n",
        "                workgroup_arn\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"RedshiftServerlessGetCredentials\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": \"redshift-serverless:GetCredentials\",\n",
        "            \"Resource\": [\n",
        "                workgroup_arn\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"SqlWorkbenchAccess\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"sqlworkbench:GetSqlRecommendations\",\n",
        "                \"sqlworkbench:PutSqlGenerationContext\",\n",
        "                \"sqlworkbench:GetSqlGenerationContext\", \n",
        "                \"sqlworkbench:DeleteSqlGenerationContext\"\n",
        "            ],\n",
        "            \"Resource\": \"*\"\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"BedrockAccess\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"bedrock:GenerateQuery\",\n",
        "                \"bedrock:InvokeModel\",\n",
        "                \"bedrock:Retrieve\",\n",
        "                \"bedrock:RetrieveAndGenerate\"\n",
        "            ],\n",
        "            \"Resource\": \"*\"\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"LoggingPermissions\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"logs:CreateLogGroup\",\n",
        "                \"logs:CreateLogStream\",\n",
        "                \"logs:PutLogEvents\"\n",
        "            ],\n",
        "            \"Resource\": \"arn:aws:logs:*:*:*\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create and attach the enhanced policy\n",
        "enhanced_policy_name = f'EnhancedBedrockRedshiftPolicy-{suffix}'\n",
        "\n",
        "try:\n",
        "    # Create the enhanced policy\n",
        "    enhanced_policy_response = iam_client.create_policy(\n",
        "        PolicyName=enhanced_policy_name,\n",
        "        PolicyDocument=json.dumps(enhanced_policy_document),\n",
        "        Description='Enhanced policy for Bedrock Knowledge Base with Redshift access'\n",
        "    )\n",
        "    enhanced_policy_arn = enhanced_policy_response['Policy']['Arn']\n",
        "    print(f\"‚úÖ Created enhanced policy: {enhanced_policy_name}\")\n",
        "    \n",
        "    # Attach the enhanced policy to the role\n",
        "    iam_client.attach_role_policy(\n",
        "        RoleName=bedrock_role_name,\n",
        "        PolicyArn=enhanced_policy_arn\n",
        "    )\n",
        "    print(f\"‚úÖ Attached enhanced policy to role: {bedrock_role_name}\")\n",
        "    \n",
        "except iam_client.exceptions.EntityAlreadyExistsException:\n",
        "    print(f\"‚ÑπÔ∏è Enhanced policy {enhanced_policy_name} already exists\")\n",
        "    enhanced_policy_arn = f\"arn:aws:iam::{account_id}:policy/{enhanced_policy_name}\"\n",
        "    try:\n",
        "        iam_client.attach_role_policy(\n",
        "            RoleName=bedrock_role_name,\n",
        "            PolicyArn=enhanced_policy_arn\n",
        "        )\n",
        "        print(f\"‚úÖ Attached existing enhanced policy to role\")\n",
        "    except iam_client.exceptions.EntityAlreadyExistsException:\n",
        "        print(f\"‚ÑπÔ∏è Enhanced policy already attached to role\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating/attaching enhanced policy: {str(e)}\")\n",
        "\n",
        "print(\"\\\\nüîÑ Waiting 30 seconds for IAM changes to propagate...\")\n",
        "import time\n",
        "time.sleep(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting ingestion job with enhanced permissions...\n",
            "üìã Using data source ID: EOZFBUZD6F\n",
            "‚úÖ Ingestion job started successfully!\n",
            "üìä Job ID: JKTRBFLDGT\n",
            "üìä Initial Status: STARTING\n",
            "\\n‚è≥ Monitoring ingestion job progress...\n",
            "Attempt 1/20: ‚è≥ Status: IN_PROGRESS\n",
            "   ‚è≥ Waiting 15 seconds before next check...\n",
            "Attempt 2/20: ‚ùå Status: FAILED\n",
            "\\n‚ùå Ingestion job failed!\n"
          ]
        }
      ],
      "source": [
        "# Start ingestion job directly using Bedrock Agent client\n",
        "print(\"üöÄ Starting ingestion job with enhanced permissions...\")\n",
        "\n",
        "try:\n",
        "    # Get the data source ID\n",
        "    data_sources = bedrock_agent_client.list_data_sources(\n",
        "        knowledgeBaseId=kb_id,\n",
        "        maxResults=10\n",
        "    )\n",
        "    \n",
        "    if not data_sources['dataSourceSummaries']:\n",
        "        print(\"‚ùå No data sources found for this knowledge base\")\n",
        "    else:\n",
        "        data_source_id = data_sources['dataSourceSummaries'][0]['dataSourceId']\n",
        "        print(f\"üìã Using data source ID: {data_source_id}\")\n",
        "        \n",
        "        # Start the ingestion job directly\n",
        "        start_job_response = bedrock_agent_client.start_ingestion_job(\n",
        "            knowledgeBaseId=kb_id,\n",
        "            dataSourceId=data_source_id\n",
        "        )\n",
        "        \n",
        "        job = start_job_response[\"ingestionJob\"]\n",
        "        print(f\"‚úÖ Ingestion job started successfully!\")\n",
        "        print(f\"üìä Job ID: {job['ingestionJobId']}\")\n",
        "        print(f\"üìä Initial Status: {job['status']}\")\n",
        "        \n",
        "        # Monitor the job status\n",
        "        job_id = job['ingestionJobId']\n",
        "        max_attempts = 20\n",
        "        wait_time = 15\n",
        "        \n",
        "        print(f\"\\\\n‚è≥ Monitoring ingestion job progress...\")\n",
        "        \n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                get_job_response = bedrock_agent_client.get_ingestion_job(\n",
        "                    knowledgeBaseId=kb_id,\n",
        "                    dataSourceId=data_source_id,\n",
        "                    ingestionJobId=job_id\n",
        "                )\n",
        "                \n",
        "                current_job = get_job_response[\"ingestionJob\"]\n",
        "                status = current_job['status']\n",
        "                \n",
        "                status_emoji = {\n",
        "                    'COMPLETE': '‚úÖ',\n",
        "                    'IN_PROGRESS': '‚è≥',\n",
        "                    'STARTING': 'üîÑ', \n",
        "                    'FAILED': '‚ùå',\n",
        "                    'STOPPED': '‚èπÔ∏è'\n",
        "                }.get(status, '‚ùì')\n",
        "                \n",
        "                print(f\"Attempt {attempt + 1}/{max_attempts}: {status_emoji} Status: {status}\")\n",
        "                \n",
        "                if status == 'COMPLETE':\n",
        "                    print(\"\\\\nüéâ Ingestion job completed successfully!\")\n",
        "                    print(\"üéØ Knowledge Base is now ready to answer queries!\")\n",
        "                    break\n",
        "                elif status == 'FAILED':\n",
        "                    print(\"\\\\n‚ùå Ingestion job failed!\")\n",
        "                    if 'failureReasons' in current_job:\n",
        "                        print(f\"Failure reasons: {current_job['failureReasons']}\")\n",
        "                    if 'statistics' in current_job:\n",
        "                        print(f\"Statistics: {current_job['statistics']}\")\n",
        "                    break\n",
        "                elif status == 'STOPPED':\n",
        "                    print(\"\\\\n‚èπÔ∏è Ingestion job was stopped!\")\n",
        "                    break\n",
        "                elif status in ['IN_PROGRESS', 'STARTING']:\n",
        "                    if attempt < max_attempts - 1:\n",
        "                        print(f\"   ‚è≥ Waiting {wait_time} seconds before next check...\")\n",
        "                        time.sleep(wait_time)\n",
        "                    else:\n",
        "                        print(\"\\\\n‚è∞ Timeout reached. Job may still be running in background.\")\n",
        "                        print(\"Use the check_ingestion_status function to monitor progress.\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error checking job status: {str(e)}\")\n",
        "                if attempt < max_attempts - 1:\n",
        "                    print(f\"   üîÑ Retrying in {wait_time} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(\"\\\\n‚ùå Maximum retries reached for status check\")\n",
        "                    break\n",
        "                \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error starting ingestion job: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Note: Use test_knowledge_base() function after ingestion completes\n",
            "Example: test_knowledge_base()\n"
          ]
        }
      ],
      "source": [
        "# Test the Knowledge Base once ingestion is successful\n",
        "def test_knowledge_base():\n",
        "    \"\"\"Test the Knowledge Base with sample queries\"\"\"\n",
        "    print(\"üß™ Testing Knowledge Base functionality...\")\n",
        "    \n",
        "    # Test queries\n",
        "    test_queries = [\n",
        "        \"How many orders are there in total?\",\n",
        "        \"What is the average order value?\", \n",
        "        \"Which payment method is most popular?\",\n",
        "        \"How many different products have been ordered?\"\n",
        "    ]\n",
        "    \n",
        "    foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "    \n",
        "    for i, query in enumerate(test_queries, 1):\n",
        "        print(f\"\\\\nüîç Test Query {i}: {query}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        try:\n",
        "            # Test with retrieve_and_generate\n",
        "            response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
        "                input={\"text\": query},\n",
        "                retrieveAndGenerateConfiguration={\n",
        "                    \"type\": \"KNOWLEDGE_BASE\",\n",
        "                    \"knowledgeBaseConfiguration\": {\n",
        "                        'knowledgeBaseId': kb_id,\n",
        "                        \"modelArn\": f\"arn:aws:bedrock:{region}::foundation-model/{foundation_model}\",\n",
        "                        \"retrievalConfiguration\": {\n",
        "                            \"vectorSearchConfiguration\": {\n",
        "                                \"numberOfResults\": 5\n",
        "                            } \n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            print(f\"‚úÖ Response: {response['output']['text']}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {str(e)}\")\n",
        "            \n",
        "        print()\n",
        "\n",
        "# Note: Only run this after ingestion completes successfully\n",
        "print(\"üìù Note: Use test_knowledge_base() function after ingestion completes\")\n",
        "print(\"Example: test_knowledge_base()\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wait_for_statement(statement_id):\n",
        "    \"\"\"Wait for a Redshift Data API statement to complete\"\"\"\n",
        "    max_attempts = 30\n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            response = redshift_data_client.describe_statement(Id=statement_id)\n",
        "            status = response['Status']\n",
        "            if status == 'FINISHED':\n",
        "                return response\n",
        "            elif status == 'FAILED':\n",
        "                raise Exception(f\"Statement failed: {response.get('Error', 'Unknown error')}\")\n",
        "            elif status == 'CANCELLED':\n",
        "                raise Exception(\"Statement was cancelled\")\n",
        "            else:\n",
        "                print(f\"Statement status: {status}, waiting...\")\n",
        "                time.sleep(5)\n",
        "        except Exception as e:\n",
        "            if \"Statement failed\" in str(e) or \"cancelled\" in str(e):\n",
        "                raise\n",
        "            print(f\"Error checking statement status: {str(e)}, retrying...\")\n",
        "            time.sleep(5)\n",
        "    \n",
        "    raise Exception(\"Timeout waiting for statement to complete\")\n",
        "\n",
        "def run_redshift_statement(sql_statement):\n",
        "    \"\"\"Execute a SQL statement in Redshift\"\"\"\n",
        "    try:\n",
        "        response = redshift_data_client.execute_statement(\n",
        "            WorkgroupName=REDSHIFT_WORKGROUP,\n",
        "            Database=REDSHIFT_DATABASE,\n",
        "            Sql=sql_statement\n",
        "        )\n",
        "        statement_id = response['Id']\n",
        "        print(f\"Executing statement: {statement_id}\")\n",
        "        result = wait_for_statement(statement_id)\n",
        "        print(f\"Statement completed successfully\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing statement: {str(e)}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating table: orders\n",
            "Executing statement: 4d669868-ea32-4ce0-9180-c977ebc99514\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Created table: orders\n",
            "-------------\n",
            "Creating table: order_items\n",
            "Executing statement: ea378983-2fac-43f2-b3ce-9c8d016ef05d\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Created table: order_items\n",
            "-------------\n",
            "Creating table: payments\n",
            "Executing statement: a75c6c7a-9c38-4b72-8c74-82563692520b\n",
            "Statement status: SUBMITTED, waiting...\n",
            "Statement completed successfully\n",
            "Created table: payments\n",
            "-------------\n",
            "Creating table: reviews\n",
            "Executing statement: e227ecbd-4eb6-4a0b-a9d9-f41049822560\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Created table: reviews\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# Create tables in Redshift\n",
        "def create_tables():\n",
        "    \"\"\"Create all necessary tables in Redshift\"\"\"\n",
        "    \n",
        "    # Orders table\n",
        "    orders_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS orders (\n",
        "        order_id VARCHAR(255) PRIMARY KEY,\n",
        "        customer_id VARCHAR(255),\n",
        "        order_total DECIMAL(10,2),\n",
        "        order_status VARCHAR(50),\n",
        "        payment_method VARCHAR(50),\n",
        "        shipping_address TEXT,\n",
        "        created_at TIMESTAMP,\n",
        "        updated_at TIMESTAMP\n",
        "    );\n",
        "    \"\"\"\n",
        "    \n",
        "    # Order Items table\n",
        "    order_items_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS order_items (\n",
        "        order_item_id VARCHAR(255) PRIMARY KEY,\n",
        "        order_id VARCHAR(255),\n",
        "        product_id VARCHAR(255),\n",
        "        quantity INTEGER,\n",
        "        price DECIMAL(10,2)\n",
        "    );\n",
        "    \"\"\"\n",
        "    \n",
        "    # Payments table\n",
        "    payments_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS payments (\n",
        "        payment_id VARCHAR(255) PRIMARY KEY,\n",
        "        order_id VARCHAR(255),\n",
        "        customer_id VARCHAR(255),\n",
        "        amount DECIMAL(10,2),\n",
        "        payment_method VARCHAR(50),\n",
        "        payment_status VARCHAR(50),\n",
        "        created_at DATE\n",
        "    );\n",
        "    \"\"\"\n",
        "    \n",
        "    # Reviews table\n",
        "    reviews_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS reviews (\n",
        "        review_id VARCHAR(255) PRIMARY KEY,\n",
        "        product_id VARCHAR(255),\n",
        "        customer_id VARCHAR(255),\n",
        "        rating INTEGER,\n",
        "        created_at DATE\n",
        "    );\n",
        "    \"\"\"\n",
        "    \n",
        "    tables = {\n",
        "        'orders': orders_sql,\n",
        "        'order_items': order_items_sql,\n",
        "        'payments': payments_sql,\n",
        "        'reviews': reviews_sql\n",
        "    }\n",
        "    \n",
        "    for table_name, sql in tables.items():\n",
        "        print(f\"Creating table: {table_name}\")\n",
        "        run_redshift_statement(sql)\n",
        "        print(f\"Created table: {table_name}\")\n",
        "        print(\"-------------\")\n",
        "\n",
        "# Create tables\n",
        "create_tables()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data into orders from orders.csv\n",
            "Executing statement: 0a23526b-aa4a-4d34-9458-a6d0f8c0433a\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Loaded data into orders\n",
            "Loading data into order_items from order_items.csv\n",
            "Executing statement: 84d54756-be6c-43c2-9bb8-4f47f5657c37\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Loaded data into order_items\n",
            "Loading data into payments from payments.csv\n",
            "Executing statement: 8e48260c-1a43-45c8-a626-4ca55af5866c\n",
            "Statement status: SUBMITTED, waiting...\n",
            "Statement completed successfully\n",
            "Loaded data into payments\n",
            "Loading data into reviews from reviews.csv\n",
            "Executing statement: 6ba97e21-83cd-4e22-843c-068c0bfeb201\n",
            "Statement status: SUBMITTED, waiting...\n",
            "Statement completed successfully\n",
            "Loaded data into reviews\n"
          ]
        }
      ],
      "source": [
        "# Load data from S3 into Redshift tables\n",
        "def load_data_from_s3():\n",
        "    \"\"\"Load data from S3 CSV files into Redshift tables\"\"\"\n",
        "    \n",
        "    tables_and_files = {\n",
        "        'orders': 'orders.csv',\n",
        "        'order_items': 'order_items.csv',\n",
        "        'payments': 'payments.csv',\n",
        "        'reviews': 'reviews.csv'\n",
        "    }\n",
        "    \n",
        "    for table_name, file_name in tables_and_files.items():\n",
        "        print(f\"Loading data into {table_name} from {file_name}\")\n",
        "        \n",
        "        copy_sql = f\"\"\"\n",
        "        COPY {table_name}\n",
        "        FROM 's3://{S3_BUCKET}/{file_name}'\n",
        "        IAM_ROLE '{redshift_role_arn}'\n",
        "        CSV\n",
        "        IGNOREHEADER 1\n",
        "        DELIMITER ','\n",
        "        REGION '{region}';\n",
        "        \"\"\"\n",
        "        \n",
        "        try:\n",
        "            run_redshift_statement(copy_sql)\n",
        "            print(f\"Loaded data into {table_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data into {table_name}: {str(e)}\")\n",
        "\n",
        "# Load data from S3\n",
        "load_data_from_s3()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 4 - Create Bedrock Knowledge Base with Redshift Data Source\n",
        "\n",
        "Now we'll create the Bedrock Knowledge Base configured to use our Redshift data as a structured data source.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Knowledge Base Configuration for IAM Role + Redshift Serverless WorkGroup\n",
        "kbConfigParam = {\n",
        "    \"type\": \"SQL\",\n",
        "    \"sqlKnowledgeBaseConfiguration\": {\n",
        "        \"type\": \"REDSHIFT\",\n",
        "        \"redshiftConfiguration\": {\n",
        "            \"storageConfigurations\": [{\n",
        "                \"type\": \"REDSHIFT\",\n",
        "                \"redshiftConfiguration\": {\n",
        "                    \"databaseName\": REDSHIFT_DATABASE\n",
        "                }\n",
        "            }],\n",
        "            \"queryEngineConfiguration\": {\n",
        "                \"type\": \"SERVERLESS\",\n",
        "                \"serverlessConfiguration\": {\n",
        "                    \"workgroupArn\": workgroup_arn,\n",
        "                    \"authConfiguration\": {\n",
        "                        \"type\": \"IAM\"\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 5 - Create Knowledge Base\n",
        "\n",
        "This step creates the Bedrock Knowledge Base configured for IAM Role + Redshift Serverless WorkGroup access pattern. The knowledge base will use IAM authentication to securely connect to your Redshift Serverless workgroup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-06-20 21:04:12,179] p74707 {credentials.py:1352} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Knowledge Base with IAM Role + Redshift Serverless WorkGroup access pattern...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-06-20 21:04:12,950] p74707 {credentials.py:1352} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================\n",
            "Step 1 - Creating Knowledge Base Execution Role (AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647) and Policies\n",
            "========================================================================================\n",
            "Step 2 - Creating Knowledge Base\n",
            "{ 'createdAt': datetime.datetime(2025, 6, 21, 4, 4, 13, 800686, tzinfo=tzutc()),\n",
            "  'description': 'Sample Structured KB',\n",
            "  'knowledgeBaseArn': 'arn:aws:bedrock:us-west-2:533267284022:knowledge-base/WCNCTKFCKY',\n",
            "  'knowledgeBaseConfiguration': { 'sqlKnowledgeBaseConfiguration': { 'redshiftConfiguration': { 'queryEngineConfiguration': { 'serverlessConfiguration': { 'authConfiguration': { 'type': 'IAM'},\n",
            "                                                                                                                                                           'workgroupArn': 'arn:aws:redshift-serverless:us-west-2:533267284022:workgroup/a68f79bf-ace3-46d7-abc3-3ee32e4998aa'},\n",
            "                                                                                                                              'type': 'SERVERLESS'},\n",
            "                                                                                                'storageConfigurations': [ { 'redshiftConfiguration': { 'databaseName': 'sds-ecommerce'},\n",
            "                                                                                                                             'type': 'REDSHIFT'}]},\n",
            "                                                                     'type': 'REDSHIFT'},\n",
            "                                  'type': 'SQL'},\n",
            "  'knowledgeBaseId': 'WCNCTKFCKY',\n",
            "  'name': 'bedrock-sample-structured-kb-0205647',\n",
            "  'roleArn': 'arn:aws:iam::533267284022:role/AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647',\n",
            "  'status': 'CREATING',\n",
            "  'updatedAt': datetime.datetime(2025, 6, 21, 4, 4, 13, 800686, tzinfo=tzutc())}\n",
            "Creating Data Sources aka query engine\n",
            "{ 'createdAt': datetime.datetime(2025, 6, 21, 4, 4, 13, 916661, tzinfo=tzutc()),\n",
            "  'dataSourceConfiguration': {'type': 'REDSHIFT_METADATA'},\n",
            "  'dataSourceId': 'EOZFBUZD6F',\n",
            "  'description': 'Query engine',\n",
            "  'knowledgeBaseId': 'WCNCTKFCKY',\n",
            "  'name': 'bedrock-sample-structured-kb-0205647-ds',\n",
            "  'status': 'AVAILABLE',\n",
            "  'updatedAt': datetime.datetime(2025, 6, 21, 4, 4, 13, 916661, tzinfo=tzutc())}\n",
            "========================================================================================\n",
            "Knowledge Base created successfully!\n",
            "'WCNCTKFCKY'\n",
            "Knowledge Base ID: WCNCTKFCKY\n"
          ]
        }
      ],
      "source": [
        "# Create the Knowledge Base using IAM Role + Redshift Serverless WorkGroup\n",
        "print(\"Creating Knowledge Base with IAM Role + Redshift Serverless WorkGroup access pattern...\")\n",
        "\n",
        "knowledge_base = BedrockStructuredKnowledgeBase(\n",
        "    kb_name=knowledge_base_name,\n",
        "    kb_description=knowledge_base_description,\n",
        "    workgroup_arn=workgroup_arn,\n",
        "    kbConfigParam=kbConfigParam,\n",
        "    generation_model=foundation_model,\n",
        "    suffix=suffix\n",
        ")\n",
        "\n",
        "print(\"Knowledge Base created successfully!\")\n",
        "print(f\"Knowledge Base ID: {knowledge_base.get_knowledge_base_id()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "### Grant Database Access to the Bedrock IAM Role\n",
        "\n",
        "For the IAM access pattern, you need to grant database access to the IAM role that Bedrock Knowledge Base uses for authentication. Execute the following SQL commands in your Redshift Query Editor to create the IAM user and grant appropriate permissions.\n",
        "\n",
        "For more detailed steps, please see the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-structured.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execute the following SQL commands in your Redshift Query Editor:\n",
            "======================================================================\n",
            "CREATE USER \"IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\" WITH PASSWORD DISABLE;\n",
            "GRANT USAGE ON SCHEMA public TO \"IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\";\n",
            "GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\";\n",
            "======================================================================\n",
            "\\nThese commands will:\n",
            "1. Create an IAM-based database user for the Bedrock execution role\n",
            "2. Grant USAGE permission on the public schema\n",
            "3. Grant SELECT permission on all tables in the public schema\n",
            "\\nNote: Adjust the schema and table permissions based on your specific requirements.\n"
          ]
        }
      ],
      "source": [
        "# Get the Bedrock execution role name\n",
        "bedrock_role_name = knowledge_base.bedrock_kb_execution_role_name\n",
        "\n",
        "# Display the SQL commands that need to be executed in Redshift Query Editor\n",
        "print(\"Execute the following SQL commands in your Redshift Query Editor:\")\n",
        "print(\"=\" * 70)\n",
        "print(f'CREATE USER \"IAMR:{bedrock_role_name}\" WITH PASSWORD DISABLE;')\n",
        "print(f'GRANT USAGE ON SCHEMA public TO \"IAMR:{bedrock_role_name}\";')\n",
        "print(f'GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"IAMR:{bedrock_role_name}\";')\n",
        "print(\"=\" * 70)\n",
        "print(\"\\\\nThese commands will:\")\n",
        "print(\"1. Create an IAM-based database user for the Bedrock execution role\")\n",
        "print(\"2. Grant USAGE permission on the public schema\")\n",
        "print(\"3. Grant SELECT permission on all tables in the public schema\")\n",
        "print(\"\\\\nNote: Adjust the schema and table permissions based on your specific requirements.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 6 - Start the ingestion job\n",
        "\n",
        "This step is to start the ingestion job to sync the datasources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "job  started successfully\n",
            "\n",
            "{ 'dataSourceId': 'EOZFBUZD6F',\n",
            "  'ingestionJobId': 'HOYDC9A0DV',\n",
            "  'knowledgeBaseId': 'WCNCTKFCKY',\n",
            "  'startedAt': datetime.datetime(2025, 6, 21, 4, 5, 39, 955341, tzinfo=tzutc()),\n",
            "  'status': 'FAILED',\n",
            "  'updatedAt': datetime.datetime(2025, 6, 21, 4, 5, 43, 167059, tzinfo=tzutc())}\n",
            "'WCNCTKFCKY'\n"
          ]
        }
      ],
      "source": [
        "# ensure that the kb is available\n",
        "time.sleep(60)\n",
        "# sync knowledge base\n",
        "knowledge_base.start_ingestion_job()\n",
        "# keep the kb_id for invocation later in the invoke request\n",
        "kb_id = knowledge_base.get_knowledge_base_id()\n",
        "# %store kb_id\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 7 - Test the Structured Knowledge Base\n",
        "\n",
        "Now the Knowledge Base is available we can test it out using the retrieve, retrieve_and_generate, and generate_query functions.\n",
        "\n",
        "- When you use **retrieve**, the response returns the result of the SQL query execution.\n",
        "- When you use **retrieve_and_generate**, the generated response is based on the result of the SQL query execution\n",
        "- When using the **generate_query** API, it transforms a natural language query into SQL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"How many orders are there in total?\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "### 7.1 - Using RetrieveAndGenerate API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValidationException",
          "evalue": "An error occurred (ValidationException) when calling the RetrieveAndGenerate operation: No metadata found. Please trigger an ingestion and try again. (Service: BedrockAgentRuntime, Status Code: 400, Request ID: 4846aded-b9bf-403d-ad46-00cc9556ccc6) (SDK Attempt Count: 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n",
            "\u001b[1;32m      1\u001b[0m foundation_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic.claude-3-sonnet-20240229-v1:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mbedrock_agent_runtime_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_and_generate\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n",
            "\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\n",
            "\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrieveAndGenerateConfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n",
            "\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKNOWLEDGE_BASE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mknowledgeBaseConfiguration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n",
            "\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mknowledgeBaseId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkb_id\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodelArn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marn:aws:bedrock:\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m::foundation-model/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoundation_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretrievalConfiguration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n",
            "\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvectorSearchConfiguration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n",
            "\u001b[1;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumberOfResults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n",
            "\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\n",
            "\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n",
            "\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n",
            "\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n",
            "\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
            "\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
            "\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages/botocore/client.py:598\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n",
            "\u001b[1;32m    595\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m    596\u001b[0m     )\n",
            "\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n",
            "\u001b[0;32m--> 598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n",
            "\u001b[1;32m    122\u001b[0m     hook()\n",
            "\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages/botocore/client.py:1061\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n",
            "\u001b[1;32m   1057\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n",
            "\u001b[1;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m   1059\u001b[0m     )\n",
            "\u001b[1;32m   1060\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n",
            "\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n",
            "\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
            "\n",
            "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the RetrieveAndGenerate operation: No metadata found. Please trigger an ingestion and try again. (Service: BedrockAgentRuntime, Status Code: 400, Request ID: 4846aded-b9bf-403d-ad46-00cc9556ccc6) (SDK Attempt Count: 1)"
          ]
        }
      ],
      "source": [
        "foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "\n",
        "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
        "    input={\n",
        "        \"text\": query\n",
        "    },\n",
        "    retrieveAndGenerateConfiguration={\n",
        "        \"type\": \"KNOWLEDGE_BASE\",\n",
        "        \"knowledgeBaseConfiguration\": {\n",
        "            'knowledgeBaseId': kb_id,\n",
        "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
        "            \"retrievalConfiguration\": {\n",
        "                \"vectorSearchConfiguration\": {\n",
        "                    \"numberOfResults\": 5\n",
        "                } \n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response['output']['text'], end='\\n'*2)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "### 7.2 - Using Retrieve API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_ret = bedrock_agent_runtime_client.retrieve(\n",
        "    knowledgeBaseId=kb_id, \n",
        "    nextToken='string',\n",
        "    retrievalConfiguration={\n",
        "        \"vectorSearchConfiguration\": {\n",
        "            \"numberOfResults\": 5,\n",
        "        } \n",
        "    },\n",
        "    retrievalQuery={\n",
        "        \"text\": query\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Function to extract retrieved results from Retrieve API response into a pandas dataframe.\n",
        "def response_print(retrieve_resp):\n",
        "    # Extract the retrievalResults list\n",
        "    retrieval_results = retrieve_resp['retrievalResults']\n",
        "\n",
        "    # Dictionary to store the extracted data\n",
        "    extracted_data = {}\n",
        "\n",
        "    # Iterate through each item in retrievalResults\n",
        "    for item in retrieval_results:\n",
        "        row = item['content']['row']\n",
        "        for col in row:\n",
        "            column_name = col['columnName']\n",
        "            column_value = col['columnValue']\n",
        "            \n",
        "            # If this column hasn't been seen before, create a new list for it\n",
        "            if column_name not in extracted_data:\n",
        "                extracted_data[column_name] = []\n",
        "            \n",
        "            # Append the value to the appropriate list\n",
        "            extracted_data[column_name].append(column_value)\n",
        "\n",
        "    # Create a DataFrame from the extracted data\n",
        "    df = pd.DataFrame(extracted_data)\n",
        "    return df\n",
        "    \n",
        "# Display the Retrieved results records\n",
        "df = response_print(response_ret)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "### 7.3 - Using Generate Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_response = bedrock_agent_runtime_client.generate_query(\n",
        "    queryGenerationInput={\n",
        "        \"text\": query,\n",
        "        \"type\": \"TEXT\"\n",
        "    },\n",
        "    transformationConfiguration={\n",
        "        \"mode\": \"TEXT_TO_SQL\",\n",
        "        \"textToSqlConfiguration\": {\n",
        "            \"type\": \"KNOWLEDGE_BASE\",\n",
        "            \"knowledgeBaseConfiguration\": {\n",
        "                \"knowledgeBaseArn\": knowledge_base.knowledge_base['knowledgeBaseArn']\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "generated_sql = query_response['queries'][0]['sql']\n",
        "generated_sql\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 8 - Clean Up\n",
        "\n",
        "Please make sure to uncomment and run the below section to delete all the resources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete resources\n",
        "# print(\"=============================== Deleting resources ==============================\\n\")\n",
        "# knowledge_base.delete_kb(delete_iam_roles_and_policies=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.38.36\n"
          ]
        }
      ],
      "source": [
        "# %pip install --upgrade pip --quiet\n",
        "# %pip install -r ../requirements.txt --no-deps --quiet\n",
        "# %pip install -r ../requirements.txt --upgrade --quiet\n",
        "# %pip install --upgrade boto3\n",
        "import boto3\n",
        "print(boto3.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<script>Jupyter.notebook.kernel.restart()</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# restart kernel\n",
        "from IPython.core.display import HTML\n",
        "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "This code is part of the setup and used to:\n",
        "- Add the parent directory to the python system path\n",
        "- Imports a custom module (BedrockStructuredKnowledgeBase) from utils necessary for later executions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/Users/manojs/Documents/Code/samples/05-agentic-rag/2-unstructure-structured-rag_agent', '/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python312.zip', '/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12', '/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/lib-dynload', '', '/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages', '/Users/manojs/Documents/Code/samples/05-agentic-rag']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "current_path = Path().resolve()\n",
        "current_path = current_path.parent\n",
        "\n",
        "if str(current_path) not in sys.path:\n",
        "    sys.path.append(str(current_path))\n",
        "\n",
        "# Print sys.path to verify\n",
        "print(sys.path)\n",
        "\n",
        "from utils.structured_knowledge_base import BedrockStructuredKnowledgeBase\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup and initialize boto3 clients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('us-west-2', '533267284022')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "s3_client = boto3.client('s3')\n",
        "sts_client = boto3.client('sts')\n",
        "session = boto3.session.Session(region_name='us-west-2')\n",
        "region = session.region_name\n",
        "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
        "bedrock_agent_client = boto3.client('bedrock-agent')\n",
        "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime')\n",
        "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "region, account_id\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "Initialize and configure the knowledge base name and the foundational model. This foundational model will be used to generate the natural language response based on the records received from the structured data store.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Get the current timestamp\n",
        "current_time = time.time()\n",
        "\n",
        "# Format the timestamp as a string\n",
        "timestamp_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(current_time))[-7:]\n",
        "# Create the suffix using the timestamp\n",
        "suffix = f\"{timestamp_str}\"\n",
        "\n",
        "knowledge_base_name = f\"bedrock-sample-structured-kb-{suffix}\"\n",
        "knowledge_base_description = \"Sample Structured KB\"\n",
        "\n",
        "foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "Amazon Bedrock Knowledge Bases uses a service role to connect knowledge bases to structured data stores, retrieve data from these data stores, and generate SQL queries based on user queries and the structure of the data stores. \n",
        "\n",
        "This notebook demonstrates the **IAM Role + Redshift Serverless WorkGroup** access pattern, which provides secure access to your Redshift Serverless workgroup using IAM authentication.\n",
        "\n",
        "**Required Configuration Variables:**\n",
        "- workgroup_id: Your Redshift Serverless workgroup identifier\n",
        "- redshiftDBName: Your database name within the workgroup\n",
        "\n",
        "The knowledge base configuration will use these parameters to establish the connection and perform necessary authentication using IAM roles.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 1 - Set up Redshift Serverless Infrastructure\n",
        "\n",
        "This section will create the necessary Redshift Serverless components: namespace and workgroup. This infrastructure will host our structured data that the Knowledge Base will query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Redshift Namespace: sds-ecommerce-0205647\n",
            "Redshift Workgroup: sds-ecommerce-wg-0205647\n",
            "Database: sds-ecommerce\n",
            "S3 Bucket: sds-ecommerce-redshift-0205647\n"
          ]
        }
      ],
      "source": [
        "# Configuration for Redshift resources\n",
        "REDSHIFT_NAMESPACE = f'sds-ecommerce-{suffix}'\n",
        "REDSHIFT_WORKGROUP = f'sds-ecommerce-wg-{suffix}'\n",
        "REDSHIFT_DATABASE = f'sds-ecommerce'\n",
        "S3_BUCKET = f'sds-ecommerce-redshift-{suffix}'\n",
        "\n",
        "print(f\"Redshift Namespace: {REDSHIFT_NAMESPACE}\")\n",
        "print(f\"Redshift Workgroup: {REDSHIFT_WORKGROUP}\")\n",
        "print(f\"Database: {REDSHIFT_DATABASE}\")\n",
        "print(f\"S3 Bucket: {S3_BUCKET}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created role RedshiftS3AccessRole-0205647\n",
            "Redshift IAM Role ARN: arn:aws:iam::533267284022:role/RedshiftS3AccessRole-0205647\n",
            "Role RedshiftS3AccessRole-0205647 already exists\n",
            "Redshift IAM Role ARN: arn:aws:iam::533267284022:role/RedshiftS3AccessRole-0205647\n"
          ]
        }
      ],
      "source": [
        "def create_iam_role_for_redshift():\n",
        "    \"\"\"Create IAM role for Redshift to access S3\"\"\"\n",
        "    try:\n",
        "        # Get account ID\n",
        "        account_id = sts_client.get_caller_identity()['Account']\n",
        "        \n",
        "        # Create IAM role if it doesn't exist\n",
        "        role_name = f'RedshiftS3AccessRole-{suffix}'\n",
        "        try:\n",
        "            role_response = iam_client.get_role(RoleName=role_name)\n",
        "            print(f'Role {role_name} already exists')\n",
        "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
        "        except iam_client.exceptions.NoSuchEntityException:\n",
        "            trust_policy = {\n",
        "                \"Version\": \"2012-10-17\",\n",
        "                \"Statement\": [\n",
        "                    {\n",
        "                        \"Effect\": \"Allow\",\n",
        "                        \"Principal\": {\n",
        "                            \"Service\": \"redshift.amazonaws.com\"\n",
        "                        },\n",
        "                        \"Action\": \"sts:AssumeRole\"\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "            \n",
        "            iam_client.create_role(\n",
        "                RoleName=role_name,\n",
        "                AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
        "            )\n",
        "            \n",
        "            # Attach necessary policies\n",
        "            iam_client.attach_role_policy(\n",
        "                RoleName=role_name,\n",
        "                PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n",
        "            )\n",
        "            \n",
        "            print(f'Created role {role_name}')\n",
        "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f'Error creating IAM role: {str(e)}')\n",
        "        raise\n",
        "\n",
        "# Initialize additional clients\n",
        "import json\n",
        "import os\n",
        "iam_client = boto3.client('iam')\n",
        "redshift_client = boto3.client('redshift-serverless', region_name=region)\n",
        "redshift_data_client = boto3.client('redshift-data', region_name=region)\n",
        "\n",
        "redshift_role_arn = create_iam_role_for_redshift()\n",
        "print(f\"Redshift IAM Role ARN: {redshift_role_arn}\")\n",
        "\n",
        "redshift_role_arn = create_iam_role_for_redshift()\n",
        "print(f\"Redshift IAM Role ARN: {redshift_role_arn}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating namespace sds-ecommerce-0205647...\n",
            "Created namespace sds-ecommerce-0205647\n",
            "Waiting for namespace to be available...\n",
            "Namespace sds-ecommerce-0205647 is now available\n"
          ]
        }
      ],
      "source": [
        "def create_redshift_namespace():\n",
        "    \"\"\"Create Redshift Serverless namespace\"\"\"\n",
        "    try:\n",
        "        # Check if namespace already exists\n",
        "        try:\n",
        "            response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
        "            print(f'Namespace {REDSHIFT_NAMESPACE} already exists')\n",
        "            return response['namespace']\n",
        "        except redshift_client.exceptions.ResourceNotFoundException:\n",
        "            print(f'Creating namespace {REDSHIFT_NAMESPACE}...')\n",
        "        \n",
        "        # Create the namespace\n",
        "        response = redshift_client.create_namespace(\n",
        "            namespaceName=REDSHIFT_NAMESPACE,\n",
        "            adminUsername='admin',\n",
        "            adminUserPassword='TempPassword123!',  # Change this in production\n",
        "            dbName=REDSHIFT_DATABASE,\n",
        "            defaultIamRoleArn=redshift_role_arn,\n",
        "            iamRoles=[redshift_role_arn]\n",
        "        )\n",
        "        \n",
        "        print(f'Created namespace {REDSHIFT_NAMESPACE}')\n",
        "        \n",
        "        # Wait for namespace to be available\n",
        "        print('Waiting for namespace to be available...')\n",
        "        max_attempts = 30\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                namespace_response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
        "                status = namespace_response['namespace']['status']\n",
        "                if status == 'AVAILABLE':\n",
        "                    print(f'Namespace {REDSHIFT_NAMESPACE} is now available')\n",
        "                    return namespace_response['namespace']\n",
        "                else:\n",
        "                    print(f'Namespace status: {status}, waiting...')\n",
        "                    time.sleep(10)\n",
        "            except Exception as e:\n",
        "                print(f'Error checking namespace status: {str(e)}, retrying...')\n",
        "                time.sleep(10)\n",
        "        \n",
        "        print('Timeout waiting for namespace, but proceeding...')\n",
        "        return response['namespace']\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f'Error creating namespace: {str(e)}')\n",
        "        raise\n",
        "\n",
        "# Create namespace\n",
        "namespace = create_redshift_namespace()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating workgroup sds-ecommerce-wg-0205647...\n",
            "Created workgroup sds-ecommerce-wg-0205647\n",
            "Waiting for workgroup to be available...\n",
            "Workgroup status: CREATING, waiting...\n",
            "Workgroup status: CREATING, waiting...\n",
            "Workgroup status: CREATING, waiting...\n",
            "Workgroup status: CREATING, waiting...\n",
            "Workgroup sds-ecommerce-wg-0205647 is now available\n",
            "Workgroup ARN: arn:aws:redshift-serverless:us-west-2:533267284022:workgroup/a68f79bf-ace3-46d7-abc3-3ee32e4998aa\n"
          ]
        }
      ],
      "source": [
        "def create_redshift_workgroup():\n",
        "    \"\"\"Create Redshift Serverless workgroup\"\"\"\n",
        "    try:\n",
        "        # Check if workgroup already exists\n",
        "        try:\n",
        "            response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
        "            print(f'Workgroup {REDSHIFT_WORKGROUP} already exists')\n",
        "            return response['workgroup']\n",
        "        except redshift_client.exceptions.ResourceNotFoundException:\n",
        "            print(f'Creating workgroup {REDSHIFT_WORKGROUP}...')\n",
        "        \n",
        "        # Create the workgroup\n",
        "        response = redshift_client.create_workgroup(\n",
        "            workgroupName=REDSHIFT_WORKGROUP,\n",
        "            namespaceName=REDSHIFT_NAMESPACE,\n",
        "            baseCapacity=8,  # Minimum base capacity\n",
        "            enhancedVpcRouting=False,\n",
        "            publiclyAccessible=True,\n",
        "            configParameters=[\n",
        "                {\n",
        "                    'parameterKey': 'enable_user_activity_logging',\n",
        "                    'parameterValue': 'true'\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        print(f'Created workgroup {REDSHIFT_WORKGROUP}')\n",
        "        \n",
        "        # Wait for workgroup to be available\n",
        "        print('Waiting for workgroup to be available...')\n",
        "        max_attempts = 30\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                workgroup_response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
        "                status = workgroup_response['workgroup']['status']\n",
        "                if status == 'AVAILABLE':\n",
        "                    print(f'Workgroup {REDSHIFT_WORKGROUP} is now available')\n",
        "                    return workgroup_response['workgroup']\n",
        "                else:\n",
        "                    print(f'Workgroup status: {status}, waiting...')\n",
        "                    time.sleep(10)\n",
        "            except Exception as e:\n",
        "                print(f'Error checking workgroup status: {str(e)}, retrying...')\n",
        "                time.sleep(10)\n",
        "        \n",
        "        print('Timeout waiting for workgroup, but proceeding...')\n",
        "        return response['workgroup']\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f'Error creating workgroup: {str(e)}')\n",
        "        raise\n",
        "\n",
        "# Create workgroup\n",
        "workgroup = create_redshift_workgroup()\n",
        "workgroup_arn = workgroup['workgroupArn']\n",
        "print(f\"Workgroup ARN: {workgroup_arn}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 - Create S3 Bucket and Load Sample Data\n",
        "\n",
        "We will create an S3 bucket to stage our sample e-commerce data before loading it into Redshift tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created bucket sds-ecommerce-redshift-0205647\n"
          ]
        }
      ],
      "source": [
        "def create_s3_bucket():\n",
        "    \"\"\"Create S3 bucket for data staging\"\"\"\n",
        "    try:\n",
        "        s3_client.head_bucket(Bucket=S3_BUCKET)\n",
        "        print(f'Bucket {S3_BUCKET} already exists')\n",
        "    except:\n",
        "        try:\n",
        "            if region == 'us-east-1':\n",
        "                s3_client.create_bucket(Bucket=S3_BUCKET)\n",
        "            else:\n",
        "                s3_client.create_bucket(\n",
        "                    Bucket=S3_BUCKET,\n",
        "                    CreateBucketConfiguration={'LocationConstraint': region}\n",
        "                )\n",
        "            print(f'Created bucket {S3_BUCKET}')\n",
        "        except Exception as e:\n",
        "            print(f'Error creating bucket: {str(e)}')\n",
        "            raise\n",
        "\n",
        "# Create S3 bucket\n",
        "create_s3_bucket()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading sample data files to S3...\n",
            "Uploaded orders.csv (1.8 MB) to S3\n",
            "Uploaded order_items.csv (1.3 MB) to S3\n",
            "Uploaded payments.csv (0.8 MB) to S3\n",
            "Uploaded reviews.csv (0.5 MB) to S3\n",
            "Successfully uploaded all 4 data files to S3\n"
          ]
        }
      ],
      "source": [
        "def upload_sample_data():\n",
        "    \"\"\"Upload sample CSV files to S3\"\"\"\n",
        "    data_files = ['orders.csv', 'order_items.csv', 'payments.csv', 'reviews.csv']\n",
        "    sds_directory = 'sample_data'\n",
        "    \n",
        "    print(\"Uploading sample data files to S3...\")\n",
        "    files_found = 0\n",
        "    \n",
        "    for file_name in data_files:\n",
        "        local_path = os.path.join(sds_directory, file_name)\n",
        "        if os.path.exists(local_path):\n",
        "            # Get file size for informational purposes\n",
        "            file_size = os.path.getsize(local_path)\n",
        "            file_size_mb = file_size / (1024 * 1024)\n",
        "            \n",
        "            s3_client.upload_file(local_path, S3_BUCKET, file_name)\n",
        "            print(f'Uploaded {file_name} ({file_size_mb:.1f} MB) to S3')\n",
        "            files_found += 1\n",
        "        else:\n",
        "            print(f'Warning: {local_path} not found')\n",
        "    \n",
        "    if files_found == len(data_files):\n",
        "        print(f\"Successfully uploaded all {files_found} data files to S3\")\n",
        "    else:\n",
        "        print(f\"Only {files_found} out of {len(data_files)} files were found and uploaded\")\n",
        "\n",
        "# Upload sample data\n",
        "upload_sample_data()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 3 - Create Redshift Tables and Load Data\n",
        "\n",
        "Now we will create the database tables in Redshift and load our sample e-commerce data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Executing SQL commands to grant database access to Bedrock execution role...\n",
            "Creating user: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "Executing statement: cb83ac9a-aa81-41ad-8bde-91ceb92cfc0c\n",
            "Statement status: SUBMITTED, waiting...\n",
            "Statement status: STARTED, waiting...\n",
            "Statement status: STARTED, waiting...\n",
            "Error executing statement: Statement failed: ERROR: user \"IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\" already exists\n",
            "‚ÑπÔ∏è User already exists, continuing...\n",
            "Granting USAGE on schema public to: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "Executing statement: c948acbc-0201-440d-b2e1-fee3521a65ac\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "‚úÖ USAGE permission granted successfully!\n",
            "Granting SELECT on all tables to: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "Executing statement: 6c413ca2-d25d-41bb-ae99-d28f3506ee19\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "‚úÖ SELECT permissions granted successfully!\n",
            "\\nüéâ All database permissions have been granted to the Bedrock execution role!\n",
            "The Knowledge Base should now be able to access the database tables.\n"
          ]
        }
      ],
      "source": [
        "# Execute the required SQL commands to grant database access to Bedrock execution role\n",
        "print(\"üîß Executing SQL commands to grant database access to Bedrock execution role...\")\n",
        "\n",
        "bedrock_role_name = knowledge_base.bedrock_kb_execution_role_name\n",
        "\n",
        "# Step 1: Create the IAM user in Redshift database\n",
        "create_user_sql = f'CREATE USER \"IAMR:{bedrock_role_name}\" WITH PASSWORD DISABLE;'\n",
        "\n",
        "try:\n",
        "    print(f\"Creating user: IAMR:{bedrock_role_name}\")\n",
        "    run_redshift_statement(create_user_sql)\n",
        "    print(\"‚úÖ IAM user created successfully!\")\n",
        "except Exception as e:\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        print(\"‚ÑπÔ∏è User already exists, continuing...\")\n",
        "    else:\n",
        "        print(f\"‚ùå Error creating user: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Step 2: Grant USAGE permission on public schema\n",
        "grant_usage_sql = f'GRANT USAGE ON SCHEMA public TO \"IAMR:{bedrock_role_name}\";'\n",
        "\n",
        "try:\n",
        "    print(f\"Granting USAGE on schema public to: IAMR:{bedrock_role_name}\")\n",
        "    run_redshift_statement(grant_usage_sql)\n",
        "    print(\"‚úÖ USAGE permission granted successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error granting USAGE permission: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Step 3: Grant SELECT permission on all tables in public schema\n",
        "grant_select_sql = f'GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"IAMR:{bedrock_role_name}\";'\n",
        "\n",
        "try:\n",
        "    print(f\"Granting SELECT on all tables to: IAMR:{bedrock_role_name}\")\n",
        "    run_redshift_statement(grant_select_sql)\n",
        "    print(\"‚úÖ SELECT permissions granted successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error granting SELECT permissions: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "print(\"\\\\nüéâ All database permissions have been granted to the Bedrock execution role!\")\n",
        "print(\"The Knowledge Base should now be able to access the database tables.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Retrying the ingestion job with proper permissions...\n",
            "job  started successfully\n",
            "\n",
            "{ 'dataSourceId': 'EOZFBUZD6F',\n",
            "  'ingestionJobId': 'YBQXYQU4WL',\n",
            "  'knowledgeBaseId': 'WCNCTKFCKY',\n",
            "  'startedAt': datetime.datetime(2025, 6, 21, 7, 12, 41, 597571, tzinfo=tzutc()),\n",
            "  'status': 'FAILED',\n",
            "  'updatedAt': datetime.datetime(2025, 6, 21, 7, 12, 44, 860633, tzinfo=tzutc())}\n",
            "‚úÖ New ingestion job started successfully!\n",
            "Ingestion Job Response: None\n",
            "‚è≥ Waiting for ingestion job to complete...\n",
            "‚ùå Error starting ingestion job: 'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[25], line 16\u001b[0m\n",
            "\u001b[1;32m     13\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m30\u001b[39m)\n",
            "\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Check the ingestion job status\u001b[39;00m\n",
            "\u001b[0;32m---> 16\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mingestionJobId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
            "\u001b[1;32m     17\u001b[0m job_status \u001b[38;5;241m=\u001b[39m bedrock_agent_client\u001b[38;5;241m.\u001b[39mget_ingestion_job(\n",
            "\u001b[1;32m     18\u001b[0m     knowledgeBaseId\u001b[38;5;241m=\u001b[39mkb_id,\n",
            "\u001b[1;32m     19\u001b[0m     dataSourceId\u001b[38;5;241m=\u001b[39mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataSourceId\u001b[39m\u001b[38;5;124m'\u001b[39m],\n",
            "\u001b[1;32m     20\u001b[0m     ingestionJobId\u001b[38;5;241m=\u001b[39mjob_id\n",
            "\u001b[1;32m     21\u001b[0m )\n",
            "\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnüìä Ingestion Job Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_status[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mingestionJob\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# Retry the ingestion job now that permissions are fixed\n",
        "print(\"üîÑ Retrying the ingestion job with proper permissions...\")\n",
        "\n",
        "try:\n",
        "    # Start a new ingestion job\n",
        "    response = knowledge_base.start_ingestion_job()\n",
        "    print(\"‚úÖ New ingestion job started successfully!\")\n",
        "    print(f\"Ingestion Job Response: {response}\")\n",
        "    \n",
        "    # Wait a bit for the job to process\n",
        "    import time\n",
        "    print(\"‚è≥ Waiting for ingestion job to complete...\")\n",
        "    time.sleep(30)\n",
        "    \n",
        "    # Check the ingestion job status\n",
        "    job_id = response['ingestionJobId']\n",
        "    job_status = bedrock_agent_client.get_ingestion_job(\n",
        "        knowledgeBaseId=kb_id,\n",
        "        dataSourceId=response['dataSourceId'],\n",
        "        ingestionJobId=job_id\n",
        "    )\n",
        "    \n",
        "    print(f\"\\\\nüìä Ingestion Job Status: {job_status['ingestionJob']['status']}\")\n",
        "    if job_status['ingestionJob']['status'] == 'COMPLETE':\n",
        "        print(\"üéâ Ingestion job completed successfully!\")\n",
        "    elif job_status['ingestionJob']['status'] == 'IN_PROGRESS':\n",
        "        print(\"‚è≥ Ingestion job is still in progress. Wait a few more minutes.\")\n",
        "    else:\n",
        "        print(f\"‚ùå Ingestion job status: {job_status['ingestionJob']['status']}\")\n",
        "        if 'failureReasons' in job_status['ingestionJob']:\n",
        "            print(f\"Failure reasons: {job_status['ingestionJob']['failureReasons']}\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error starting ingestion job: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to check ingestion job status\n",
        "def check_ingestion_status(kb_id, data_source_id=None, job_id=None):\n",
        "    \"\"\"Check the status of ingestion jobs for a knowledge base\"\"\"\n",
        "    try:\n",
        "        # If no data_source_id provided, get the first one\n",
        "        if not data_source_id:\n",
        "            data_sources = bedrock_agent_client.list_data_sources(\n",
        "                knowledgeBaseId=kb_id,\n",
        "                maxResults=10\n",
        "            )\n",
        "            if data_sources['dataSourceSummaries']:\n",
        "                data_source_id = data_sources['dataSourceSummaries'][0]['dataSourceId']\n",
        "            else:\n",
        "                print(\"‚ùå No data sources found for this knowledge base\")\n",
        "                return\n",
        "        \n",
        "        # List ingestion jobs\n",
        "        ingestion_jobs = bedrock_agent_client.list_ingestion_jobs(\n",
        "            knowledgeBaseId=kb_id,\n",
        "            dataSourceId=data_source_id,\n",
        "            maxResults=5\n",
        "        )\n",
        "        \n",
        "        print(f\"üìã Recent ingestion jobs for Knowledge Base {kb_id}:\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        for i, job in enumerate(ingestion_jobs['ingestionJobSummaries'][:3]):\n",
        "            status_emoji = {\n",
        "                'COMPLETE': '‚úÖ',\n",
        "                'IN_PROGRESS': '‚è≥', \n",
        "                'FAILED': '‚ùå',\n",
        "                'STARTING': 'üîÑ'\n",
        "            }.get(job['status'], '‚ùì')\n",
        "            \n",
        "            print(f\"{i+1}. Job ID: {job['ingestionJobId']}\")\n",
        "            print(f\"   Status: {status_emoji} {job['status']}\")\n",
        "            print(f\"   Started: {job['startedAt']}\")\n",
        "            print(f\"   Updated: {job['updatedAt']}\")\n",
        "            print()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error checking ingestion status: {str(e)}\")\n",
        "\n",
        "# Check current ingestion status\n",
        "print(\"üîç Checking current ingestion job status...\")\n",
        "check_ingestion_status(kb_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Checking and fixing IAM service role permissions...\n",
            "Service Role: AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "Service Role ARN: arn:aws:iam::533267284022:role/AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "‚úÖ Created enhanced policy: EnhancedBedrockRedshiftPolicy-0205647\n",
            "‚úÖ Attached enhanced policy to role: AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "\\nüîÑ Waiting 30 seconds for IAM changes to propagate...\n"
          ]
        }
      ],
      "source": [
        "# Fix the IAM service role permissions and retry ingestion\n",
        "print(\"üîß Checking and fixing IAM service role permissions...\")\n",
        "\n",
        "# Get the current service role\n",
        "bedrock_role_name = knowledge_base.bedrock_kb_execution_role_name\n",
        "role_arn = f\"arn:aws:iam::{account_id}:role/{bedrock_role_name}\"\n",
        "\n",
        "print(f\"Service Role: {bedrock_role_name}\")\n",
        "print(f\"Service Role ARN: {role_arn}\")\n",
        "\n",
        "# Create a comprehensive policy that matches AWS documentation requirements\n",
        "enhanced_policy_document = {\n",
        "    \"Version\": \"2012-10-17\",\n",
        "    \"Statement\": [\n",
        "        {\n",
        "            \"Sid\": \"RedshiftDataAPIStatementPermissions\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"redshift-data:GetStatementResult\",\n",
        "                \"redshift-data:DescribeStatement\", \n",
        "                \"redshift-data:CancelStatement\"\n",
        "            ],\n",
        "            \"Resource\": \"*\",\n",
        "            \"Condition\": {\n",
        "                \"StringEquals\": {\n",
        "                    \"redshift-data:statement-owner-iam-userid\": \"${aws:userid}\"\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"RedshiftDataAPIExecutePermissions\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"redshift-data:ExecuteStatement\"\n",
        "            ],\n",
        "            \"Resource\": [\n",
        "                workgroup_arn\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"RedshiftServerlessGetCredentials\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": \"redshift-serverless:GetCredentials\",\n",
        "            \"Resource\": [\n",
        "                workgroup_arn\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"SqlWorkbenchAccess\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"sqlworkbench:GetSqlRecommendations\",\n",
        "                \"sqlworkbench:PutSqlGenerationContext\",\n",
        "                \"sqlworkbench:GetSqlGenerationContext\", \n",
        "                \"sqlworkbench:DeleteSqlGenerationContext\"\n",
        "            ],\n",
        "            \"Resource\": \"*\"\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"BedrockAccess\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"bedrock:GenerateQuery\",\n",
        "                \"bedrock:InvokeModel\",\n",
        "                \"bedrock:Retrieve\",\n",
        "                \"bedrock:RetrieveAndGenerate\"\n",
        "            ],\n",
        "            \"Resource\": \"*\"\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"LoggingPermissions\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"logs:CreateLogGroup\",\n",
        "                \"logs:CreateLogStream\",\n",
        "                \"logs:PutLogEvents\"\n",
        "            ],\n",
        "            \"Resource\": \"arn:aws:logs:*:*:*\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create and attach the enhanced policy\n",
        "enhanced_policy_name = f'EnhancedBedrockRedshiftPolicy-{suffix}'\n",
        "\n",
        "try:\n",
        "    # Create the enhanced policy\n",
        "    enhanced_policy_response = iam_client.create_policy(\n",
        "        PolicyName=enhanced_policy_name,\n",
        "        PolicyDocument=json.dumps(enhanced_policy_document),\n",
        "        Description='Enhanced policy for Bedrock Knowledge Base with Redshift access'\n",
        "    )\n",
        "    enhanced_policy_arn = enhanced_policy_response['Policy']['Arn']\n",
        "    print(f\"‚úÖ Created enhanced policy: {enhanced_policy_name}\")\n",
        "    \n",
        "    # Attach the enhanced policy to the role\n",
        "    iam_client.attach_role_policy(\n",
        "        RoleName=bedrock_role_name,\n",
        "        PolicyArn=enhanced_policy_arn\n",
        "    )\n",
        "    print(f\"‚úÖ Attached enhanced policy to role: {bedrock_role_name}\")\n",
        "    \n",
        "except iam_client.exceptions.EntityAlreadyExistsException:\n",
        "    print(f\"‚ÑπÔ∏è Enhanced policy {enhanced_policy_name} already exists\")\n",
        "    enhanced_policy_arn = f\"arn:aws:iam::{account_id}:policy/{enhanced_policy_name}\"\n",
        "    try:\n",
        "        iam_client.attach_role_policy(\n",
        "            RoleName=bedrock_role_name,\n",
        "            PolicyArn=enhanced_policy_arn\n",
        "        )\n",
        "        print(f\"‚úÖ Attached existing enhanced policy to role\")\n",
        "    except iam_client.exceptions.EntityAlreadyExistsException:\n",
        "        print(f\"‚ÑπÔ∏è Enhanced policy already attached to role\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating/attaching enhanced policy: {str(e)}\")\n",
        "\n",
        "print(\"\\\\nüîÑ Waiting 30 seconds for IAM changes to propagate...\")\n",
        "import time\n",
        "time.sleep(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting ingestion job with enhanced permissions...\n",
            "üìã Using data source ID: EOZFBUZD6F\n",
            "‚úÖ Ingestion job started successfully!\n",
            "üìä Job ID: JKTRBFLDGT\n",
            "üìä Initial Status: STARTING\n",
            "\\n‚è≥ Monitoring ingestion job progress...\n",
            "Attempt 1/20: ‚è≥ Status: IN_PROGRESS\n",
            "   ‚è≥ Waiting 15 seconds before next check...\n",
            "Attempt 2/20: ‚ùå Status: FAILED\n",
            "\\n‚ùå Ingestion job failed!\n"
          ]
        }
      ],
      "source": [
        "# Start ingestion job directly using Bedrock Agent client\n",
        "print(\"üöÄ Starting ingestion job with enhanced permissions...\")\n",
        "\n",
        "try:\n",
        "    # Get the data source ID\n",
        "    data_sources = bedrock_agent_client.list_data_sources(\n",
        "        knowledgeBaseId=kb_id,\n",
        "        maxResults=10\n",
        "    )\n",
        "    \n",
        "    if not data_sources['dataSourceSummaries']:\n",
        "        print(\"‚ùå No data sources found for this knowledge base\")\n",
        "    else:\n",
        "        data_source_id = data_sources['dataSourceSummaries'][0]['dataSourceId']\n",
        "        print(f\"üìã Using data source ID: {data_source_id}\")\n",
        "        \n",
        "        # Start the ingestion job directly\n",
        "        start_job_response = bedrock_agent_client.start_ingestion_job(\n",
        "            knowledgeBaseId=kb_id,\n",
        "            dataSourceId=data_source_id\n",
        "        )\n",
        "        \n",
        "        job = start_job_response[\"ingestionJob\"]\n",
        "        print(f\"‚úÖ Ingestion job started successfully!\")\n",
        "        print(f\"üìä Job ID: {job['ingestionJobId']}\")\n",
        "        print(f\"üìä Initial Status: {job['status']}\")\n",
        "        \n",
        "        # Monitor the job status\n",
        "        job_id = job['ingestionJobId']\n",
        "        max_attempts = 20\n",
        "        wait_time = 15\n",
        "        \n",
        "        print(f\"\\\\n‚è≥ Monitoring ingestion job progress...\")\n",
        "        \n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                get_job_response = bedrock_agent_client.get_ingestion_job(\n",
        "                    knowledgeBaseId=kb_id,\n",
        "                    dataSourceId=data_source_id,\n",
        "                    ingestionJobId=job_id\n",
        "                )\n",
        "                \n",
        "                current_job = get_job_response[\"ingestionJob\"]\n",
        "                status = current_job['status']\n",
        "                \n",
        "                status_emoji = {\n",
        "                    'COMPLETE': '‚úÖ',\n",
        "                    'IN_PROGRESS': '‚è≥',\n",
        "                    'STARTING': 'üîÑ', \n",
        "                    'FAILED': '‚ùå',\n",
        "                    'STOPPED': '‚èπÔ∏è'\n",
        "                }.get(status, '‚ùì')\n",
        "                \n",
        "                print(f\"Attempt {attempt + 1}/{max_attempts}: {status_emoji} Status: {status}\")\n",
        "                \n",
        "                if status == 'COMPLETE':\n",
        "                    print(\"\\\\nüéâ Ingestion job completed successfully!\")\n",
        "                    print(\"üéØ Knowledge Base is now ready to answer queries!\")\n",
        "                    break\n",
        "                elif status == 'FAILED':\n",
        "                    print(\"\\\\n‚ùå Ingestion job failed!\")\n",
        "                    if 'failureReasons' in current_job:\n",
        "                        print(f\"Failure reasons: {current_job['failureReasons']}\")\n",
        "                    if 'statistics' in current_job:\n",
        "                        print(f\"Statistics: {current_job['statistics']}\")\n",
        "                    break\n",
        "                elif status == 'STOPPED':\n",
        "                    print(\"\\\\n‚èπÔ∏è Ingestion job was stopped!\")\n",
        "                    break\n",
        "                elif status in ['IN_PROGRESS', 'STARTING']:\n",
        "                    if attempt < max_attempts - 1:\n",
        "                        print(f\"   ‚è≥ Waiting {wait_time} seconds before next check...\")\n",
        "                        time.sleep(wait_time)\n",
        "                    else:\n",
        "                        print(\"\\\\n‚è∞ Timeout reached. Job may still be running in background.\")\n",
        "                        print(\"Use the check_ingestion_status function to monitor progress.\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error checking job status: {str(e)}\")\n",
        "                if attempt < max_attempts - 1:\n",
        "                    print(f\"   üîÑ Retrying in {wait_time} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(\"\\\\n‚ùå Maximum retries reached for status check\")\n",
        "                    break\n",
        "                \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error starting ingestion job: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Note: Use test_knowledge_base() function after ingestion completes\n",
            "Example: test_knowledge_base()\n"
          ]
        }
      ],
      "source": [
        "# Test the Knowledge Base once ingestion is successful\n",
        "def test_knowledge_base():\n",
        "    \"\"\"Test the Knowledge Base with sample queries\"\"\"\n",
        "    print(\"üß™ Testing Knowledge Base functionality...\")\n",
        "    \n",
        "    # Test queries\n",
        "    test_queries = [\n",
        "        \"How many orders are there in total?\",\n",
        "        \"What is the average order value?\", \n",
        "        \"Which payment method is most popular?\",\n",
        "        \"How many different products have been ordered?\"\n",
        "    ]\n",
        "    \n",
        "    foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "    \n",
        "    for i, query in enumerate(test_queries, 1):\n",
        "        print(f\"\\\\nüîç Test Query {i}: {query}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        try:\n",
        "            # Test with retrieve_and_generate\n",
        "            response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
        "                input={\"text\": query},\n",
        "                retrieveAndGenerateConfiguration={\n",
        "                    \"type\": \"KNOWLEDGE_BASE\",\n",
        "                    \"knowledgeBaseConfiguration\": {\n",
        "                        'knowledgeBaseId': kb_id,\n",
        "                        \"modelArn\": f\"arn:aws:bedrock:{region}::foundation-model/{foundation_model}\",\n",
        "                        \"retrievalConfiguration\": {\n",
        "                            \"vectorSearchConfiguration\": {\n",
        "                                \"numberOfResults\": 5\n",
        "                            } \n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            print(f\"‚úÖ Response: {response['output']['text']}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {str(e)}\")\n",
        "            \n",
        "        print()\n",
        "\n",
        "# Note: Only run this after ingestion completes successfully\n",
        "print(\"üìù Note: Use test_knowledge_base() function after ingestion completes\")\n",
        "print(\"Example: test_knowledge_base()\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wait_for_statement(statement_id):\n",
        "    \"\"\"Wait for a Redshift Data API statement to complete\"\"\"\n",
        "    max_attempts = 30\n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            response = redshift_data_client.describe_statement(Id=statement_id)\n",
        "            status = response['Status']\n",
        "            if status == 'FINISHED':\n",
        "                return response\n",
        "            elif status == 'FAILED':\n",
        "                raise Exception(f\"Statement failed: {response.get('Error', 'Unknown error')}\")\n",
        "            elif status == 'CANCELLED':\n",
        "                raise Exception(\"Statement was cancelled\")\n",
        "            else:\n",
        "                print(f\"Statement status: {status}, waiting...\")\n",
        "                time.sleep(5)\n",
        "        except Exception as e:\n",
        "            if \"Statement failed\" in str(e) or \"cancelled\" in str(e):\n",
        "                raise\n",
        "            print(f\"Error checking statement status: {str(e)}, retrying...\")\n",
        "            time.sleep(5)\n",
        "    \n",
        "    raise Exception(\"Timeout waiting for statement to complete\")\n",
        "\n",
        "def run_redshift_statement(sql_statement):\n",
        "    \"\"\"Execute a SQL statement in Redshift\"\"\"\n",
        "    try:\n",
        "        response = redshift_data_client.execute_statement(\n",
        "            WorkgroupName=REDSHIFT_WORKGROUP,\n",
        "            Database=REDSHIFT_DATABASE,\n",
        "            Sql=sql_statement\n",
        "        )\n",
        "        statement_id = response['Id']\n",
        "        print(f\"Executing statement: {statement_id}\")\n",
        "        result = wait_for_statement(statement_id)\n",
        "        print(f\"Statement completed successfully\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing statement: {str(e)}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating table: orders\n",
            "Executing statement: 4d669868-ea32-4ce0-9180-c977ebc99514\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Created table: orders\n",
            "-------------\n",
            "Creating table: order_items\n",
            "Executing statement: ea378983-2fac-43f2-b3ce-9c8d016ef05d\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Created table: order_items\n",
            "-------------\n",
            "Creating table: payments\n",
            "Executing statement: a75c6c7a-9c38-4b72-8c74-82563692520b\n",
            "Statement status: SUBMITTED, waiting...\n",
            "Statement completed successfully\n",
            "Created table: payments\n",
            "-------------\n",
            "Creating table: reviews\n",
            "Executing statement: e227ecbd-4eb6-4a0b-a9d9-f41049822560\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Created table: reviews\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# Create tables in Redshift\n",
        "def create_tables():\n",
        "    \"\"\"Create all necessary tables in Redshift\"\"\"\n",
        "    \n",
        "    # Orders table\n",
        "    orders_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS orders (\n",
        "        order_id VARCHAR(255) PRIMARY KEY,\n",
        "        customer_id VARCHAR(255),\n",
        "        order_total DECIMAL(10,2),\n",
        "        order_status VARCHAR(50),\n",
        "        payment_method VARCHAR(50),\n",
        "        shipping_address TEXT,\n",
        "        created_at TIMESTAMP,\n",
        "        updated_at TIMESTAMP\n",
        "    );\n",
        "    \"\"\"\n",
        "    \n",
        "    # Order Items table\n",
        "    order_items_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS order_items (\n",
        "        order_item_id VARCHAR(255) PRIMARY KEY,\n",
        "        order_id VARCHAR(255),\n",
        "        product_id VARCHAR(255),\n",
        "        quantity INTEGER,\n",
        "        price DECIMAL(10,2)\n",
        "    );\n",
        "    \"\"\"\n",
        "    \n",
        "    # Payments table\n",
        "    payments_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS payments (\n",
        "        payment_id VARCHAR(255) PRIMARY KEY,\n",
        "        order_id VARCHAR(255),\n",
        "        customer_id VARCHAR(255),\n",
        "        amount DECIMAL(10,2),\n",
        "        payment_method VARCHAR(50),\n",
        "        payment_status VARCHAR(50),\n",
        "        created_at DATE\n",
        "    );\n",
        "    \"\"\"\n",
        "    \n",
        "    # Reviews table\n",
        "    reviews_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS reviews (\n",
        "        review_id VARCHAR(255) PRIMARY KEY,\n",
        "        product_id VARCHAR(255),\n",
        "        customer_id VARCHAR(255),\n",
        "        rating INTEGER,\n",
        "        created_at DATE\n",
        "    );\n",
        "    \"\"\"\n",
        "    \n",
        "    tables = {\n",
        "        'orders': orders_sql,\n",
        "        'order_items': order_items_sql,\n",
        "        'payments': payments_sql,\n",
        "        'reviews': reviews_sql\n",
        "    }\n",
        "    \n",
        "    for table_name, sql in tables.items():\n",
        "        print(f\"Creating table: {table_name}\")\n",
        "        run_redshift_statement(sql)\n",
        "        print(f\"Created table: {table_name}\")\n",
        "        print(\"-------------\")\n",
        "\n",
        "# Create tables\n",
        "create_tables()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data into orders from orders.csv\n",
            "Executing statement: 0a23526b-aa4a-4d34-9458-a6d0f8c0433a\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Loaded data into orders\n",
            "Loading data into order_items from order_items.csv\n",
            "Executing statement: 84d54756-be6c-43c2-9bb8-4f47f5657c37\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Loaded data into order_items\n",
            "Loading data into payments from payments.csv\n",
            "Executing statement: 8e48260c-1a43-45c8-a626-4ca55af5866c\n",
            "Statement status: SUBMITTED, waiting...\n",
            "Statement completed successfully\n",
            "Loaded data into payments\n",
            "Loading data into reviews from reviews.csv\n",
            "Executing statement: 6ba97e21-83cd-4e22-843c-068c0bfeb201\n",
            "Statement status: SUBMITTED, waiting...\n",
            "Statement completed successfully\n",
            "Loaded data into reviews\n"
          ]
        }
      ],
      "source": [
        "# Load data from S3 into Redshift tables\n",
        "def load_data_from_s3():\n",
        "    \"\"\"Load data from S3 CSV files into Redshift tables\"\"\"\n",
        "    \n",
        "    tables_and_files = {\n",
        "        'orders': 'orders.csv',\n",
        "        'order_items': 'order_items.csv',\n",
        "        'payments': 'payments.csv',\n",
        "        'reviews': 'reviews.csv'\n",
        "    }\n",
        "    \n",
        "    for table_name, file_name in tables_and_files.items():\n",
        "        print(f\"Loading data into {table_name} from {file_name}\")\n",
        "        \n",
        "        copy_sql = f\"\"\"\n",
        "        COPY {table_name}\n",
        "        FROM 's3://{S3_BUCKET}/{file_name}'\n",
        "        IAM_ROLE '{redshift_role_arn}'\n",
        "        CSV\n",
        "        IGNOREHEADER 1\n",
        "        DELIMITER ','\n",
        "        REGION '{region}';\n",
        "        \"\"\"\n",
        "        \n",
        "        try:\n",
        "            run_redshift_statement(copy_sql)\n",
        "            print(f\"Loaded data into {table_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data into {table_name}: {str(e)}\")\n",
        "\n",
        "# Load data from S3\n",
        "load_data_from_s3()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 4 - Create Bedrock Knowledge Base with Redshift Data Source\n",
        "\n",
        "Now we'll create the Bedrock Knowledge Base configured to use our Redshift data as a structured data source.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Knowledge Base Configuration for IAM Role + Redshift Serverless WorkGroup\n",
        "kbConfigParam = {\n",
        "    \"type\": \"SQL\",\n",
        "    \"sqlKnowledgeBaseConfiguration\": {\n",
        "        \"type\": \"REDSHIFT\",\n",
        "        \"redshiftConfiguration\": {\n",
        "            \"storageConfigurations\": [{\n",
        "                \"type\": \"REDSHIFT\",\n",
        "                \"redshiftConfiguration\": {\n",
        "                    \"databaseName\": REDSHIFT_DATABASE\n",
        "                }\n",
        "            }],\n",
        "            \"queryEngineConfiguration\": {\n",
        "                \"type\": \"SERVERLESS\",\n",
        "                \"serverlessConfiguration\": {\n",
        "                    \"workgroupArn\": workgroup_arn,\n",
        "                    \"authConfiguration\": {\n",
        "                        \"type\": \"IAM\"\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 5 - Create Knowledge Base\n",
        "\n",
        "This step creates the Bedrock Knowledge Base configured for IAM Role + Redshift Serverless WorkGroup access pattern. The knowledge base will use IAM authentication to securely connect to your Redshift Serverless workgroup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-06-20 21:04:12,179] p74707 {credentials.py:1352} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Knowledge Base with IAM Role + Redshift Serverless WorkGroup access pattern...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-06-20 21:04:12,950] p74707 {credentials.py:1352} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================\n",
            "Step 1 - Creating Knowledge Base Execution Role (AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647) and Policies\n",
            "========================================================================================\n",
            "Step 2 - Creating Knowledge Base\n",
            "{ 'createdAt': datetime.datetime(2025, 6, 21, 4, 4, 13, 800686, tzinfo=tzutc()),\n",
            "  'description': 'Sample Structured KB',\n",
            "  'knowledgeBaseArn': 'arn:aws:bedrock:us-west-2:533267284022:knowledge-base/WCNCTKFCKY',\n",
            "  'knowledgeBaseConfiguration': { 'sqlKnowledgeBaseConfiguration': { 'redshiftConfiguration': { 'queryEngineConfiguration': { 'serverlessConfiguration': { 'authConfiguration': { 'type': 'IAM'},\n",
            "                                                                                                                                                           'workgroupArn': 'arn:aws:redshift-serverless:us-west-2:533267284022:workgroup/a68f79bf-ace3-46d7-abc3-3ee32e4998aa'},\n",
            "                                                                                                                              'type': 'SERVERLESS'},\n",
            "                                                                                                'storageConfigurations': [ { 'redshiftConfiguration': { 'databaseName': 'sds-ecommerce'},\n",
            "                                                                                                                             'type': 'REDSHIFT'}]},\n",
            "                                                                     'type': 'REDSHIFT'},\n",
            "                                  'type': 'SQL'},\n",
            "  'knowledgeBaseId': 'WCNCTKFCKY',\n",
            "  'name': 'bedrock-sample-structured-kb-0205647',\n",
            "  'roleArn': 'arn:aws:iam::533267284022:role/AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647',\n",
            "  'status': 'CREATING',\n",
            "  'updatedAt': datetime.datetime(2025, 6, 21, 4, 4, 13, 800686, tzinfo=tzutc())}\n",
            "Creating Data Sources aka query engine\n",
            "{ 'createdAt': datetime.datetime(2025, 6, 21, 4, 4, 13, 916661, tzinfo=tzutc()),\n",
            "  'dataSourceConfiguration': {'type': 'REDSHIFT_METADATA'},\n",
            "  'dataSourceId': 'EOZFBUZD6F',\n",
            "  'description': 'Query engine',\n",
            "  'knowledgeBaseId': 'WCNCTKFCKY',\n",
            "  'name': 'bedrock-sample-structured-kb-0205647-ds',\n",
            "  'status': 'AVAILABLE',\n",
            "  'updatedAt': datetime.datetime(2025, 6, 21, 4, 4, 13, 916661, tzinfo=tzutc())}\n",
            "========================================================================================\n",
            "Knowledge Base created successfully!\n",
            "'WCNCTKFCKY'\n",
            "Knowledge Base ID: WCNCTKFCKY\n"
          ]
        }
      ],
      "source": [
        "# Create the Knowledge Base using IAM Role + Redshift Serverless WorkGroup\n",
        "print(\"Creating Knowledge Base with IAM Role + Redshift Serverless WorkGroup access pattern...\")\n",
        "\n",
        "knowledge_base = BedrockStructuredKnowledgeBase(\n",
        "    kb_name=knowledge_base_name,\n",
        "    kb_description=knowledge_base_description,\n",
        "    workgroup_arn=workgroup_arn,\n",
        "    kbConfigParam=kbConfigParam,\n",
        "    generation_model=foundation_model,\n",
        "    suffix=suffix\n",
        ")\n",
        "\n",
        "print(\"Knowledge Base created successfully!\")\n",
        "print(f\"Knowledge Base ID: {knowledge_base.get_knowledge_base_id()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "### Grant Database Access to the Bedrock IAM Role\n",
        "\n",
        "For the IAM access pattern, you need to grant database access to the IAM role that Bedrock Knowledge Base uses for authentication. Execute the following SQL commands in your Redshift Query Editor to create the IAM user and grant appropriate permissions.\n",
        "\n",
        "For more detailed steps, please see the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-structured.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execute the following SQL commands in your Redshift Query Editor:\n",
            "======================================================================\n",
            "CREATE USER \"IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\" WITH PASSWORD DISABLE;\n",
            "GRANT USAGE ON SCHEMA public TO \"IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\";\n",
            "GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\";\n",
            "======================================================================\n",
            "\\nThese commands will:\n",
            "1. Create an IAM-based database user for the Bedrock execution role\n",
            "2. Grant USAGE permission on the public schema\n",
            "3. Grant SELECT permission on all tables in the public schema\n",
            "\\nNote: Adjust the schema and table permissions based on your specific requirements.\n"
          ]
        }
      ],
      "source": [
        "# Get the Bedrock execution role name\n",
        "bedrock_role_name = knowledge_base.bedrock_kb_execution_role_name\n",
        "\n",
        "# Display the SQL commands that need to be executed in Redshift Query Editor\n",
        "print(\"Execute the following SQL commands in your Redshift Query Editor:\")\n",
        "print(\"=\" * 70)\n",
        "print(f'CREATE USER \"IAMR:{bedrock_role_name}\" WITH PASSWORD DISABLE;')\n",
        "print(f'GRANT USAGE ON SCHEMA public TO \"IAMR:{bedrock_role_name}\";')\n",
        "print(f'GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"IAMR:{bedrock_role_name}\";')\n",
        "print(\"=\" * 70)\n",
        "print(\"\\\\nThese commands will:\")\n",
        "print(\"1. Create an IAM-based database user for the Bedrock execution role\")\n",
        "print(\"2. Grant USAGE permission on the public schema\")\n",
        "print(\"3. Grant SELECT permission on all tables in the public schema\")\n",
        "print(\"\\\\nNote: Adjust the schema and table permissions based on your specific requirements.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 6 - Start the ingestion job\n",
        "\n",
        "This step is to start the ingestion job to sync the datasources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "job  started successfully\n",
            "\n",
            "{ 'dataSourceId': 'EOZFBUZD6F',\n",
            "  'ingestionJobId': 'HOYDC9A0DV',\n",
            "  'knowledgeBaseId': 'WCNCTKFCKY',\n",
            "  'startedAt': datetime.datetime(2025, 6, 21, 4, 5, 39, 955341, tzinfo=tzutc()),\n",
            "  'status': 'FAILED',\n",
            "  'updatedAt': datetime.datetime(2025, 6, 21, 4, 5, 43, 167059, tzinfo=tzutc())}\n",
            "'WCNCTKFCKY'\n"
          ]
        }
      ],
      "source": [
        "# ensure that the kb is available\n",
        "time.sleep(60)\n",
        "# sync knowledge base\n",
        "knowledge_base.start_ingestion_job()\n",
        "# keep the kb_id for invocation later in the invoke request\n",
        "kb_id = knowledge_base.get_knowledge_base_id()\n",
        "# %store kb_id\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 7 - Test the Structured Knowledge Base\n",
        "\n",
        "Now the Knowledge Base is available we can test it out using the retrieve, retrieve_and_generate, and generate_query functions.\n",
        "\n",
        "- When you use **retrieve**, the response returns the result of the SQL query execution.\n",
        "- When you use **retrieve_and_generate**, the generated response is based on the result of the SQL query execution\n",
        "- When using the **generate_query** API, it transforms a natural language query into SQL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"How many orders are there in total?\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "### 7.1 - Using RetrieveAndGenerate API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValidationException",
          "evalue": "An error occurred (ValidationException) when calling the RetrieveAndGenerate operation: No metadata found. Please trigger an ingestion and try again. (Service: BedrockAgentRuntime, Status Code: 400, Request ID: 4846aded-b9bf-403d-ad46-00cc9556ccc6) (SDK Attempt Count: 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n",
            "\u001b[1;32m      1\u001b[0m foundation_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic.claude-3-sonnet-20240229-v1:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mbedrock_agent_runtime_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_and_generate\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n",
            "\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\n",
            "\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrieveAndGenerateConfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n",
            "\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKNOWLEDGE_BASE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mknowledgeBaseConfiguration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n",
            "\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mknowledgeBaseId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkb_id\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodelArn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marn:aws:bedrock:\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m::foundation-model/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoundation_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretrievalConfiguration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n",
            "\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvectorSearchConfiguration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n",
            "\u001b[1;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumberOfResults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n",
            "\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\n",
            "\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n",
            "\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n",
            "\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n",
            "\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
            "\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
            "\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages/botocore/client.py:598\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n",
            "\u001b[1;32m    595\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m    596\u001b[0m     )\n",
            "\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n",
            "\u001b[0;32m--> 598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n",
            "\u001b[1;32m    122\u001b[0m     hook()\n",
            "\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages/botocore/client.py:1061\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n",
            "\u001b[1;32m   1057\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n",
            "\u001b[1;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m   1059\u001b[0m     )\n",
            "\u001b[1;32m   1060\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n",
            "\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n",
            "\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
            "\n",
            "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the RetrieveAndGenerate operation: No metadata found. Please trigger an ingestion and try again. (Service: BedrockAgentRuntime, Status Code: 400, Request ID: 4846aded-b9bf-403d-ad46-00cc9556ccc6) (SDK Attempt Count: 1)"
          ]
        }
      ],
      "source": [
        "foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "\n",
        "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
        "    input={\n",
        "        \"text\": query\n",
        "    },\n",
        "    retrieveAndGenerateConfiguration={\n",
        "        \"type\": \"KNOWLEDGE_BASE\",\n",
        "        \"knowledgeBaseConfiguration\": {\n",
        "            'knowledgeBaseId': kb_id,\n",
        "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
        "            \"retrievalConfiguration\": {\n",
        "                \"vectorSearchConfiguration\": {\n",
        "                    \"numberOfResults\": 5\n",
        "                } \n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response['output']['text'], end='\\n'*2)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "### 7.2 - Using Retrieve API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_ret = bedrock_agent_runtime_client.retrieve(\n",
        "    knowledgeBaseId=kb_id, \n",
        "    nextToken='string',\n",
        "    retrievalConfiguration={\n",
        "        \"vectorSearchConfiguration\": {\n",
        "            \"numberOfResults\": 5,\n",
        "        } \n",
        "    },\n",
        "    retrievalQuery={\n",
        "        \"text\": query\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Function to extract retrieved results from Retrieve API response into a pandas dataframe.\n",
        "def response_print(retrieve_resp):\n",
        "    # Extract the retrievalResults list\n",
        "    retrieval_results = retrieve_resp['retrievalResults']\n",
        "\n",
        "    # Dictionary to store the extracted data\n",
        "    extracted_data = {}\n",
        "\n",
        "    # Iterate through each item in retrievalResults\n",
        "    for item in retrieval_results:\n",
        "        row = item['content']['row']\n",
        "        for col in row:\n",
        "            column_name = col['columnName']\n",
        "            column_value = col['columnValue']\n",
        "            \n",
        "            # If this column hasn't been seen before, create a new list for it\n",
        "            if column_name not in extracted_data:\n",
        "                extracted_data[column_name] = []\n",
        "            \n",
        "            # Append the value to the appropriate list\n",
        "            extracted_data[column_name].append(column_value)\n",
        "\n",
        "    # Create a DataFrame from the extracted data\n",
        "    df = pd.DataFrame(extracted_data)\n",
        "    return df\n",
        "    \n",
        "# Display the Retrieved results records\n",
        "df = response_print(response_ret)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "### 7.3 - Using Generate Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_response = bedrock_agent_runtime_client.generate_query(\n",
        "    queryGenerationInput={\n",
        "        \"text\": query,\n",
        "        \"type\": \"TEXT\"\n",
        "    },\n",
        "    transformationConfiguration={\n",
        "        \"mode\": \"TEXT_TO_SQL\",\n",
        "        \"textToSqlConfiguration\": {\n",
        "            \"type\": \"KNOWLEDGE_BASE\",\n",
        "            \"knowledgeBaseConfiguration\": {\n",
        "                \"knowledgeBaseArn\": knowledge_base.knowledge_base['knowledgeBaseArn']\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "generated_sql = query_response['queries'][0]['sql']\n",
        "generated_sql\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## 8 - Clean Up\n",
        "\n",
        "Please make sure to uncomment and run the below section to delete all the resources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete resources\n",
        "# print(\"=============================== Deleting resources ==============================\\n\")\n",
        "# knowledge_base.delete_kb(delete_iam_roles_and_policies=True)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 0 - Setup\n",
        "\n",
        "Before running the rest of this notebook, you'll need to run the cells below to ensure necessary libraries are installed and connect to Bedrock.\n",
        "\n",
        "Please ignore any pip dependency error (if you see any while installing libraries)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.38.36\n"
          ]
        }
      ],
      "source": [
        "# %pip install --upgrade pip --quiet\n",
        "# %pip install -r ../requirements.txt --no-deps --quiet\n",
        "# %pip install -r ../requirements.txt --upgrade --quiet\n",
        "# %pip install --upgrade boto3\n",
        "import boto3\n",
        "print(boto3.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<script>Jupyter.notebook.kernel.restart()</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# restart kernel\n",
        "from IPython.core.display import HTML\n",
        "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "This code is part of the setup and used to:\n",
        "- Add the parent directory to the python system path\n",
        "- Imports a custom module (BedrockStructuredKnowledgeBase) from utils necessary for later executions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/Users/manojs/Documents/Code/samples/05-agentic-rag/2-unstructure-structured-rag_agent', '/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python312.zip', '/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12', '/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/lib-dynload', '', '/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages', '/Users/manojs/Documents/Code/samples/05-agentic-rag']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "current_path = Path().resolve()\n",
        "current_path = current_path.parent\n",
        "\n",
        "if str(current_path) not in sys.path:\n",
        "    sys.path.append(str(current_path))\n",
        "\n",
        "# Print sys.path to verify\n",
        "print(sys.path)\n",
        "\n",
        "from utils.structured_knowledge_base import BedrockStructuredKnowledgeBase\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "Setup and initialize boto3 clients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('us-west-2', '533267284022')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s3_client = boto3.client('s3')\n",
        "sts_client = boto3.client('sts')\n",
        "session = boto3.session.Session(region_name='us-west-2')\n",
        "region = session.region_name\n",
        "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
        "bedrock_agent_client = boto3.client('bedrock-agent')\n",
        "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime')\n",
        "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "region, account_id\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "Initialize and configure the knowledge base name and the foundational model. This foundational model will be used to generate the natural language response based on the records received from the structured data store.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Get the current timestamp\n",
        "current_time = time.time()\n",
        "\n",
        "# Format the timestamp as a string\n",
        "timestamp_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(current_time))[-7:]\n",
        "# Create the suffix using the timestamp\n",
        "suffix = f\"{timestamp_str}\"\n",
        "\n",
        "knowledge_base_name = f\"bedrock-sample-structured-kb-{suffix}\"\n",
        "knowledge_base_description = \"Sample Structured KB\"\n",
        "\n",
        "foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "Amazon Bedrock Knowledge Bases uses a service role to connect knowledge bases to structured data stores, retrieve data from these data stores, and generate SQL queries based on user queries and the structure of the data stores. \n",
        "\n",
        "This notebook demonstrates the **IAM Role + Redshift Serverless WorkGroup** access pattern, which provides secure access to your Redshift Serverless workgroup using IAM authentication.\n",
        "\n",
        "**Required Configuration Variables:**\n",
        "- workgroup_id: Your Redshift Serverless workgroup identifier\n",
        "- redshiftDBName: Your database name within the workgroup\n",
        "\n",
        "The knowledge base configuration will use these parameters to establish the connection and perform necessary authentication using IAM roles.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1 - Set up Redshift Serverless Infrastructure\n",
        "\n",
        "This section will create the necessary Redshift Serverless components: namespace and workgroup. This infrastructure will host our structured data that the Knowledge Base will query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Redshift Namespace: sds-ecommerce-0205647\n",
            "Redshift Workgroup: sds-ecommerce-wg-0205647\n",
            "Database: sds-ecommerce\n",
            "S3 Bucket: sds-ecommerce-redshift-0205647\n"
          ]
        }
      ],
      "source": [
        "# Configuration for Redshift resources\n",
        "REDSHIFT_NAMESPACE = f'sds-ecommerce-{suffix}'\n",
        "REDSHIFT_WORKGROUP = f'sds-ecommerce-wg-{suffix}'\n",
        "REDSHIFT_DATABASE = f'sds-ecommerce'\n",
        "S3_BUCKET = f'sds-ecommerce-redshift-{suffix}'\n",
        "\n",
        "print(f\"Redshift Namespace: {REDSHIFT_NAMESPACE}\")\n",
        "print(f\"Redshift Workgroup: {REDSHIFT_WORKGROUP}\")\n",
        "print(f\"Database: {REDSHIFT_DATABASE}\")\n",
        "print(f\"S3 Bucket: {S3_BUCKET}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created role RedshiftS3AccessRole-0205647\n",
            "Redshift IAM Role ARN: arn:aws:iam::533267284022:role/RedshiftS3AccessRole-0205647\n",
            "Role RedshiftS3AccessRole-0205647 already exists\n",
            "Redshift IAM Role ARN: arn:aws:iam::533267284022:role/RedshiftS3AccessRole-0205647\n"
          ]
        }
      ],
      "source": [
        "def create_iam_role_for_redshift():\n",
        "    \"\"\"Create IAM role for Redshift to access S3\"\"\"\n",
        "    try:\n",
        "        # Get account ID\n",
        "        account_id = sts_client.get_caller_identity()['Account']\n",
        "        \n",
        "        # Create IAM role if it doesn't exist\n",
        "        role_name = f'RedshiftS3AccessRole-{suffix}'\n",
        "        try:\n",
        "            role_response = iam_client.get_role(RoleName=role_name)\n",
        "            print(f'Role {role_name} already exists')\n",
        "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
        "        except iam_client.exceptions.NoSuchEntityException:\n",
        "            trust_policy = {\n",
        "                \"Version\": \"2012-10-17\",\n",
        "                \"Statement\": [\n",
        "                    {\n",
        "                        \"Effect\": \"Allow\",\n",
        "                        \"Principal\": {\n",
        "                            \"Service\": \"redshift.amazonaws.com\"\n",
        "                        },\n",
        "                        \"Action\": \"sts:AssumeRole\"\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "            \n",
        "            iam_client.create_role(\n",
        "                RoleName=role_name,\n",
        "                AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
        "            )\n",
        "            \n",
        "            # Attach necessary policies\n",
        "            iam_client.attach_role_policy(\n",
        "                RoleName=role_name,\n",
        "                PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n",
        "            )\n",
        "            \n",
        "            print(f'Created role {role_name}')\n",
        "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f'Error creating IAM role: {str(e)}')\n",
        "        raise\n",
        "\n",
        "# Initialize additional clients\n",
        "import json\n",
        "import os\n",
        "iam_client = boto3.client('iam')\n",
        "redshift_client = boto3.client('redshift-serverless', region_name=region)\n",
        "redshift_data_client = boto3.client('redshift-data', region_name=region)\n",
        "\n",
        "redshift_role_arn = create_iam_role_for_redshift()\n",
        "print(f\"Redshift IAM Role ARN: {redshift_role_arn}\")\n",
        "\n",
        "redshift_role_arn = create_iam_role_for_redshift()\n",
        "print(f\"Redshift IAM Role ARN: {redshift_role_arn}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating namespace sds-ecommerce-0205647...\n",
            "Created namespace sds-ecommerce-0205647\n",
            "Waiting for namespace to be available...\n",
            "Namespace sds-ecommerce-0205647 is now available\n"
          ]
        }
      ],
      "source": [
        "def create_redshift_namespace():\n",
        "    \"\"\"Create Redshift Serverless namespace\"\"\"\n",
        "    try:\n",
        "        # Check if namespace already exists\n",
        "        try:\n",
        "            response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
        "            print(f'Namespace {REDSHIFT_NAMESPACE} already exists')\n",
        "            return response['namespace']\n",
        "        except redshift_client.exceptions.ResourceNotFoundException:\n",
        "            print(f'Creating namespace {REDSHIFT_NAMESPACE}...')\n",
        "        \n",
        "        # Create the namespace\n",
        "        response = redshift_client.create_namespace(\n",
        "            namespaceName=REDSHIFT_NAMESPACE,\n",
        "            adminUsername='admin',\n",
        "            adminUserPassword='TempPassword123!',  # Change this in production\n",
        "            dbName=REDSHIFT_DATABASE,\n",
        "            defaultIamRoleArn=redshift_role_arn,\n",
        "            iamRoles=[redshift_role_arn]\n",
        "        )\n",
        "        \n",
        "        print(f'Created namespace {REDSHIFT_NAMESPACE}')\n",
        "        \n",
        "        # Wait for namespace to be available\n",
        "        print('Waiting for namespace to be available...')\n",
        "        max_attempts = 30\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                namespace_response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
        "                status = namespace_response['namespace']['status']\n",
        "                if status == 'AVAILABLE':\n",
        "                    print(f'Namespace {REDSHIFT_NAMESPACE} is now available')\n",
        "                    return namespace_response['namespace']\n",
        "                else:\n",
        "                    print(f'Namespace status: {status}, waiting...')\n",
        "                    time.sleep(10)\n",
        "            except Exception as e:\n",
        "                print(f'Error checking namespace status: {str(e)}, retrying...')\n",
        "                time.sleep(10)\n",
        "        \n",
        "        print('Timeout waiting for namespace, but proceeding...')\n",
        "        return response['namespace']\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f'Error creating namespace: {str(e)}')\n",
        "        raise\n",
        "\n",
        "# Create namespace\n",
        "namespace = create_redshift_namespace()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating workgroup sds-ecommerce-wg-0205647...\n",
            "Created workgroup sds-ecommerce-wg-0205647\n",
            "Waiting for workgroup to be available...\n",
            "Workgroup status: CREATING, waiting...\n",
            "Workgroup status: CREATING, waiting...\n",
            "Workgroup status: CREATING, waiting...\n",
            "Workgroup status: CREATING, waiting...\n",
            "Workgroup sds-ecommerce-wg-0205647 is now available\n",
            "Workgroup ARN: arn:aws:redshift-serverless:us-west-2:533267284022:workgroup/a68f79bf-ace3-46d7-abc3-3ee32e4998aa\n"
          ]
        }
      ],
      "source": [
        "def create_redshift_workgroup():\n",
        "    \"\"\"Create Redshift Serverless workgroup\"\"\"\n",
        "    try:\n",
        "        # Check if workgroup already exists\n",
        "        try:\n",
        "            response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
        "            print(f'Workgroup {REDSHIFT_WORKGROUP} already exists')\n",
        "            return response['workgroup']\n",
        "        except redshift_client.exceptions.ResourceNotFoundException:\n",
        "            print(f'Creating workgroup {REDSHIFT_WORKGROUP}...')\n",
        "        \n",
        "        # Create the workgroup\n",
        "        response = redshift_client.create_workgroup(\n",
        "            workgroupName=REDSHIFT_WORKGROUP,\n",
        "            namespaceName=REDSHIFT_NAMESPACE,\n",
        "            baseCapacity=8,  # Minimum base capacity\n",
        "            enhancedVpcRouting=False,\n",
        "            publiclyAccessible=True,\n",
        "            configParameters=[\n",
        "                {\n",
        "                    'parameterKey': 'enable_user_activity_logging',\n",
        "                    'parameterValue': 'true'\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        print(f'Created workgroup {REDSHIFT_WORKGROUP}')\n",
        "        \n",
        "        # Wait for workgroup to be available\n",
        "        print('Waiting for workgroup to be available...')\n",
        "        max_attempts = 30\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                workgroup_response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
        "                status = workgroup_response['workgroup']['status']\n",
        "                if status == 'AVAILABLE':\n",
        "                    print(f'Workgroup {REDSHIFT_WORKGROUP} is now available')\n",
        "                    return workgroup_response['workgroup']\n",
        "                else:\n",
        "                    print(f'Workgroup status: {status}, waiting...')\n",
        "                    time.sleep(10)\n",
        "            except Exception as e:\n",
        "                print(f'Error checking workgroup status: {str(e)}, retrying...')\n",
        "                time.sleep(10)\n",
        "        \n",
        "        print('Timeout waiting for workgroup, but proceeding...')\n",
        "        return response['workgroup']\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f'Error creating workgroup: {str(e)}')\n",
        "        raise\n",
        "\n",
        "# Create workgroup\n",
        "workgroup = create_redshift_workgroup()\n",
        "workgroup_arn = workgroup['workgroupArn']\n",
        "print(f\"Workgroup ARN: {workgroup_arn}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2 - Create S3 Bucket and Load Sample Data\n",
        "\n",
        "We will create an S3 bucket to stage our sample e-commerce data before loading it into Redshift tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created bucket sds-ecommerce-redshift-0205647\n"
          ]
        }
      ],
      "source": [
        "def create_s3_bucket():\n",
        "    \"\"\"Create S3 bucket for data staging\"\"\"\n",
        "    try:\n",
        "        s3_client.head_bucket(Bucket=S3_BUCKET)\n",
        "        print(f'Bucket {S3_BUCKET} already exists')\n",
        "    except:\n",
        "        try:\n",
        "            if region == 'us-east-1':\n",
        "                s3_client.create_bucket(Bucket=S3_BUCKET)\n",
        "            else:\n",
        "                s3_client.create_bucket(\n",
        "                    Bucket=S3_BUCKET,\n",
        "                    CreateBucketConfiguration={'LocationConstraint': region}\n",
        "                )\n",
        "            print(f'Created bucket {S3_BUCKET}')\n",
        "        except Exception as e:\n",
        "            print(f'Error creating bucket: {str(e)}')\n",
        "            raise\n",
        "\n",
        "# Create S3 bucket\n",
        "create_s3_bucket()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading sample data files to S3...\n",
            "Uploaded orders.csv (1.8 MB) to S3\n",
            "Uploaded order_items.csv (1.3 MB) to S3\n",
            "Uploaded payments.csv (0.8 MB) to S3\n",
            "Uploaded reviews.csv (0.5 MB) to S3\n",
            "Successfully uploaded all 4 data files to S3\n"
          ]
        }
      ],
      "source": [
        "def upload_sample_data():\n",
        "    \"\"\"Upload sample CSV files to S3\"\"\"\n",
        "    data_files = ['orders.csv', 'order_items.csv', 'payments.csv', 'reviews.csv']\n",
        "    sds_directory = 'sample_data'\n",
        "    \n",
        "    print(\"Uploading sample data files to S3...\")\n",
        "    files_found = 0\n",
        "    \n",
        "    for file_name in data_files:\n",
        "        local_path = os.path.join(sds_directory, file_name)\n",
        "        if os.path.exists(local_path):\n",
        "            # Get file size for informational purposes\n",
        "            file_size = os.path.getsize(local_path)\n",
        "            file_size_mb = file_size / (1024 * 1024)\n",
        "            \n",
        "            s3_client.upload_file(local_path, S3_BUCKET, file_name)\n",
        "            print(f'Uploaded {file_name} ({file_size_mb:.1f} MB) to S3')\n",
        "            files_found += 1\n",
        "        else:\n",
        "            print(f'Warning: {local_path} not found')\n",
        "    \n",
        "    if files_found == len(data_files):\n",
        "        print(f\"Successfully uploaded all {files_found} data files to S3\")\n",
        "    else:\n",
        "        print(f\"Only {files_found} out of {len(data_files)} files were found and uploaded\")\n",
        "\n",
        "# Upload sample data\n",
        "upload_sample_data()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3 - Create Redshift Tables and Load Data\n",
        "\n",
        "Now we will create the database tables in Redshift and load our sample e-commerce data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Executing SQL commands to grant database access to Bedrock execution role...\n",
            "Creating user: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "Executing statement: cb83ac9a-aa81-41ad-8bde-91ceb92cfc0c\n",
            "Statement status: SUBMITTED, waiting...\n",
            "Statement status: STARTED, waiting...\n",
            "Statement status: STARTED, waiting...\n",
            "Error executing statement: Statement failed: ERROR: user \"IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\" already exists\n",
            "‚ÑπÔ∏è User already exists, continuing...\n",
            "Granting USAGE on schema public to: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "Executing statement: c948acbc-0201-440d-b2e1-fee3521a65ac\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "‚úÖ USAGE permission granted successfully!\n",
            "Granting SELECT on all tables to: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "Executing statement: 6c413ca2-d25d-41bb-ae99-d28f3506ee19\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "‚úÖ SELECT permissions granted successfully!\n",
            "\\nüéâ All database permissions have been granted to the Bedrock execution role!\n",
            "The Knowledge Base should now be able to access the database tables.\n"
          ]
        }
      ],
      "source": [
        "# Execute the required SQL commands to grant database access to Bedrock execution role\n",
        "print(\"üîß Executing SQL commands to grant database access to Bedrock execution role...\")\n",
        "\n",
        "bedrock_role_name = knowledge_base.bedrock_kb_execution_role_name\n",
        "\n",
        "# Step 1: Create the IAM user in Redshift database\n",
        "create_user_sql = f'CREATE USER \"IAMR:{bedrock_role_name}\" WITH PASSWORD DISABLE;'\n",
        "\n",
        "try:\n",
        "    print(f\"Creating user: IAMR:{bedrock_role_name}\")\n",
        "    run_redshift_statement(create_user_sql)\n",
        "    print(\"‚úÖ IAM user created successfully!\")\n",
        "except Exception as e:\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        print(\"‚ÑπÔ∏è User already exists, continuing...\")\n",
        "    else:\n",
        "        print(f\"‚ùå Error creating user: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Step 2: Grant USAGE permission on public schema\n",
        "grant_usage_sql = f'GRANT USAGE ON SCHEMA public TO \"IAMR:{bedrock_role_name}\";'\n",
        "\n",
        "try:\n",
        "    print(f\"Granting USAGE on schema public to: IAMR:{bedrock_role_name}\")\n",
        "    run_redshift_statement(grant_usage_sql)\n",
        "    print(\"‚úÖ USAGE permission granted successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error granting USAGE permission: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Step 3: Grant SELECT permission on all tables in public schema\n",
        "grant_select_sql = f'GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"IAMR:{bedrock_role_name}\";'\n",
        "\n",
        "try:\n",
        "    print(f\"Granting SELECT on all tables to: IAMR:{bedrock_role_name}\")\n",
        "    run_redshift_statement(grant_select_sql)\n",
        "    print(\"‚úÖ SELECT permissions granted successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error granting SELECT permissions: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "print(\"\\\\nüéâ All database permissions have been granted to the Bedrock execution role!\")\n",
        "print(\"The Knowledge Base should now be able to access the database tables.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Retrying the ingestion job with proper permissions...\n",
            "job  started successfully\n",
            "\n",
            "{ 'dataSourceId': 'EOZFBUZD6F',\n",
            "  'ingestionJobId': 'YBQXYQU4WL',\n",
            "  'knowledgeBaseId': 'WCNCTKFCKY',\n",
            "  'startedAt': datetime.datetime(2025, 6, 21, 7, 12, 41, 597571, tzinfo=tzutc()),\n",
            "  'status': 'FAILED',\n",
            "  'updatedAt': datetime.datetime(2025, 6, 21, 7, 12, 44, 860633, tzinfo=tzutc())}\n",
            "‚úÖ New ingestion job started successfully!\n",
            "Ingestion Job Response: None\n",
            "‚è≥ Waiting for ingestion job to complete...\n",
            "‚ùå Error starting ingestion job: 'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Check the ingestion job status\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mingestionJobId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     17\u001b[0m job_status \u001b[38;5;241m=\u001b[39m bedrock_agent_client\u001b[38;5;241m.\u001b[39mget_ingestion_job(\n\u001b[1;32m     18\u001b[0m     knowledgeBaseId\u001b[38;5;241m=\u001b[39mkb_id,\n\u001b[1;32m     19\u001b[0m     dataSourceId\u001b[38;5;241m=\u001b[39mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataSourceId\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     20\u001b[0m     ingestionJobId\u001b[38;5;241m=\u001b[39mjob_id\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnüìä Ingestion Job Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_status[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mingestionJob\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# Retry the ingestion job now that permissions are fixed\n",
        "print(\"üîÑ Retrying the ingestion job with proper permissions...\")\n",
        "\n",
        "try:\n",
        "    # Start a new ingestion job\n",
        "    response = knowledge_base.start_ingestion_job()\n",
        "    print(\"‚úÖ New ingestion job started successfully!\")\n",
        "    print(f\"Ingestion Job Response: {response}\")\n",
        "    \n",
        "    # Wait a bit for the job to process\n",
        "    import time\n",
        "    print(\"‚è≥ Waiting for ingestion job to complete...\")\n",
        "    time.sleep(30)\n",
        "    \n",
        "    # Check the ingestion job status\n",
        "    job_id = response['ingestionJobId']\n",
        "    job_status = bedrock_agent_client.get_ingestion_job(\n",
        "        knowledgeBaseId=kb_id,\n",
        "        dataSourceId=response['dataSourceId'],\n",
        "        ingestionJobId=job_id\n",
        "    )\n",
        "    \n",
        "    print(f\"\\\\nüìä Ingestion Job Status: {job_status['ingestionJob']['status']}\")\n",
        "    if job_status['ingestionJob']['status'] == 'COMPLETE':\n",
        "        print(\"üéâ Ingestion job completed successfully!\")\n",
        "    elif job_status['ingestionJob']['status'] == 'IN_PROGRESS':\n",
        "        print(\"‚è≥ Ingestion job is still in progress. Wait a few more minutes.\")\n",
        "    else:\n",
        "        print(f\"‚ùå Ingestion job status: {job_status['ingestionJob']['status']}\")\n",
        "        if 'failureReasons' in job_status['ingestionJob']:\n",
        "            print(f\"Failure reasons: {job_status['ingestionJob']['failureReasons']}\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error starting ingestion job: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to check ingestion job status\n",
        "def check_ingestion_status(kb_id, data_source_id=None, job_id=None):\n",
        "    \"\"\"Check the status of ingestion jobs for a knowledge base\"\"\"\n",
        "    try:\n",
        "        # If no data_source_id provided, get the first one\n",
        "        if not data_source_id:\n",
        "            data_sources = bedrock_agent_client.list_data_sources(\n",
        "                knowledgeBaseId=kb_id,\n",
        "                maxResults=10\n",
        "            )\n",
        "            if data_sources['dataSourceSummaries']:\n",
        "                data_source_id = data_sources['dataSourceSummaries'][0]['dataSourceId']\n",
        "            else:\n",
        "                print(\"‚ùå No data sources found for this knowledge base\")\n",
        "                return\n",
        "        \n",
        "        # List ingestion jobs\n",
        "        ingestion_jobs = bedrock_agent_client.list_ingestion_jobs(\n",
        "            knowledgeBaseId=kb_id,\n",
        "            dataSourceId=data_source_id,\n",
        "            maxResults=5\n",
        "        )\n",
        "        \n",
        "        print(f\"üìã Recent ingestion jobs for Knowledge Base {kb_id}:\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        for i, job in enumerate(ingestion_jobs['ingestionJobSummaries'][:3]):\n",
        "            status_emoji = {\n",
        "                'COMPLETE': '‚úÖ',\n",
        "                'IN_PROGRESS': '‚è≥', \n",
        "                'FAILED': '‚ùå',\n",
        "                'STARTING': 'üîÑ'\n",
        "            }.get(job['status'], '‚ùì')\n",
        "            \n",
        "            print(f\"{i+1}. Job ID: {job['ingestionJobId']}\")\n",
        "            print(f\"   Status: {status_emoji} {job['status']}\")\n",
        "            print(f\"   Started: {job['startedAt']}\")\n",
        "            print(f\"   Updated: {job['updatedAt']}\")\n",
        "            print()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error checking ingestion status: {str(e)}\")\n",
        "\n",
        "# Check current ingestion status\n",
        "print(\"üîç Checking current ingestion job status...\")\n",
        "check_ingestion_status(kb_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Checking and fixing IAM service role permissions...\n",
            "Service Role: AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "Service Role ARN: arn:aws:iam::533267284022:role/AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "‚úÖ Created enhanced policy: EnhancedBedrockRedshiftPolicy-0205647\n",
            "‚úÖ Attached enhanced policy to role: AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\n",
            "\\nüîÑ Waiting 30 seconds for IAM changes to propagate...\n"
          ]
        }
      ],
      "source": [
        "# Fix the IAM service role permissions and retry ingestion\n",
        "print(\"üîß Checking and fixing IAM service role permissions...\")\n",
        "\n",
        "# Get the current service role\n",
        "bedrock_role_name = knowledge_base.bedrock_kb_execution_role_name\n",
        "role_arn = f\"arn:aws:iam::{account_id}:role/{bedrock_role_name}\"\n",
        "\n",
        "print(f\"Service Role: {bedrock_role_name}\")\n",
        "print(f\"Service Role ARN: {role_arn}\")\n",
        "\n",
        "# Create a comprehensive policy that matches AWS documentation requirements\n",
        "enhanced_policy_document = {\n",
        "    \"Version\": \"2012-10-17\",\n",
        "    \"Statement\": [\n",
        "        {\n",
        "            \"Sid\": \"RedshiftDataAPIStatementPermissions\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"redshift-data:GetStatementResult\",\n",
        "                \"redshift-data:DescribeStatement\", \n",
        "                \"redshift-data:CancelStatement\"\n",
        "            ],\n",
        "            \"Resource\": \"*\",\n",
        "            \"Condition\": {\n",
        "                \"StringEquals\": {\n",
        "                    \"redshift-data:statement-owner-iam-userid\": \"${aws:userid}\"\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"RedshiftDataAPIExecutePermissions\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"redshift-data:ExecuteStatement\"\n",
        "            ],\n",
        "            \"Resource\": [\n",
        "                workgroup_arn\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"RedshiftServerlessGetCredentials\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": \"redshift-serverless:GetCredentials\",\n",
        "            \"Resource\": [\n",
        "                workgroup_arn\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"SqlWorkbenchAccess\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"sqlworkbench:GetSqlRecommendations\",\n",
        "                \"sqlworkbench:PutSqlGenerationContext\",\n",
        "                \"sqlworkbench:GetSqlGenerationContext\", \n",
        "                \"sqlworkbench:DeleteSqlGenerationContext\"\n",
        "            ],\n",
        "            \"Resource\": \"*\"\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"BedrockAccess\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"bedrock:GenerateQuery\",\n",
        "                \"bedrock:InvokeModel\",\n",
        "                \"bedrock:Retrieve\",\n",
        "                \"bedrock:RetrieveAndGenerate\"\n",
        "            ],\n",
        "            \"Resource\": \"*\"\n",
        "        },\n",
        "        {\n",
        "            \"Sid\": \"LoggingPermissions\",\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"logs:CreateLogGroup\",\n",
        "                \"logs:CreateLogStream\",\n",
        "                \"logs:PutLogEvents\"\n",
        "            ],\n",
        "            \"Resource\": \"arn:aws:logs:*:*:*\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create and attach the enhanced policy\n",
        "enhanced_policy_name = f'EnhancedBedrockRedshiftPolicy-{suffix}'\n",
        "\n",
        "try:\n",
        "    # Create the enhanced policy\n",
        "    enhanced_policy_response = iam_client.create_policy(\n",
        "        PolicyName=enhanced_policy_name,\n",
        "        PolicyDocument=json.dumps(enhanced_policy_document),\n",
        "        Description='Enhanced policy for Bedrock Knowledge Base with Redshift access'\n",
        "    )\n",
        "    enhanced_policy_arn = enhanced_policy_response['Policy']['Arn']\n",
        "    print(f\"‚úÖ Created enhanced policy: {enhanced_policy_name}\")\n",
        "    \n",
        "    # Attach the enhanced policy to the role\n",
        "    iam_client.attach_role_policy(\n",
        "        RoleName=bedrock_role_name,\n",
        "        PolicyArn=enhanced_policy_arn\n",
        "    )\n",
        "    print(f\"‚úÖ Attached enhanced policy to role: {bedrock_role_name}\")\n",
        "    \n",
        "except iam_client.exceptions.EntityAlreadyExistsException:\n",
        "    print(f\"‚ÑπÔ∏è Enhanced policy {enhanced_policy_name} already exists\")\n",
        "    enhanced_policy_arn = f\"arn:aws:iam::{account_id}:policy/{enhanced_policy_name}\"\n",
        "    try:\n",
        "        iam_client.attach_role_policy(\n",
        "            RoleName=bedrock_role_name,\n",
        "            PolicyArn=enhanced_policy_arn\n",
        "        )\n",
        "        print(f\"‚úÖ Attached existing enhanced policy to role\")\n",
        "    except iam_client.exceptions.EntityAlreadyExistsException:\n",
        "        print(f\"‚ÑπÔ∏è Enhanced policy already attached to role\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating/attaching enhanced policy: {str(e)}\")\n",
        "\n",
        "print(\"\\\\nüîÑ Waiting 30 seconds for IAM changes to propagate...\")\n",
        "import time\n",
        "time.sleep(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting ingestion job with enhanced permissions...\n",
            "üìã Using data source ID: EOZFBUZD6F\n",
            "‚úÖ Ingestion job started successfully!\n",
            "üìä Job ID: JKTRBFLDGT\n",
            "üìä Initial Status: STARTING\n",
            "\\n‚è≥ Monitoring ingestion job progress...\n",
            "Attempt 1/20: ‚è≥ Status: IN_PROGRESS\n",
            "   ‚è≥ Waiting 15 seconds before next check...\n",
            "Attempt 2/20: ‚ùå Status: FAILED\n",
            "\\n‚ùå Ingestion job failed!\n"
          ]
        }
      ],
      "source": [
        "# Start ingestion job directly using Bedrock Agent client\n",
        "print(\"üöÄ Starting ingestion job with enhanced permissions...\")\n",
        "\n",
        "try:\n",
        "    # Get the data source ID\n",
        "    data_sources = bedrock_agent_client.list_data_sources(\n",
        "        knowledgeBaseId=kb_id,\n",
        "        maxResults=10\n",
        "    )\n",
        "    \n",
        "    if not data_sources['dataSourceSummaries']:\n",
        "        print(\"‚ùå No data sources found for this knowledge base\")\n",
        "    else:\n",
        "        data_source_id = data_sources['dataSourceSummaries'][0]['dataSourceId']\n",
        "        print(f\"üìã Using data source ID: {data_source_id}\")\n",
        "        \n",
        "        # Start the ingestion job directly\n",
        "        start_job_response = bedrock_agent_client.start_ingestion_job(\n",
        "            knowledgeBaseId=kb_id,\n",
        "            dataSourceId=data_source_id\n",
        "        )\n",
        "        \n",
        "        job = start_job_response[\"ingestionJob\"]\n",
        "        print(f\"‚úÖ Ingestion job started successfully!\")\n",
        "        print(f\"üìä Job ID: {job['ingestionJobId']}\")\n",
        "        print(f\"üìä Initial Status: {job['status']}\")\n",
        "        \n",
        "        # Monitor the job status\n",
        "        job_id = job['ingestionJobId']\n",
        "        max_attempts = 20\n",
        "        wait_time = 15\n",
        "        \n",
        "        print(f\"\\\\n‚è≥ Monitoring ingestion job progress...\")\n",
        "        \n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                get_job_response = bedrock_agent_client.get_ingestion_job(\n",
        "                    knowledgeBaseId=kb_id,\n",
        "                    dataSourceId=data_source_id,\n",
        "                    ingestionJobId=job_id\n",
        "                )\n",
        "                \n",
        "                current_job = get_job_response[\"ingestionJob\"]\n",
        "                status = current_job['status']\n",
        "                \n",
        "                status_emoji = {\n",
        "                    'COMPLETE': '‚úÖ',\n",
        "                    'IN_PROGRESS': '‚è≥',\n",
        "                    'STARTING': 'üîÑ', \n",
        "                    'FAILED': '‚ùå',\n",
        "                    'STOPPED': '‚èπÔ∏è'\n",
        "                }.get(status, '‚ùì')\n",
        "                \n",
        "                print(f\"Attempt {attempt + 1}/{max_attempts}: {status_emoji} Status: {status}\")\n",
        "                \n",
        "                if status == 'COMPLETE':\n",
        "                    print(\"\\\\nüéâ Ingestion job completed successfully!\")\n",
        "                    print(\"üéØ Knowledge Base is now ready to answer queries!\")\n",
        "                    break\n",
        "                elif status == 'FAILED':\n",
        "                    print(\"\\\\n‚ùå Ingestion job failed!\")\n",
        "                    if 'failureReasons' in current_job:\n",
        "                        print(f\"Failure reasons: {current_job['failureReasons']}\")\n",
        "                    if 'statistics' in current_job:\n",
        "                        print(f\"Statistics: {current_job['statistics']}\")\n",
        "                    break\n",
        "                elif status == 'STOPPED':\n",
        "                    print(\"\\\\n‚èπÔ∏è Ingestion job was stopped!\")\n",
        "                    break\n",
        "                elif status in ['IN_PROGRESS', 'STARTING']:\n",
        "                    if attempt < max_attempts - 1:\n",
        "                        print(f\"   ‚è≥ Waiting {wait_time} seconds before next check...\")\n",
        "                        time.sleep(wait_time)\n",
        "                    else:\n",
        "                        print(\"\\\\n‚è∞ Timeout reached. Job may still be running in background.\")\n",
        "                        print(\"Use the check_ingestion_status function to monitor progress.\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error checking job status: {str(e)}\")\n",
        "                if attempt < max_attempts - 1:\n",
        "                    print(f\"   üîÑ Retrying in {wait_time} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(\"\\\\n‚ùå Maximum retries reached for status check\")\n",
        "                    break\n",
        "                \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error starting ingestion job: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Note: Use test_knowledge_base() function after ingestion completes\n",
            "Example: test_knowledge_base()\n"
          ]
        }
      ],
      "source": [
        "# Test the Knowledge Base once ingestion is successful\n",
        "def test_knowledge_base():\n",
        "    \"\"\"Test the Knowledge Base with sample queries\"\"\"\n",
        "    print(\"üß™ Testing Knowledge Base functionality...\")\n",
        "    \n",
        "    # Test queries\n",
        "    test_queries = [\n",
        "        \"How many orders are there in total?\",\n",
        "        \"What is the average order value?\", \n",
        "        \"Which payment method is most popular?\",\n",
        "        \"How many different products have been ordered?\"\n",
        "    ]\n",
        "    \n",
        "    foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "    \n",
        "    for i, query in enumerate(test_queries, 1):\n",
        "        print(f\"\\\\nüîç Test Query {i}: {query}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        try:\n",
        "            # Test with retrieve_and_generate\n",
        "            response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
        "                input={\"text\": query},\n",
        "                retrieveAndGenerateConfiguration={\n",
        "                    \"type\": \"KNOWLEDGE_BASE\",\n",
        "                    \"knowledgeBaseConfiguration\": {\n",
        "                        'knowledgeBaseId': kb_id,\n",
        "                        \"modelArn\": f\"arn:aws:bedrock:{region}::foundation-model/{foundation_model}\",\n",
        "                        \"retrievalConfiguration\": {\n",
        "                            \"vectorSearchConfiguration\": {\n",
        "                                \"numberOfResults\": 5\n",
        "                            } \n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            print(f\"‚úÖ Response: {response['output']['text']}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {str(e)}\")\n",
        "            \n",
        "        print()\n",
        "\n",
        "# Note: Only run this after ingestion completes successfully\n",
        "print(\"üìù Note: Use test_knowledge_base() function after ingestion completes\")\n",
        "print(\"Example: test_knowledge_base()\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wait_for_statement(statement_id):\n",
        "    \"\"\"Wait for a Redshift Data API statement to complete\"\"\"\n",
        "    max_attempts = 30\n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            response = redshift_data_client.describe_statement(Id=statement_id)\n",
        "            status = response['Status']\n",
        "            if status == 'FINISHED':\n",
        "                return response\n",
        "            elif status == 'FAILED':\n",
        "                raise Exception(f\"Statement failed: {response.get('Error', 'Unknown error')}\")\n",
        "            elif status == 'CANCELLED':\n",
        "                raise Exception(\"Statement was cancelled\")\n",
        "            else:\n",
        "                print(f\"Statement status: {status}, waiting...\")\n",
        "                time.sleep(5)\n",
        "        except Exception as e:\n",
        "            if \"Statement failed\" in str(e) or \"cancelled\" in str(e):\n",
        "                raise\n",
        "            print(f\"Error checking statement status: {str(e)}, retrying...\")\n",
        "            time.sleep(5)\n",
        "    \n",
        "    raise Exception(\"Timeout waiting for statement to complete\")\n",
        "\n",
        "def run_redshift_statement(sql_statement):\n",
        "    \"\"\"Execute a SQL statement in Redshift\"\"\"\n",
        "    try:\n",
        "        response = redshift_data_client.execute_statement(\n",
        "            WorkgroupName=REDSHIFT_WORKGROUP,\n",
        "            Database=REDSHIFT_DATABASE,\n",
        "            Sql=sql_statement\n",
        "        )\n",
        "        statement_id = response['Id']\n",
        "        print(f\"Executing statement: {statement_id}\")\n",
        "        result = wait_for_statement(statement_id)\n",
        "        print(f\"Statement completed successfully\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing statement: {str(e)}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating table: orders\n",
            "Executing statement: 4d669868-ea32-4ce0-9180-c977ebc99514\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Created table: orders\n",
            "-------------\n",
            "Creating table: order_items\n",
            "Executing statement: ea378983-2fac-43f2-b3ce-9c8d016ef05d\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Created table: order_items\n",
            "-------------\n",
            "Creating table: payments\n",
            "Executing statement: a75c6c7a-9c38-4b72-8c74-82563692520b\n",
            "Statement status: SUBMITTED, waiting...\n",
            "Statement completed successfully\n",
            "Created table: payments\n",
            "-------------\n",
            "Creating table: reviews\n",
            "Executing statement: e227ecbd-4eb6-4a0b-a9d9-f41049822560\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Created table: reviews\n",
            "-------------\n"
          ]
        }
      ],
      "source": [
        "# Create tables in Redshift\n",
        "def create_tables():\n",
        "    \"\"\"Create all necessary tables in Redshift\"\"\"\n",
        "    \n",
        "    # Orders table\n",
        "    orders_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS orders (\n",
        "        order_id VARCHAR(255) PRIMARY KEY,\n",
        "        customer_id VARCHAR(255),\n",
        "        order_total DECIMAL(10,2),\n",
        "        order_status VARCHAR(50),\n",
        "        payment_method VARCHAR(50),\n",
        "        shipping_address TEXT,\n",
        "        created_at TIMESTAMP,\n",
        "        updated_at TIMESTAMP\n",
        "    );\n",
        "    \"\"\"\n",
        "    \n",
        "    # Order Items table\n",
        "    order_items_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS order_items (\n",
        "        order_item_id VARCHAR(255) PRIMARY KEY,\n",
        "        order_id VARCHAR(255),\n",
        "        product_id VARCHAR(255),\n",
        "        quantity INTEGER,\n",
        "        price DECIMAL(10,2)\n",
        "    );\n",
        "    \"\"\"\n",
        "    \n",
        "    # Payments table\n",
        "    payments_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS payments (\n",
        "        payment_id VARCHAR(255) PRIMARY KEY,\n",
        "        order_id VARCHAR(255),\n",
        "        customer_id VARCHAR(255),\n",
        "        amount DECIMAL(10,2),\n",
        "        payment_method VARCHAR(50),\n",
        "        payment_status VARCHAR(50),\n",
        "        created_at DATE\n",
        "    );\n",
        "    \"\"\"\n",
        "    \n",
        "    # Reviews table\n",
        "    reviews_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS reviews (\n",
        "        review_id VARCHAR(255) PRIMARY KEY,\n",
        "        product_id VARCHAR(255),\n",
        "        customer_id VARCHAR(255),\n",
        "        rating INTEGER,\n",
        "        created_at DATE\n",
        "    );\n",
        "    \"\"\"\n",
        "    \n",
        "    tables = {\n",
        "        'orders': orders_sql,\n",
        "        'order_items': order_items_sql,\n",
        "        'payments': payments_sql,\n",
        "        'reviews': reviews_sql\n",
        "    }\n",
        "    \n",
        "    for table_name, sql in tables.items():\n",
        "        print(f\"Creating table: {table_name}\")\n",
        "        run_redshift_statement(sql)\n",
        "        print(f\"Created table: {table_name}\")\n",
        "        print(\"-------------\")\n",
        "\n",
        "# Create tables\n",
        "create_tables()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data into orders from orders.csv\n",
            "Executing statement: 0a23526b-aa4a-4d34-9458-a6d0f8c0433a\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Loaded data into orders\n",
            "Loading data into order_items from order_items.csv\n",
            "Executing statement: 84d54756-be6c-43c2-9bb8-4f47f5657c37\n",
            "Statement status: PICKED, waiting...\n",
            "Statement completed successfully\n",
            "Loaded data into order_items\n",
            "Loading data into payments from payments.csv\n",
            "Executing statement: 8e48260c-1a43-45c8-a626-4ca55af5866c\n",
            "Statement status: SUBMITTED, waiting...\n",
            "Statement completed successfully\n",
            "Loaded data into payments\n",
            "Loading data into reviews from reviews.csv\n",
            "Executing statement: 6ba97e21-83cd-4e22-843c-068c0bfeb201\n",
            "Statement status: SUBMITTED, waiting...\n",
            "Statement completed successfully\n",
            "Loaded data into reviews\n"
          ]
        }
      ],
      "source": [
        "# Load data from S3 into Redshift tables\n",
        "def load_data_from_s3():\n",
        "    \"\"\"Load data from S3 CSV files into Redshift tables\"\"\"\n",
        "    \n",
        "    tables_and_files = {\n",
        "        'orders': 'orders.csv',\n",
        "        'order_items': 'order_items.csv',\n",
        "        'payments': 'payments.csv',\n",
        "        'reviews': 'reviews.csv'\n",
        "    }\n",
        "    \n",
        "    for table_name, file_name in tables_and_files.items():\n",
        "        print(f\"Loading data into {table_name} from {file_name}\")\n",
        "        \n",
        "        copy_sql = f\"\"\"\n",
        "        COPY {table_name}\n",
        "        FROM 's3://{S3_BUCKET}/{file_name}'\n",
        "        IAM_ROLE '{redshift_role_arn}'\n",
        "        CSV\n",
        "        IGNOREHEADER 1\n",
        "        DELIMITER ','\n",
        "        REGION '{region}';\n",
        "        \"\"\"\n",
        "        \n",
        "        try:\n",
        "            run_redshift_statement(copy_sql)\n",
        "            print(f\"Loaded data into {table_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data into {table_name}: {str(e)}\")\n",
        "\n",
        "# Load data from S3\n",
        "load_data_from_s3()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4 - Create Bedrock Knowledge Base with Redshift Data Source\n",
        "\n",
        "Now we'll create the Bedrock Knowledge Base configured to use our Redshift data as a structured data source.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Knowledge Base Configuration for IAM Role + Redshift Serverless WorkGroup\n",
        "kbConfigParam = {\n",
        "    \"type\": \"SQL\",\n",
        "    \"sqlKnowledgeBaseConfiguration\": {\n",
        "        \"type\": \"REDSHIFT\",\n",
        "        \"redshiftConfiguration\": {\n",
        "            \"storageConfigurations\": [{\n",
        "                \"type\": \"REDSHIFT\",\n",
        "                \"redshiftConfiguration\": {\n",
        "                    \"databaseName\": REDSHIFT_DATABASE\n",
        "                }\n",
        "            }],\n",
        "            \"queryEngineConfiguration\": {\n",
        "                \"type\": \"SERVERLESS\",\n",
        "                \"serverlessConfiguration\": {\n",
        "                    \"workgroupArn\": workgroup_arn,\n",
        "                    \"authConfiguration\": {\n",
        "                        \"type\": \"IAM\"\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5 - Create Knowledge Base\n",
        "\n",
        "This step creates the Bedrock Knowledge Base configured for IAM Role + Redshift Serverless WorkGroup access pattern. The knowledge base will use IAM authentication to securely connect to your Redshift Serverless workgroup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-06-20 21:04:12,179] p74707 {credentials.py:1352} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Knowledge Base with IAM Role + Redshift Serverless WorkGroup access pattern...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-06-20 21:04:12,950] p74707 {credentials.py:1352} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================\n",
            "Step 1 - Creating Knowledge Base Execution Role (AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647) and Policies\n",
            "========================================================================================\n",
            "Step 2 - Creating Knowledge Base\n",
            "{ 'createdAt': datetime.datetime(2025, 6, 21, 4, 4, 13, 800686, tzinfo=tzutc()),\n",
            "  'description': 'Sample Structured KB',\n",
            "  'knowledgeBaseArn': 'arn:aws:bedrock:us-west-2:533267284022:knowledge-base/WCNCTKFCKY',\n",
            "  'knowledgeBaseConfiguration': { 'sqlKnowledgeBaseConfiguration': { 'redshiftConfiguration': { 'queryEngineConfiguration': { 'serverlessConfiguration': { 'authConfiguration': { 'type': 'IAM'},\n",
            "                                                                                                                                                           'workgroupArn': 'arn:aws:redshift-serverless:us-west-2:533267284022:workgroup/a68f79bf-ace3-46d7-abc3-3ee32e4998aa'},\n",
            "                                                                                                                              'type': 'SERVERLESS'},\n",
            "                                                                                                'storageConfigurations': [ { 'redshiftConfiguration': { 'databaseName': 'sds-ecommerce'},\n",
            "                                                                                                                             'type': 'REDSHIFT'}]},\n",
            "                                                                     'type': 'REDSHIFT'},\n",
            "                                  'type': 'SQL'},\n",
            "  'knowledgeBaseId': 'WCNCTKFCKY',\n",
            "  'name': 'bedrock-sample-structured-kb-0205647',\n",
            "  'roleArn': 'arn:aws:iam::533267284022:role/AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647',\n",
            "  'status': 'CREATING',\n",
            "  'updatedAt': datetime.datetime(2025, 6, 21, 4, 4, 13, 800686, tzinfo=tzutc())}\n",
            "Creating Data Sources aka query engine\n",
            "{ 'createdAt': datetime.datetime(2025, 6, 21, 4, 4, 13, 916661, tzinfo=tzutc()),\n",
            "  'dataSourceConfiguration': {'type': 'REDSHIFT_METADATA'},\n",
            "  'dataSourceId': 'EOZFBUZD6F',\n",
            "  'description': 'Query engine',\n",
            "  'knowledgeBaseId': 'WCNCTKFCKY',\n",
            "  'name': 'bedrock-sample-structured-kb-0205647-ds',\n",
            "  'status': 'AVAILABLE',\n",
            "  'updatedAt': datetime.datetime(2025, 6, 21, 4, 4, 13, 916661, tzinfo=tzutc())}\n",
            "========================================================================================\n",
            "Knowledge Base created successfully!\n",
            "'WCNCTKFCKY'\n",
            "Knowledge Base ID: WCNCTKFCKY\n"
          ]
        }
      ],
      "source": [
        "# Create the Knowledge Base using IAM Role + Redshift Serverless WorkGroup\n",
        "print(\"Creating Knowledge Base with IAM Role + Redshift Serverless WorkGroup access pattern...\")\n",
        "\n",
        "knowledge_base = BedrockStructuredKnowledgeBase(\n",
        "    kb_name=knowledge_base_name,\n",
        "    kb_description=knowledge_base_description,\n",
        "    workgroup_arn=workgroup_arn,\n",
        "    kbConfigParam=kbConfigParam,\n",
        "    generation_model=foundation_model,\n",
        "    suffix=suffix\n",
        ")\n",
        "\n",
        "print(\"Knowledge Base created successfully!\")\n",
        "print(f\"Knowledge Base ID: {knowledge_base.get_knowledge_base_id()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Grant Database Access to the Bedrock IAM Role\n",
        "\n",
        "For the IAM access pattern, you need to grant database access to the IAM role that Bedrock Knowledge Base uses for authentication. Execute the following SQL commands in your Redshift Query Editor to create the IAM user and grant appropriate permissions.\n",
        "\n",
        "For more detailed steps, please see the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-structured.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execute the following SQL commands in your Redshift Query Editor:\n",
            "======================================================================\n",
            "CREATE USER \"IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\" WITH PASSWORD DISABLE;\n",
            "GRANT USAGE ON SCHEMA public TO \"IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\";\n",
            "GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0205647\";\n",
            "======================================================================\n",
            "\\nThese commands will:\n",
            "1. Create an IAM-based database user for the Bedrock execution role\n",
            "2. Grant USAGE permission on the public schema\n",
            "3. Grant SELECT permission on all tables in the public schema\n",
            "\\nNote: Adjust the schema and table permissions based on your specific requirements.\n"
          ]
        }
      ],
      "source": [
        "# Get the Bedrock execution role name\n",
        "bedrock_role_name = knowledge_base.bedrock_kb_execution_role_name\n",
        "\n",
        "# Display the SQL commands that need to be executed in Redshift Query Editor\n",
        "print(\"Execute the following SQL commands in your Redshift Query Editor:\")\n",
        "print(\"=\" * 70)\n",
        "print(f'CREATE USER \"IAMR:{bedrock_role_name}\" WITH PASSWORD DISABLE;')\n",
        "print(f'GRANT USAGE ON SCHEMA public TO \"IAMR:{bedrock_role_name}\";')\n",
        "print(f'GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"IAMR:{bedrock_role_name}\";')\n",
        "print(\"=\" * 70)\n",
        "print(\"\\\\nThese commands will:\")\n",
        "print(\"1. Create an IAM-based database user for the Bedrock execution role\")\n",
        "print(\"2. Grant USAGE permission on the public schema\")\n",
        "print(\"3. Grant SELECT permission on all tables in the public schema\")\n",
        "print(\"\\\\nNote: Adjust the schema and table permissions based on your specific requirements.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6 - Start the ingestion job\n",
        "\n",
        "This step is to start the ingestion job to sync the datasources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "job  started successfully\n",
            "\n",
            "{ 'dataSourceId': 'EOZFBUZD6F',\n",
            "  'ingestionJobId': 'HOYDC9A0DV',\n",
            "  'knowledgeBaseId': 'WCNCTKFCKY',\n",
            "  'startedAt': datetime.datetime(2025, 6, 21, 4, 5, 39, 955341, tzinfo=tzutc()),\n",
            "  'status': 'FAILED',\n",
            "  'updatedAt': datetime.datetime(2025, 6, 21, 4, 5, 43, 167059, tzinfo=tzutc())}\n",
            "'WCNCTKFCKY'\n"
          ]
        }
      ],
      "source": [
        "# ensure that the kb is available\n",
        "time.sleep(60)\n",
        "# sync knowledge base\n",
        "knowledge_base.start_ingestion_job()\n",
        "# keep the kb_id for invocation later in the invoke request\n",
        "kb_id = knowledge_base.get_knowledge_base_id()\n",
        "# %store kb_id\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7 - Test the Structured Knowledge Base\n",
        "\n",
        "Now the Knowledge Base is available we can test it out using the retrieve, retrieve_and_generate, and generate_query functions.\n",
        "\n",
        "- When you use **retrieve**, the response returns the result of the SQL query execution.\n",
        "- When you use **retrieve_and_generate**, the generated response is based on the result of the SQL query execution\n",
        "- When using the **generate_query** API, it transforms a natural language query into SQL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"How many orders are there in total?\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 7.1 - Using RetrieveAndGenerate API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValidationException",
          "evalue": "An error occurred (ValidationException) when calling the RetrieveAndGenerate operation: No metadata found. Please trigger an ingestion and try again. (Service: BedrockAgentRuntime, Status Code: 400, Request ID: 4846aded-b9bf-403d-ad46-00cc9556ccc6) (SDK Attempt Count: 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m foundation_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic.claude-3-sonnet-20240229-v1:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mbedrock_agent_runtime_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_and_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrieveAndGenerateConfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKNOWLEDGE_BASE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mknowledgeBaseConfiguration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mknowledgeBaseId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkb_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodelArn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marn:aws:bedrock:\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m::foundation-model/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoundation_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretrievalConfiguration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvectorSearchConfiguration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumberOfResults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages/botocore/client.py:598\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    595\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    596\u001b[0m     )\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[1;32m    122\u001b[0m     hook()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages/botocore/client.py:1061\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1059\u001b[0m     )\n\u001b[1;32m   1060\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
            "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the RetrieveAndGenerate operation: No metadata found. Please trigger an ingestion and try again. (Service: BedrockAgentRuntime, Status Code: 400, Request ID: 4846aded-b9bf-403d-ad46-00cc9556ccc6) (SDK Attempt Count: 1)"
          ]
        }
      ],
      "source": [
        "foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "\n",
        "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
        "    input={\n",
        "        \"text\": query\n",
        "    },\n",
        "    retrieveAndGenerateConfiguration={\n",
        "        \"type\": \"KNOWLEDGE_BASE\",\n",
        "        \"knowledgeBaseConfiguration\": {\n",
        "            'knowledgeBaseId': kb_id,\n",
        "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
        "            \"retrievalConfiguration\": {\n",
        "                \"vectorSearchConfiguration\": {\n",
        "                    \"numberOfResults\": 5\n",
        "                } \n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response['output']['text'], end='\\n'*2)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 7.2 - Using Retrieve API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_ret = bedrock_agent_runtime_client.retrieve(\n",
        "    knowledgeBaseId=kb_id, \n",
        "    nextToken='string',\n",
        "    retrievalConfiguration={\n",
        "        \"vectorSearchConfiguration\": {\n",
        "            \"numberOfResults\": 5,\n",
        "        } \n",
        "    },\n",
        "    retrievalQuery={\n",
        "        \"text\": query\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Function to extract retrieved results from Retrieve API response into a pandas dataframe.\n",
        "def response_print(retrieve_resp):\n",
        "    # Extract the retrievalResults list\n",
        "    retrieval_results = retrieve_resp['retrievalResults']\n",
        "\n",
        "    # Dictionary to store the extracted data\n",
        "    extracted_data = {}\n",
        "\n",
        "    # Iterate through each item in retrievalResults\n",
        "    for item in retrieval_results:\n",
        "        row = item['content']['row']\n",
        "        for col in row:\n",
        "            column_name = col['columnName']\n",
        "            column_value = col['columnValue']\n",
        "            \n",
        "            # If this column hasn't been seen before, create a new list for it\n",
        "            if column_name not in extracted_data:\n",
        "                extracted_data[column_name] = []\n",
        "            \n",
        "            # Append the value to the appropriate list\n",
        "            extracted_data[column_name].append(column_value)\n",
        "\n",
        "    # Create a DataFrame from the extracted data\n",
        "    df = pd.DataFrame(extracted_data)\n",
        "    return df\n",
        "    \n",
        "# Display the Retrieved results records\n",
        "df = response_print(response_ret)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 7.3 - Using Generate Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_response = bedrock_agent_runtime_client.generate_query(\n",
        "    queryGenerationInput={\n",
        "        \"text\": query,\n",
        "        \"type\": \"TEXT\"\n",
        "    },\n",
        "    transformationConfiguration={\n",
        "        \"mode\": \"TEXT_TO_SQL\",\n",
        "        \"textToSqlConfiguration\": {\n",
        "            \"type\": \"KNOWLEDGE_BASE\",\n",
        "            \"knowledgeBaseConfiguration\": {\n",
        "                \"knowledgeBaseArn\": knowledge_base.knowledge_base['knowledgeBaseArn']\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "generated_sql = query_response['queries'][0]['sql']\n",
        "generated_sql\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8 - Clean Up\n",
        "\n",
        "Please make sure to uncomment and run the below section to delete all the resources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete resources\n",
        "# print(\"=============================== Deleting resources ==============================\\n\")\n",
        "# knowledge_base.delete_kb(delete_iam_roles_and_policies=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "genai-on-aws",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
