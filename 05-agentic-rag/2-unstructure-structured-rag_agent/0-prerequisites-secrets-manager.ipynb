{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "725828ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import logging\n",
    "import requests\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a61171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS clients\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "sts_client = boto3.client('sts')\n",
    "redshift_client = boto3.client('redshift-serverless', region_name=region)\n",
    "redshift_data_client = boto3.client('redshift-data', region_name=region)\n",
    "iam_client = boto3.client('iam')\n",
    "secrets_client = boto3.client('secretsmanager')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5d40a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using suffix: 1093701\n"
     ]
    }
   ],
   "source": [
    "# Generate unique suffix for resource names\n",
    "current_time = time.time()\n",
    "timestamp_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(current_time))[-7:]\n",
    "suffix = f\"{timestamp_str}\"\n",
    "\n",
    "print(f\"Using suffix: {suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d05e550",
   "metadata": {},
   "source": [
    "## Step 1: Download Bedrock Knowledge Base Utilities\n",
    "\n",
    "Lets download the structured knowledge base utility to help with Knowledge Base configuration and creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ab0fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded structured KB utils to utils/structured_knowledge_base.py\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/rag/knowledge-bases/features-examples/utils/structured_knowledge_base.py\"\n",
    "target_path = \"utils/structured_knowledge_base.py\"\n",
    "os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "response = requests.get(url)\n",
    "with open(target_path, \"w\") as f:\n",
    "    f.write(response.text)\n",
    "print(f\"Downloaded structured KB utils to {target_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f62900bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.structured_knowledge_base import BedrockStructuredKnowledgeBase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2ccb8",
   "metadata": {},
   "source": [
    "## Step 2: Set up Redshift Serverless Infrastructure\n",
    "\n",
    "Next we will create the necessary Redshift Serverless components: namespace and workgroup. This infrastructure will host our structured data that the Knowledge Base will query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7182df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift Namespace: sds-ecommerce-1093701\n",
      "Redshift Workgroup: sds-ecommerce-wg-1093701\n",
      "Database: sds-ecommerce\n",
      "S3 Bucket: sds-ecommerce-redshift-1093701\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Redshift resources\n",
    "REDSHIFT_NAMESPACE = f'sds-ecommerce-{suffix}'\n",
    "REDSHIFT_WORKGROUP = f'sds-ecommerce-wg-{suffix}'\n",
    "REDSHIFT_DATABASE = f'sds-ecommerce'\n",
    "S3_BUCKET = f'sds-ecommerce-redshift-{suffix}'\n",
    "\n",
    "print(f\"Redshift Namespace: {REDSHIFT_NAMESPACE}\")\n",
    "print(f\"Redshift Workgroup: {REDSHIFT_WORKGROUP}\")\n",
    "print(f\"Database: {REDSHIFT_DATABASE}\")\n",
    "print(f\"S3 Bucket: {S3_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "109b83d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created role RedshiftS3AccessRole-1093701\n",
      "Redshift IAM Role ARN: arn:aws:iam::533267284022:role/RedshiftS3AccessRole-1093701\n"
     ]
    }
   ],
   "source": [
    "def create_iam_role_for_redshift():\n",
    "    \"\"\"Create IAM role for Redshift to access S3\"\"\"\n",
    "    try:\n",
    "        # Get account ID\n",
    "        account_id = sts_client.get_caller_identity()['Account']\n",
    "        \n",
    "        # Create IAM role if it doesn't exist\n",
    "        role_name = f'RedshiftS3AccessRole-{suffix}'\n",
    "        try:\n",
    "            role_response = iam_client.get_role(RoleName=role_name)\n",
    "            print(f'Role {role_name} already exists')\n",
    "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
    "        except iam_client.exceptions.NoSuchEntityException:\n",
    "            trust_policy = {\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"redshift.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            iam_client.create_role(\n",
    "                RoleName=role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    "            )\n",
    "            \n",
    "            # Attach necessary policies\n",
    "            iam_client.attach_role_policy(\n",
    "                RoleName=role_name,\n",
    "                PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n",
    "            )\n",
    "            \n",
    "            print(f'Created role {role_name}')\n",
    "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error creating IAM role: {str(e)}')\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "redshift_role_arn = create_iam_role_for_redshift()\n",
    "print(f\"Redshift IAM Role ARN: {redshift_role_arn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e59245e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating namespace sds-ecommerce-1093701...\n",
      "Created namespace sds-ecommerce-1093701\n",
      "Waiting for namespace to be available...\n",
      "Namespace sds-ecommerce-1093701 is now available\n"
     ]
    }
   ],
   "source": [
    "def create_redshift_namespace():\n",
    "    \"\"\"Create Redshift Serverless namespace\"\"\"\n",
    "    try:\n",
    "        # Check if namespace already exists\n",
    "        try:\n",
    "            response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
    "            print(f'Namespace {REDSHIFT_NAMESPACE} already exists')\n",
    "            return response['namespace']\n",
    "        except redshift_client.exceptions.ResourceNotFoundException:\n",
    "            print(f'Creating namespace {REDSHIFT_NAMESPACE}...')\n",
    "        \n",
    "        # Create the namespace\n",
    "        response = redshift_client.create_namespace(\n",
    "            namespaceName=REDSHIFT_NAMESPACE,\n",
    "            adminUsername='admin',\n",
    "            adminUserPassword='TempPassword123!',  # Change this in production\n",
    "            dbName=REDSHIFT_DATABASE,\n",
    "            defaultIamRoleArn=redshift_role_arn,\n",
    "            iamRoles=[redshift_role_arn]\n",
    "        )\n",
    "        \n",
    "        print(f'Created namespace {REDSHIFT_NAMESPACE}')\n",
    "        \n",
    "        # Wait for namespace to be available\n",
    "        print('Waiting for namespace to be available...')\n",
    "        max_attempts = 30\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                namespace_response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
    "                status = namespace_response['namespace']['status']\n",
    "                if status == 'AVAILABLE':\n",
    "                    print(f'Namespace {REDSHIFT_NAMESPACE} is now available')\n",
    "                    return namespace_response['namespace']\n",
    "                else:\n",
    "                    print(f'Namespace status: {status}, waiting...')\n",
    "                    time.sleep(10)\n",
    "            except Exception as e:\n",
    "                print(f'Error checking namespace status: {str(e)}, retrying...')\n",
    "                time.sleep(10)\n",
    "        \n",
    "        print('Timeout waiting for namespace, but proceeding...')\n",
    "        return response['namespace']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error creating namespace: {str(e)}')\n",
    "        raise\n",
    "\n",
    "# Create namespace\n",
    "namespace = create_redshift_namespace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "372e2dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating workgroup sds-ecommerce-wg-1093701...\n",
      "Created workgroup sds-ecommerce-wg-1093701\n",
      "Waiting for workgroup to be available...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup sds-ecommerce-wg-1093701 is now available\n",
      "Workgroup ARN: arn:aws:redshift-serverless:us-west-2:533267284022:workgroup/c9976901-a855-493d-b201-fa8bd402b6a4\n"
     ]
    }
   ],
   "source": [
    "def create_redshift_workgroup():\n",
    "    \"\"\"Create Redshift Serverless workgroup\"\"\"\n",
    "    try:\n",
    "        # Check if workgroup already exists\n",
    "        try:\n",
    "            response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
    "            print(f'Workgroup {REDSHIFT_WORKGROUP} already exists')\n",
    "            return response['workgroup']\n",
    "        except redshift_client.exceptions.ResourceNotFoundException:\n",
    "            print(f'Creating workgroup {REDSHIFT_WORKGROUP}...')\n",
    "        \n",
    "        # Create the workgroup\n",
    "        response = redshift_client.create_workgroup(\n",
    "            workgroupName=REDSHIFT_WORKGROUP,\n",
    "            namespaceName=REDSHIFT_NAMESPACE,\n",
    "            baseCapacity=8,  # Minimum base capacity\n",
    "            enhancedVpcRouting=False,\n",
    "            publiclyAccessible=True,\n",
    "            configParameters=[\n",
    "                {\n",
    "                    'parameterKey': 'enable_user_activity_logging',\n",
    "                    'parameterValue': 'true'\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f'Created workgroup {REDSHIFT_WORKGROUP}')\n",
    "        \n",
    "        # Wait for workgroup to be available\n",
    "        print('Waiting for workgroup to be available...')\n",
    "        max_attempts = 30\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                workgroup_response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
    "                status = workgroup_response['workgroup']['status']\n",
    "                if status == 'AVAILABLE':\n",
    "                    print(f'Workgroup {REDSHIFT_WORKGROUP} is now available')\n",
    "                    return workgroup_response['workgroup']\n",
    "                else:\n",
    "                    print(f'Workgroup status: {status}, waiting...')\n",
    "                    time.sleep(10)\n",
    "            except Exception as e:\n",
    "                print(f'Error checking workgroup status: {str(e)}, retrying...')\n",
    "                time.sleep(10)\n",
    "        \n",
    "        print('Timeout waiting for workgroup, but proceeding...')\n",
    "        return response['workgroup']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error creating workgroup: {str(e)}')\n",
    "        raise\n",
    "\n",
    "# Create workgroup\n",
    "workgroup = create_redshift_workgroup()\n",
    "workgroup_arn = workgroup['workgroupArn']\n",
    "print(f\"Workgroup ARN: {workgroup_arn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a03caa",
   "metadata": {},
   "source": [
    "## Step 3: Create S3 Bucket and Load Sample Data\n",
    "\n",
    "We will create an S3 bucket to stage our sample e-commerce data before loading it into Redshift tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9976c340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket sds-ecommerce-redshift-1093701\n"
     ]
    }
   ],
   "source": [
    "def create_s3_bucket():\n",
    "    \"\"\"Create S3 bucket for data staging\"\"\"\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=S3_BUCKET)\n",
    "        print(f'Bucket {S3_BUCKET} already exists')\n",
    "    except:\n",
    "        try:\n",
    "            if region == 'us-east-1':\n",
    "                s3_client.create_bucket(Bucket=S3_BUCKET)\n",
    "            else:\n",
    "                s3_client.create_bucket(\n",
    "                    Bucket=S3_BUCKET,\n",
    "                    CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                )\n",
    "            print(f'Created bucket {S3_BUCKET}')\n",
    "        except Exception as e:\n",
    "            print(f'Error creating bucket: {str(e)}')\n",
    "            raise\n",
    "\n",
    "# Create S3 bucket\n",
    "create_s3_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1be47dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading sample data files to S3...\n",
      "Uploaded orders.csv (1.8 MB) to S3\n",
      "Uploaded order_items.csv (1.3 MB) to S3\n",
      "Uploaded payments.csv (0.8 MB) to S3\n",
      "Uploaded reviews.csv (0.5 MB) to S3\n",
      "\n",
      "Successfully uploaded all 4 data files to S3\n"
     ]
    }
   ],
   "source": [
    "def upload_sample_data():\n",
    "    \"\"\"Upload sample CSV files to S3\"\"\"\n",
    "    data_files = ['orders.csv', 'order_items.csv', 'payments.csv', 'reviews.csv']\n",
    "    sds_directory = 'sample_data'\n",
    "    \n",
    "    print(\"Uploading sample data files to S3...\")\n",
    "    files_found = 0\n",
    "    \n",
    "    for file_name in data_files:\n",
    "        local_path = os.path.join(sds_directory, file_name)\n",
    "        if os.path.exists(local_path):\n",
    "            # Get file size for informational purposes\n",
    "            file_size = os.path.getsize(local_path)\n",
    "            file_size_mb = file_size / (1024 * 1024)\n",
    "            \n",
    "            s3_client.upload_file(local_path, S3_BUCKET, file_name)\n",
    "            print(f'Uploaded {file_name} ({file_size_mb:.1f} MB) to S3')\n",
    "            files_found += 1\n",
    "        else:\n",
    "            print(f'Warning: {local_path} not found')\n",
    "    \n",
    "    if files_found == len(data_files):\n",
    "        print(f\"\\nSuccessfully uploaded all {files_found} data files to S3\")\n",
    "    else:\n",
    "        print(f\"\\nOnly {files_found} out of {len(data_files)} files were found and uploaded\")\n",
    "\n",
    "# Upload sample data\n",
    "upload_sample_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd94c8",
   "metadata": {},
   "source": [
    "## Step 4: Create Redshift Tables and Load Data\n",
    "\n",
    "Now we will create the database tables in Redshift and load our sample e-commerce data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a4b8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_statement(statement_id):\n",
    "    \"\"\"Wait for a Redshift Data API statement to complete\"\"\"\n",
    "    max_attempts = 30\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            response = redshift_data_client.describe_statement(Id=statement_id)\n",
    "            status = response['Status']\n",
    "            if status == 'FINISHED':\n",
    "                return response\n",
    "            elif status == 'FAILED':\n",
    "                raise Exception(f\"Statement failed: {response.get('Error', 'Unknown error')}\")\n",
    "            elif status == 'CANCELLED':\n",
    "                raise Exception(\"Statement was cancelled\")\n",
    "            else:\n",
    "                print(f\"Statement status: {status}, waiting...\")\n",
    "                time.sleep(5)\n",
    "        except Exception as e:\n",
    "            if 'Statement failed' in str(e) or 'cancelled' in str(e):\n",
    "                raise\n",
    "            print(f\"Error checking statement status: {str(e)}, retrying...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    raise Exception(\"Timeout waiting for statement to complete\")\n",
    "\n",
    "def run_redshift_statement(sql_statement):\n",
    "    \"\"\"Execute a SQL statement in Redshift\"\"\"\n",
    "    try:\n",
    "        response = redshift_data_client.execute_statement(\n",
    "            WorkgroupName=REDSHIFT_WORKGROUP,\n",
    "            Database=REDSHIFT_DATABASE,\n",
    "            Sql=sql_statement\n",
    "        )\n",
    "        statement_id = response['Id']\n",
    "        print(f\"Executing statement: {statement_id}\")\n",
    "        result = wait_for_statement(statement_id)\n",
    "        print(f\"Statement completed successfully\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing statement: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c7d3205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table: orders\n",
      "Executing statement: df2461cb-612b-4f97-a27e-b17ba17ef8d5\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Created table: orders\n",
      "-------------\n",
      "Creating table: order_items\n",
      "Executing statement: d30eab0a-f92b-4c7d-845b-a1c9c7e85752\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Created table: order_items\n",
      "-------------\n",
      "Creating table: payments\n",
      "Executing statement: ee5381a7-1b36-41f0-88c8-4552ff32252d\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Created table: payments\n",
      "-------------\n",
      "Creating table: reviews\n",
      "Executing statement: 301426bf-7540-4682-9998-8f9f9900e4b2\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Created table: reviews\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# Create tables in Redshift\n",
    "def create_tables():\n",
    "    \"\"\"Create all necessary tables in Redshift\"\"\"\n",
    "    \n",
    "    # Orders table\n",
    "    orders_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS orders (\n",
    "        order_id VARCHAR(255) PRIMARY KEY,\n",
    "        customer_id VARCHAR(255),\n",
    "        order_total DECIMAL(10,2),\n",
    "        order_status VARCHAR(50),\n",
    "        payment_method VARCHAR(50),\n",
    "        shipping_address TEXT,\n",
    "        created_at TIMESTAMP,\n",
    "        updated_at TIMESTAMP\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Order Items table\n",
    "    order_items_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS order_items (\n",
    "        order_item_id VARCHAR(255) PRIMARY KEY,\n",
    "        order_id VARCHAR(255),\n",
    "        product_id VARCHAR(255),\n",
    "        quantity INTEGER,\n",
    "        price DECIMAL(10,2)\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Payments table\n",
    "    payments_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS payments (\n",
    "        payment_id VARCHAR(255) PRIMARY KEY,\n",
    "        order_id VARCHAR(255),\n",
    "        customer_id VARCHAR(255),\n",
    "        amount DECIMAL(10,2),\n",
    "        payment_method VARCHAR(50),\n",
    "        payment_status VARCHAR(50),\n",
    "        created_at DATE\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reviews table\n",
    "    reviews_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS reviews (\n",
    "        review_id VARCHAR(255) PRIMARY KEY,\n",
    "        product_id VARCHAR(255),\n",
    "        customer_id VARCHAR(255),\n",
    "        rating INTEGER,\n",
    "        created_at DATE\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    tables = {\n",
    "        'orders': orders_sql,\n",
    "        'order_items': order_items_sql,\n",
    "        'payments': payments_sql,\n",
    "        'reviews': reviews_sql\n",
    "    }\n",
    "    \n",
    "    for table_name, sql in tables.items():\n",
    "        print(f\"Creating table: {table_name}\")\n",
    "        run_redshift_statement(sql)\n",
    "        print(f\"Created table: {table_name}\")\n",
    "        print(\"-------------\")\n",
    "\n",
    "# Create tables\n",
    "create_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b248290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into orders from orders.csv\n",
      "Executing statement: 0268be68-6928-40fc-a17f-ef1ae762a4c7\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into orders\n",
      "Loading data into order_items from order_items.csv\n",
      "Executing statement: 69045895-bf76-4a54-90a8-f585b9f6c170\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into order_items\n",
      "Loading data into payments from payments.csv\n",
      "Executing statement: e558b6a6-fd26-465a-96fc-499aaef39713\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into payments\n",
      "Loading data into reviews from reviews.csv\n",
      "Executing statement: b4cc4ea2-c16a-4f5b-af5a-0ab863c37118\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into reviews\n"
     ]
    }
   ],
   "source": [
    "# Load data from S3 into Redshift tables\n",
    "def load_data_from_s3():\n",
    "    \"\"\"Load data from S3 CSV files into Redshift tables\"\"\"\n",
    "    \n",
    "    tables_and_files = {\n",
    "        'orders': 'orders.csv',\n",
    "        'order_items': 'order_items.csv',\n",
    "        'payments': 'payments.csv',\n",
    "        'reviews': 'reviews.csv'\n",
    "    }\n",
    "    \n",
    "    for table_name, file_name in tables_and_files.items():\n",
    "        print(f\"Loading data into {table_name} from {file_name}\")\n",
    "        \n",
    "        copy_sql = f\"\"\"\n",
    "        COPY {table_name}\n",
    "        FROM 's3://{S3_BUCKET}/{file_name}'\n",
    "        IAM_ROLE '{redshift_role_arn}'\n",
    "        CSV\n",
    "        IGNOREHEADER 1\n",
    "        DELIMITER ','\n",
    "        REGION '{region}';\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            run_redshift_statement(copy_sql)\n",
    "            print(f\"Loaded data into {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data into {table_name}: {str(e)}\")\n",
    "\n",
    "# Load data from S3\n",
    "load_data_from_s3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e51221",
   "metadata": {},
   "source": [
    "## Step 5: Verify Data Load\n",
    "\n",
    "Let's verify that our data has been loaded correctly by running some sample queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea721a6",
   "metadata": {},
   "source": [
    "## Step 6: Create Bedrock Knowledge Base with Redshift Data Source\n",
    "\n",
    "Now we'll create the Bedrock Knowledge Base configured to use our Redshift data as a structured data source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7891a188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base Name: redshift-structured-kb-1093701\n"
     ]
    }
   ],
   "source": [
    "# Configure Knowledge Base parameters\n",
    "kb_name = f\"redshift-structured-kb-{suffix}\"\n",
    "kb_description = \"Structured Knowledge Base for e-commerce data queries using Redshift\"\n",
    "generation_model = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "print(f\"Knowledge Base Name: {kb_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7f9bc",
   "metadata": {},
   "source": [
    "Amazon Bedrock Knowledge Bases uses a service role to connect knowledge bases to structured data stores, retrieve data from these data stores, and generate SQL queries based on user queries and the structure of the data stores. There are several access patterns based on if you're using Redshift Serverless vs Redshift Provisioned Cluster. In this notebook, let's use `Secrets Manager + Redshift Serverless WorkGroup` access pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8958d218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating secret redshift-credentials-1093701...\n",
      "Created secret redshift-credentials-1093701\n",
      "Secret ARN: arn:aws:secretsmanager:us-west-2:533267284022:secret:redshift-credentials-1093701-HMByU1\n",
      "Using secret: redshift-credentials-1093701\n",
      "Secret ARN: arn:aws:secretsmanager:us-west-2:533267284022:secret:redshift-credentials-1093701-HMByU1\n"
     ]
    }
   ],
   "source": [
    "# Create Secrets Manager secret for Redshift authentication\n",
    "def create_redshift_secret():\n",
    "    \"\"\"Create or retrieve Secrets Manager secret for Redshift database authentication\"\"\"\n",
    "    secret_name = f\"redshift-credentials-{suffix}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if secret already exists\n",
    "        try:\n",
    "            response = secrets_client.get_secret_value(SecretId=secret_name)\n",
    "            print(f\"Secret {secret_name} already exists\")\n",
    "            return response['Name'], f\"arn:aws:secretsmanager:{region}:{sts_client.get_caller_identity()['Account']}:secret:{secret_name}\"\n",
    "        except secrets_client.exceptions.ResourceNotFoundException:\n",
    "            print(f\"Creating secret {secret_name}...\")\n",
    "        \n",
    "        # Create the secret with database credentials\n",
    "        secret_value = {\n",
    "            \"username\": \"admin\",  # This should match the admin user from the namespace\n",
    "            \"password\": \"TempPassword123!\"  # This should match the password from the namespace\n",
    "        }\n",
    "        \n",
    "        response = secrets_client.create_secret(\n",
    "            Name=secret_name,\n",
    "            Description=\"Redshift database credentials for Bedrock Knowledge Base\",\n",
    "            SecretString=json.dumps(secret_value)\n",
    "        )\n",
    "        \n",
    "        secret_arn = response['ARN']\n",
    "        print(f\"Created secret {secret_name}\")\n",
    "        print(f\"Secret ARN: {secret_arn}\")\n",
    "        return secret_name, secret_arn\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating secret: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Create the secret\n",
    "secret_name, secret_arn = create_redshift_secret()\n",
    "print(f\"Using secret: {secret_name}\")\n",
    "print(f\"Secret ARN: {secret_arn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65137e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Knowledge Base parameters for Redshift Serverless with Secrets Manager authentication\n",
    "kb_config_param = {\n",
    "    \"type\": \"SQL\",\n",
    "    \"sqlKnowledgeBaseConfiguration\": {\n",
    "        \"type\": \"REDSHIFT\",\n",
    "        \"redshiftConfiguration\": {\n",
    "            \"storageConfigurations\": [{\n",
    "                \"type\": \"REDSHIFT\",\n",
    "                \"redshiftConfiguration\": {\n",
    "                    \"databaseName\": REDSHIFT_DATABASE\n",
    "                }\n",
    "            }],\n",
    "            \"queryEngineConfiguration\": {\n",
    "                \"type\": \"SERVERLESS\",\n",
    "                \"serverlessConfiguration\": {\n",
    "                    \"workgroupArn\": workgroup_arn,\n",
    "                    \"authConfiguration\": {}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e2fb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure authentication for Secrets Manager\n",
    "kb_config_param['sqlKnowledgeBaseConfiguration']['redshiftConfiguration']['queryEngineConfiguration']['serverlessConfiguration']['authConfiguration']['type'] = \"USERNAME_PASSWORD\"\n",
    "kb_config_param['sqlKnowledgeBaseConfiguration']['redshiftConfiguration']['queryEngineConfiguration']['serverlessConfiguration']['authConfiguration']['usernamePasswordSecretArn'] = secret_arn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "999301c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'SQL',\n",
       " 'sqlKnowledgeBaseConfiguration': {'type': 'REDSHIFT',\n",
       "  'redshiftConfiguration': {'storageConfigurations': [{'type': 'REDSHIFT',\n",
       "     'redshiftConfiguration': {'databaseName': 'sds-ecommerce'}}],\n",
       "   'queryEngineConfiguration': {'type': 'SERVERLESS',\n",
       "    'serverlessConfiguration': {'workgroupArn': 'arn:aws:redshift-serverless:us-west-2:533267284022:workgroup/c9976901-a855-493d-b201-fa8bd402b6a4',\n",
       "     'authConfiguration': {'type': 'USERNAME_PASSWORD',\n",
       "      'usernamePasswordSecretArn': 'arn:aws:secretsmanager:us-west-2:533267284022:secret:redshift-credentials-1093701-HMByU1'}}}}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_config_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "932cac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "Step 1 - Creating Knowledge Base Execution Role (AmazonBedrockExecutionRoleForStructuredKnowledgeBase_1093701) and Policies\n",
      "========================================================================================\n",
      "Step 2 - Creating Knowledge Base\n",
      "{ 'createdAt': datetime.datetime(2025, 6, 21, 16, 43, 21, 853520, tzinfo=tzutc()),\n",
      "  'description': 'Structured Knowledge Base for e-commerce data queries using '\n",
      "                 'Redshift',\n",
      "  'knowledgeBaseArn': 'arn:aws:bedrock:us-west-2:533267284022:knowledge-base/AZJAPNQSJU',\n",
      "  'knowledgeBaseConfiguration': { 'sqlKnowledgeBaseConfiguration': { 'redshiftConfiguration': { 'queryEngineConfiguration': { 'serverlessConfiguration': { 'authConfiguration': { 'type': 'USERNAME_PASSWORD',\n",
      "                                                                                                                                                                                  'usernamePasswordSecretArn': 'arn:aws:secretsmanager:us-west-2:533267284022:secret:redshift-credentials-1093701-HMByU1'},\n",
      "                                                                                                                                                           'workgroupArn': 'arn:aws:redshift-serverless:us-west-2:533267284022:workgroup/c9976901-a855-493d-b201-fa8bd402b6a4'},\n",
      "                                                                                                                              'type': 'SERVERLESS'},\n",
      "                                                                                                'storageConfigurations': [ { 'redshiftConfiguration': { 'databaseName': 'sds-ecommerce'},\n",
      "                                                                                                                             'type': 'REDSHIFT'}]},\n",
      "                                                                     'type': 'REDSHIFT'},\n",
      "                                  'type': 'SQL'},\n",
      "  'knowledgeBaseId': 'AZJAPNQSJU',\n",
      "  'name': 'redshift-structured-kb-1093701',\n",
      "  'roleArn': 'arn:aws:iam::533267284022:role/AmazonBedrockExecutionRoleForStructuredKnowledgeBase_1093701',\n",
      "  'status': 'CREATING',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 21, 16, 43, 21, 853520, tzinfo=tzutc())}\n",
      "Creating Data Sources aka query engine\n",
      "{ 'createdAt': datetime.datetime(2025, 6, 21, 16, 43, 21, 975333, tzinfo=tzutc()),\n",
      "  'dataSourceConfiguration': {'type': 'REDSHIFT_METADATA'},\n",
      "  'dataSourceId': 'Z68EQ1RAY7',\n",
      "  'description': 'Query engine',\n",
      "  'knowledgeBaseId': 'AZJAPNQSJU',\n",
      "  'name': 'redshift-structured-kb-1093701-ds',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 21, 16, 43, 21, 975333, tzinfo=tzutc())}\n",
      "========================================================================================\n",
      "Knowledge Base created successfully!\n",
      "'AZJAPNQSJU'\n",
      "Knowledge Base ID: AZJAPNQSJU\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    structured_kb = BedrockStructuredKnowledgeBase(\n",
    "        kb_name=kb_name,\n",
    "        kb_description=kb_description,\n",
    "        workgroup_arn=workgroup_arn,\n",
    "        secrets_arn=secret_arn,\n",
    "        kbConfigParam=kb_config_param,\n",
    "        generation_model=generation_model,\n",
    "        suffix=suffix\n",
    "    )\n",
    "    \n",
    "    print(\"Knowledge Base created successfully!\")\n",
    "    kb_id = structured_kb.get_knowledge_base_id()\n",
    "    print(f\"Knowledge Base ID: {kb_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating Knowledge Base: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22c74e",
   "metadata": {},
   "source": [
    "## Step 6: Database Access Configuration for Secrets Manager + Redshift Serverless WorkGroup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00549fb8",
   "metadata": {},
   "source": [
    "For the Secrets Manager + Redshift Serverless WorkGroup access pattern, the credentials stored in Secrets Manager are used to authenticate with the Redshift database. Since we're using the admin user credentials that were set during namespace creation, no additional database user configuration is required.\n",
    "\n",
    "**Key Benefits of Secrets Manager approach:**\n",
    "1. **Simplified setup**: No need to create additional database users or configure IAM role mappings\n",
    "2. **Credential management**: Passwords can be rotated through Secrets Manager\n",
    "3. **Direct authentication**: Uses standard username/password authentication to the database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "260dbdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using Secrets Manager authentication - no additional database user setup required\n",
      "Knowledge Base ARN: arn:aws:bedrock:us-west-2:533267284022:knowledge-base/AZJAPNQSJU\n",
      "Using Secret ARN: arn:aws:secretsmanager:us-west-2:533267284022:secret:redshift-credentials-1093701-HMByU1\n"
     ]
    }
   ],
   "source": [
    "# No additional database configuration needed for Secrets Manager authentication\n",
    "kb_details = structured_kb.knowledge_base\n",
    "print(\"✅ Using Secrets Manager authentication - no additional database user setup required\")\n",
    "print(f\"Knowledge Base ARN: {kb_details['knowledgeBaseArn']}\")\n",
    "print(f\"Using Secret ARN: {secret_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a7e47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Verifying Secrets Manager configuration...\n",
      "✅ Secret verified - contains username: admin\n",
      "✅ Password field present: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verify the secret is accessible and contains the correct credentials\n",
    "print(\"🔍 Verifying Secrets Manager configuration...\")\n",
    "try:\n",
    "    secret_response = secrets_client.get_secret_value(SecretId=secret_name)\n",
    "    secret_data = json.loads(secret_response['SecretString'])\n",
    "    print(f\"✅ Secret verified - contains username: {secret_data.get('username', 'NOT_FOUND')}\")\n",
    "    print(\"✅ Password field present:\", 'password' in secret_data)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error accessing secret: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53ab5fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using admin credentials from Secrets Manager - full database access already available\n",
      "📝 Note: In production, consider creating a dedicated user with minimal required permissions\n"
     ]
    }
   ],
   "source": [
    "# Note: For Secrets Manager authentication, the admin user already has full access to all tables\n",
    "# No additional permission grants are needed since we're using the admin credentials\n",
    "print(\"✅ Using admin credentials from Secrets Manager - full database access already available\")\n",
    "print(\"📝 Note: In production, consider creating a dedicated user with minimal required permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3bd6df",
   "metadata": {},
   "source": [
    "## Step 7: Start Ingestion Job\n",
    "\n",
    "Now that the database permissions are properly configured, let's start the ingestion job to sync the data from the Redshift database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0097a5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job  started successfully\n",
      "\n",
      "{ 'dataSourceId': 'Z68EQ1RAY7',\n",
      "  'ingestionJobId': 'IB6OHTOYUD',\n",
      "  'knowledgeBaseId': 'AZJAPNQSJU',\n",
      "  'startedAt': datetime.datetime(2025, 6, 21, 16, 45, 7, 116877, tzinfo=tzutc()),\n",
      "  'status': 'FAILED',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 21, 16, 45, 10, 551762, tzinfo=tzutc())}\n",
      ".....\r"
     ]
    }
   ],
   "source": [
    "# Wait a bit for the Knowledge Base to be fully ready\n",
    "time.sleep(60)\n",
    "structured_kb.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "552d4c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for querying the Knowledge Base\n",
    "\n",
    "def query_with_retrieve_and_generate(kb_id, query):\n",
    "    \"\"\"Query using retrieve_and_generate API - returns natural language response\"\"\"\n",
    "    try:\n",
    "        response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "            input={\"text\": query},\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                \"type\": \"KNOWLEDGE_BASE\",\n",
    "                \"knowledgeBaseConfiguration\": {\n",
    "                    'knowledgeBaseId': kb_id,\n",
    "                    \"modelArn\": f\"arn:aws:bedrock:{region}::foundation-model/{generation_model}\",\n",
    "                    \"retrievalConfiguration\": {\n",
    "                        \"vectorSearchConfiguration\": {\n",
    "                            \"numberOfResults\": 5\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        return response['output']['text']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def generate_sql_query(kb_arn, query):\n",
    "    \"\"\"Generate SQL query from natural language\"\"\"\n",
    "    try:\n",
    "        response = bedrock_agent_runtime_client.generate_query(\n",
    "            queryGenerationInput={\n",
    "                \"text\": query,\n",
    "                \"type\": \"TEXT\"\n",
    "            },\n",
    "            transformationConfiguration={\n",
    "                \"mode\": \"TEXT_TO_SQL\",\n",
    "                \"textToSqlConfiguration\": {\n",
    "                    \"type\": \"KNOWLEDGE_BASE\",\n",
    "                    \"knowledgeBaseConfiguration\": {\n",
    "                        \"knowledgeBaseArn\": kb_details['knowledgeBaseArn']\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.get('queries') and len(response['queries']) > 0:\n",
    "            return response['queries'][0]['sql']\n",
    "        else:\n",
    "            return \"No SQL generated\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"✅ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6ad67a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Knowledge Base with sample queries\n",
      "============================================================\n",
      "\n",
      "📝 Query 1: How many orders are in the database?\n",
      "--------------------------------------------------\n",
      "🤖 Natural Language Response:\n",
      "   There are 10,000 orders in the database.\n",
      "\n",
      "🔧 Generated SQL:\n",
      "   SELECT COUNT(*) AS \"total_orders\" FROM public.orders;\n",
      "\n",
      "==================================================\n",
      "\n",
      "📝 Query 2: What is the average order total?\n",
      "--------------------------------------------------\n",
      "🤖 Natural Language Response:\n",
      "   The average order total is $507.84.\n",
      "\n",
      "🔧 Generated SQL:\n",
      "   SELECT AVG(\"order_total\") AS \"average_order_total\" FROM public.orders;\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"How many orders are in the database?\",\n",
    "    \"What is the average order total?\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing Knowledge Base with sample queries\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n📝 Query {i}: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get natural language response\n",
    "    print(\"🤖 Natural Language Response:\")\n",
    "    nl_response = query_with_retrieve_and_generate(kb_id, query)\n",
    "    print(f\"   {nl_response}\")\n",
    "    \n",
    "    # Get generated SQL\n",
    "    print(\"\\n🔧 Generated SQL:\")\n",
    "    sql_query = generate_sql_query(kb_details['knowledgeBaseArn'], query)\n",
    "    print(f\"   {sql_query}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c65ba69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data-source Z68EQ1RAY7 (redshift-structured-kb-1093701-ds) ===\n",
      "\n",
      "  Job IB6OHTOYUD  |  status: FAILED\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from pprint import pprint\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Set your knowledge-base Id here (10-char alpha-numeric string)\n",
    "# ------------------------------------------------------------------\n",
    "KB_ID = kb_id\n",
    "\n",
    "bedrock = boto3.client(\"bedrock-agent\")\n",
    "\n",
    "def get_ingestion_jobs(kb_id: str):\n",
    "    \"\"\"\n",
    "    Prints the detail of every ingestion job that was ever started\n",
    "    for every data-source attached to the given knowledge-base.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) find all data-sources for this KB\n",
    "    sources = bedrock.list_data_sources(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        maxResults=100\n",
    "    )[\"dataSourceSummaries\"]\n",
    "\n",
    "    if not sources:\n",
    "        print(\"No data-sources found for KB:\", kb_id)\n",
    "        return\n",
    "\n",
    "    for src in sources:\n",
    "        ds_id = src[\"dataSourceId\"]\n",
    "        print(f\"\\n=== Data-source {ds_id} ({src['name']}) ===\")\n",
    "\n",
    "        # 2) list all ingestion jobs (newest first)\n",
    "        jobs = bedrock.list_ingestion_jobs(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            dataSourceId=ds_id,\n",
    "            sortBy={\"attribute\": \"STARTED_AT\", \"order\": \"DESCENDING\"},\n",
    "            maxResults=100\n",
    "        )[\"ingestionJobSummaries\"]\n",
    "\n",
    "        if not jobs:\n",
    "            print(\"  (no ingestion jobs yet)\")\n",
    "            continue\n",
    "\n",
    "        # 3) fetch & print the full record for every job found\n",
    "        for j in jobs:\n",
    "            job_id = j[\"ingestionJobId\"]\n",
    "            detail = bedrock.get_ingestion_job(\n",
    "                knowledgeBaseId=kb_id,\n",
    "                dataSourceId=ds_id,\n",
    "                ingestionJobId=job_id\n",
    "            )[\"ingestionJob\"]\n",
    "\n",
    "            print(f\"\\n  Job {job_id}  |  status: {detail['status']}\")\n",
    "            if detail.get(\"failureReasons\"):\n",
    "                print(\"     failureReasons:\", detail[\"failureReasons\"])\n",
    "            if detail.get(\"statistics\"):\n",
    "                print(\"     statistics:\")\n",
    "                pprint(detail[\"statistics\"], indent=10)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Run it\n",
    "# ------------------------------------------------------------------\n",
    "get_ingestion_jobs(KB_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d7153",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "Please make sure to uncomment and run the below section to delete all the resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2299290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Data source deleted =========\n",
      "======== Knowledge base deleted =========\n",
      "======== All IAM roles and policies deleted =========\n"
     ]
    }
   ],
   "source": [
    "# # Delete resources\n",
    "# print(\"===============================Deleteing resources ==============================\\n\")\n",
    "structured_kb.delete_kb( delete_iam_roles_and_policies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c2089dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Redshift environment cleanup...\n",
      "============================================================\n",
      "Step 1: Deleting Redshift workgroup sds-ecommerce-wg-1093701\n",
      "  Workgroup deletion initiated\n",
      "  Waiting for workgroup sds-ecommerce-wg-1093701 to be deleted...\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup deleted successfully\n",
      "\n",
      "Step 2: Deleting Redshift namespace sds-ecommerce-1093701\n",
      "  Namespace deletion initiated\n",
      "  Waiting for namespace sds-ecommerce-1093701 to be deleted...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace deleted successfully\n",
      "\n",
      "Step 3: Deleting S3 bucket sds-ecommerce-redshift-1093701\n",
      "  Emptying bucket contents...\n",
      "    Deleted 4 objects\n",
      "  Deleting bucket...\n",
      "  S3 bucket deleted successfully\n",
      "\n",
      "Step 4: Deleting Secrets Manager secret redshift-credentials-1093701\n",
      "  Secrets Manager secret deleted successfully\n",
      "\n",
      "Step 5: Deleting IAM role RedshiftS3AccessRole-1093701\n",
      "  Detaching managed policies...\n",
      "    Detached policy: AmazonS3ReadOnlyAccess\n",
      "  Deleting inline policies...\n",
      "  IAM role deleted successfully\n",
      "\n",
      "============================================================\n",
      "Redshift environment cleanup completed\n",
      "\n",
      "Summary of deleted resources:\n",
      "  - Redshift Workgroup: sds-ecommerce-wg-1093701\n",
      "  - Redshift Namespace: sds-ecommerce-1093701\n",
      "  - S3 Bucket: sds-ecommerce-redshift-1093701\n",
      "  - Secrets Manager Secret: redshift-credentials-1093701\n",
      "  - IAM Role: RedshiftS3AccessRole-1093701\n"
     ]
    }
   ],
   "source": [
    "def cleanup_redshift_environment():\n",
    "    \"\"\"\n",
    "    Delete all Redshift-related resources including workgroup, namespace, S3 bucket, and IAM role.\n",
    "    Uses the existing variables defined in the notebook.\n",
    "    \"\"\"\n",
    "    import boto3\n",
    "    import time\n",
    "    \n",
    "    # Initialize clients\n",
    "    session = boto3.session.Session()\n",
    "    region = session.region_name\n",
    "    redshift_client = boto3.client('redshift-serverless', region_name=region)\n",
    "    iam_client = boto3.client('iam')\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3_client = boto3.client('s3')\n",
    "    secrets_client = boto3.client('secretsmanager')\n",
    "    \n",
    "    def wait_for_workgroup_deleted(name, poll_interval=10, max_attempts=60):\n",
    "        \"\"\"Wait until workgroup is completely deleted\"\"\"\n",
    "        print(f\"  Waiting for workgroup {name} to be deleted...\")\n",
    "        attempts = 0\n",
    "        while attempts < max_attempts:\n",
    "            try:\n",
    "                wg = redshift_client.get_workgroup(workgroupName=name)[\"workgroup\"]\n",
    "                status = wg[\"status\"]\n",
    "                print(f\"    Workgroup status: {status}\")\n",
    "                if status == \"DELETED\":\n",
    "                    break\n",
    "                time.sleep(poll_interval)\n",
    "                attempts += 1\n",
    "            except redshift_client.exceptions.ResourceNotFoundException:\n",
    "                print(\"    Workgroup deleted successfully\")\n",
    "                return\n",
    "        \n",
    "        if attempts >= max_attempts:\n",
    "            print(f\"    Warning: Timeout waiting for workgroup deletion after {max_attempts * poll_interval} seconds\")\n",
    "    \n",
    "    def wait_for_namespace_deleted(name, poll_interval=10, max_attempts=60):\n",
    "        \"\"\"Wait until namespace is completely deleted\"\"\"\n",
    "        print(f\"  Waiting for namespace {name} to be deleted...\")\n",
    "        attempts = 0\n",
    "        while attempts < max_attempts:\n",
    "            try:\n",
    "                redshift_client.get_namespace(namespaceName=name)\n",
    "                print(f\"    Namespace still exists, waiting...\")\n",
    "                time.sleep(poll_interval)\n",
    "                attempts += 1\n",
    "            except redshift_client.exceptions.ResourceNotFoundException:\n",
    "                print(\"    Namespace deleted successfully\")\n",
    "                return\n",
    "        \n",
    "        if attempts >= max_attempts:\n",
    "            print(f\"    Warning: Timeout waiting for namespace deletion after {max_attempts * poll_interval} seconds\")\n",
    "    \n",
    "    print(\"Starting Redshift environment cleanup...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Delete Redshift workgroup first\n",
    "    print(f\"Step 1: Deleting Redshift workgroup {REDSHIFT_WORKGROUP}\")\n",
    "    try:\n",
    "        redshift_client.delete_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
    "        print(\"  Workgroup deletion initiated\")\n",
    "        wait_for_workgroup_deleted(REDSHIFT_WORKGROUP)\n",
    "    except redshift_client.exceptions.ResourceNotFoundException:\n",
    "        print(\"  Workgroup already deleted or does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error deleting workgroup: {str(e)}\")\n",
    "    \n",
    "    # 2. Delete Redshift namespace\n",
    "    print(f\"\\nStep 2: Deleting Redshift namespace {REDSHIFT_NAMESPACE}\")\n",
    "    try:\n",
    "        redshift_client.delete_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
    "        print(\"  Namespace deletion initiated\")\n",
    "        wait_for_namespace_deleted(REDSHIFT_NAMESPACE)\n",
    "    except redshift_client.exceptions.ResourceNotFoundException:\n",
    "        print(\"  Namespace already deleted or does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error deleting namespace: {str(e)}\")\n",
    "    \n",
    "    # 3. Empty and delete S3 bucket\n",
    "    print(f\"\\nStep 3: Deleting S3 bucket {S3_BUCKET}\")\n",
    "    try:\n",
    "        bucket = s3.Bucket(S3_BUCKET)\n",
    "        \n",
    "        # Check if bucket exists\n",
    "        s3_client.head_bucket(Bucket=S3_BUCKET)\n",
    "        \n",
    "        # Delete all objects in the bucket\n",
    "        print(\"  Emptying bucket contents...\")\n",
    "        objects_to_delete = []\n",
    "        for obj in bucket.objects.all():\n",
    "            objects_to_delete.append({'Key': obj.key})\n",
    "        \n",
    "        if objects_to_delete:\n",
    "            bucket.delete_objects(Delete={'Objects': objects_to_delete})\n",
    "            print(f\"    Deleted {len(objects_to_delete)} objects\")\n",
    "        else:\n",
    "            print(\"    Bucket was already empty\")\n",
    "        \n",
    "        # Delete the bucket\n",
    "        print(\"  Deleting bucket...\")\n",
    "        bucket.delete()\n",
    "        print(\"  S3 bucket deleted successfully\")\n",
    "        \n",
    "    except s3_client.exceptions.NoSuchBucket:\n",
    "        print(\"  S3 bucket already deleted or does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error deleting S3 bucket: {str(e)}\")\n",
    "    \n",
    "    # 4. Delete Secrets Manager secret\n",
    "    print(f\"\\nStep 4: Deleting Secrets Manager secret {secret_name}\")\n",
    "    try:\n",
    "        secrets_client.delete_secret(\n",
    "            SecretId=secret_name,\n",
    "            ForceDeleteWithoutRecovery=True  # Skip recovery window for cleanup\n",
    "        )\n",
    "        print(\"  Secrets Manager secret deleted successfully\")\n",
    "    except secrets_client.exceptions.ResourceNotFoundException:\n",
    "        print(\"  Secret already deleted or does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error deleting secret: {str(e)}\")\n",
    "    \n",
    "    # 5. Delete IAM role and policies\n",
    "    print(f\"\\nStep 5: Deleting IAM role {redshift_role_arn.split('/')[-1]}\")\n",
    "    role_name = redshift_role_arn.split('/')[-1]\n",
    "    try:\n",
    "        # Check if role exists\n",
    "        iam_client.get_role(RoleName=role_name)\n",
    "        \n",
    "        # Detach managed policies\n",
    "        print(\"  Detaching managed policies...\")\n",
    "        attached_policies = iam_client.list_attached_role_policies(RoleName=role_name)['AttachedPolicies']\n",
    "        for policy in attached_policies:\n",
    "            policy_arn = policy['PolicyArn']\n",
    "            iam_client.detach_role_policy(RoleName=role_name, PolicyArn=policy_arn)\n",
    "            print(f\"    Detached policy: {policy['PolicyName']}\")\n",
    "            \n",
    "            # Delete custom policies (not AWS managed)\n",
    "            if not policy_arn.startswith('arn:aws:iam::aws:policy/'):\n",
    "                try:\n",
    "                    iam_client.delete_policy(PolicyArn=policy_arn)\n",
    "                    print(f\"    Deleted custom policy: {policy['PolicyName']}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Could not delete policy {policy['PolicyName']}: {str(e)}\")\n",
    "        \n",
    "        # Delete inline policies\n",
    "        print(\"  Deleting inline policies...\")\n",
    "        inline_policies = iam_client.list_role_policies(RoleName=role_name)['PolicyNames']\n",
    "        for policy_name in inline_policies:\n",
    "            iam_client.delete_role_policy(RoleName=role_name, PolicyName=policy_name)\n",
    "            print(f\"    Deleted inline policy: {policy_name}\")\n",
    "        \n",
    "        # Delete the role\n",
    "        iam_client.delete_role(RoleName=role_name)\n",
    "        print(\"  IAM role deleted successfully\")\n",
    "        \n",
    "    except iam_client.exceptions.NoSuchEntityException:\n",
    "        print(\"  IAM role already deleted or does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error deleting IAM role: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Redshift environment cleanup completed\")\n",
    "    print(\"\\nSummary of deleted resources:\")\n",
    "    print(f\"  - Redshift Workgroup: {REDSHIFT_WORKGROUP}\")\n",
    "    print(f\"  - Redshift Namespace: {REDSHIFT_NAMESPACE}\")\n",
    "    print(f\"  - S3 Bucket: {S3_BUCKET}\")\n",
    "    print(f\"  - Secrets Manager Secret: {secret_name}\")\n",
    "    print(f\"  - IAM Role: {role_name}\")\n",
    "\n",
    "# Usage:\n",
    "cleanup_redshift_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ad527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-on-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
