{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725828ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import logging\n",
    "import requests\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a61171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS clients\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "TARGET_REGION = \"us-east-1\"\n",
    "boto3.setup_default_session(region_name=TARGET_REGION)\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "sts_client = boto3.client('sts')\n",
    "redshift_client = boto3.client('redshift-serverless', region_name=region)\n",
    "redshift_data_client = boto3.client('redshift-data', region_name=region)\n",
    "iam_client = boto3.client('iam')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e5d40a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using suffix: 1122011\n"
     ]
    }
   ],
   "source": [
    "# Generate unique suffix for resource names\n",
    "current_time = time.time()\n",
    "timestamp_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(current_time))[-7:]\n",
    "suffix = f\"{timestamp_str}\"\n",
    "\n",
    "print(f\"Using suffix: {suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d05e550",
   "metadata": {},
   "source": [
    "## Step 1: Download Bedrock Knowledge Base Utilities\n",
    "\n",
    "Lets download the structured knowledge base utility to help with Knowledge Base configuration and creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91ab0fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded structured KB utils to utils/structured_knowledge_base.py\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/rag/knowledge-bases/features-examples/utils/structured_knowledge_base.py\"\n",
    "target_path = \"utils/structured_knowledge_base.py\"\n",
    "os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "response = requests.get(url)\n",
    "with open(target_path, \"w\") as f:\n",
    "    f.write(response.text)\n",
    "print(f\"Downloaded structured KB utils to {target_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62900bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.structured_knowledge_base import BedrockStructuredKnowledgeBase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2ccb8",
   "metadata": {},
   "source": [
    "## Step 2: Set up Redshift Serverless Infrastructure\n",
    "\n",
    "Next we will create the necessary Redshift Serverless components: namespace and workgroup. This infrastructure will host our structured data that the Knowledge Base will query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7182df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift Namespace: sds-ecommerce-1122011\n",
      "Redshift Workgroup: sds-ecommerce-wg-1122011\n",
      "Database: sds-ecommerce\n",
      "S3 Bucket: sds-ecommerce-redshift-1122011\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Redshift resources\n",
    "REDSHIFT_NAMESPACE = f'sds-ecommerce-{suffix}'\n",
    "REDSHIFT_WORKGROUP = f'sds-ecommerce-wg-{suffix}'\n",
    "REDSHIFT_DATABASE = f'sds-ecommerce'\n",
    "S3_BUCKET = f'sds-ecommerce-redshift-{suffix}'\n",
    "\n",
    "print(f\"Redshift Namespace: {REDSHIFT_NAMESPACE}\")\n",
    "print(f\"Redshift Workgroup: {REDSHIFT_WORKGROUP}\")\n",
    "print(f\"Database: {REDSHIFT_DATABASE}\")\n",
    "print(f\"S3 Bucket: {S3_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109b83d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created role RedshiftS3AccessRole-1122011\n",
      "Redshift IAM Role ARN: arn:aws:iam::533267284022:role/RedshiftS3AccessRole-1122011\n"
     ]
    }
   ],
   "source": [
    "def create_iam_role_for_redshift():\n",
    "    \"\"\"Create IAM role for Redshift to access S3\"\"\"\n",
    "    try:\n",
    "        # Get account ID\n",
    "        account_id = sts_client.get_caller_identity()['Account']\n",
    "        \n",
    "        # Create IAM role if it doesn't exist\n",
    "        role_name = f'RedshiftS3AccessRole-{suffix}'\n",
    "        try:\n",
    "            role_response = iam_client.get_role(RoleName=role_name)\n",
    "            print(f'Role {role_name} already exists')\n",
    "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
    "        except iam_client.exceptions.NoSuchEntityException:\n",
    "            trust_policy = {\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"redshift.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            iam_client.create_role(\n",
    "                RoleName=role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    "            )\n",
    "            \n",
    "            # Attach necessary policies\n",
    "            iam_client.attach_role_policy(\n",
    "                RoleName=role_name,\n",
    "                PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n",
    "            )\n",
    "            \n",
    "            print(f'Created role {role_name}')\n",
    "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error creating IAM role: {str(e)}')\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "redshift_role_arn = create_iam_role_for_redshift()\n",
    "print(f\"Redshift IAM Role ARN: {redshift_role_arn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59245e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating namespace sds-ecommerce-1122011...\n",
      "Created namespace sds-ecommerce-1122011\n",
      "Waiting for namespace to be available...\n",
      "Namespace sds-ecommerce-1122011 is now available\n"
     ]
    }
   ],
   "source": [
    "def create_redshift_namespace():\n",
    "    \"\"\"Create Redshift Serverless namespace\"\"\"\n",
    "    try:\n",
    "        # Check if namespace already exists\n",
    "        try:\n",
    "            response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
    "            print(f'Namespace {REDSHIFT_NAMESPACE} already exists')\n",
    "            return response['namespace']\n",
    "        except redshift_client.exceptions.ResourceNotFoundException:\n",
    "            print(f'Creating namespace {REDSHIFT_NAMESPACE}...')\n",
    "        \n",
    "        # Create the namespace\n",
    "        response = redshift_client.create_namespace(\n",
    "            namespaceName=REDSHIFT_NAMESPACE,\n",
    "            adminUsername='admin',\n",
    "            adminUserPassword='TempPassword123!',  # Change this in production\n",
    "            dbName=REDSHIFT_DATABASE,\n",
    "            defaultIamRoleArn=redshift_role_arn,\n",
    "            iamRoles=[redshift_role_arn]\n",
    "        )\n",
    "        \n",
    "        print(f'Created namespace {REDSHIFT_NAMESPACE}')\n",
    "        \n",
    "        # Wait for namespace to be available\n",
    "        print('Waiting for namespace to be available...')\n",
    "        max_attempts = 30\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                namespace_response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
    "                status = namespace_response['namespace']['status']\n",
    "                if status == 'AVAILABLE':\n",
    "                    print(f'Namespace {REDSHIFT_NAMESPACE} is now available')\n",
    "                    return namespace_response['namespace']\n",
    "                else:\n",
    "                    print(f'Namespace status: {status}, waiting...')\n",
    "                    time.sleep(10)\n",
    "            except Exception as e:\n",
    "                print(f'Error checking namespace status: {str(e)}, retrying...')\n",
    "                time.sleep(10)\n",
    "        \n",
    "        print('Timeout waiting for namespace, but proceeding...')\n",
    "        return response['namespace']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error creating namespace: {str(e)}')\n",
    "        raise\n",
    "\n",
    "# Create namespace\n",
    "namespace = create_redshift_namespace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "372e2dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating workgroup sds-ecommerce-wg-1122011...\n",
      "Created workgroup sds-ecommerce-wg-1122011\n",
      "Waiting for workgroup to be available...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup sds-ecommerce-wg-1122011 is now available\n",
      "Workgroup ARN: arn:aws:redshift-serverless:us-east-1:533267284022:workgroup/229deaab-1493-4fae-be7f-e08d65b05aa3\n"
     ]
    }
   ],
   "source": [
    "def create_redshift_workgroup():\n",
    "    \"\"\"Create Redshift Serverless workgroup\"\"\"\n",
    "    try:\n",
    "        # Check if workgroup already exists\n",
    "        try:\n",
    "            response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
    "            print(f'Workgroup {REDSHIFT_WORKGROUP} already exists')\n",
    "            return response['workgroup']\n",
    "        except redshift_client.exceptions.ResourceNotFoundException:\n",
    "            print(f'Creating workgroup {REDSHIFT_WORKGROUP}...')\n",
    "        \n",
    "        # Create the workgroup\n",
    "        response = redshift_client.create_workgroup(\n",
    "            workgroupName=REDSHIFT_WORKGROUP,\n",
    "            namespaceName=REDSHIFT_NAMESPACE,\n",
    "            baseCapacity=8,  # Minimum base capacity\n",
    "            enhancedVpcRouting=False,\n",
    "            publiclyAccessible=True,\n",
    "            configParameters=[\n",
    "                {\n",
    "                    'parameterKey': 'enable_user_activity_logging',\n",
    "                    'parameterValue': 'true'\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f'Created workgroup {REDSHIFT_WORKGROUP}')\n",
    "        \n",
    "        # Wait for workgroup to be available\n",
    "        print('Waiting for workgroup to be available...')\n",
    "        max_attempts = 30\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                workgroup_response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
    "                status = workgroup_response['workgroup']['status']\n",
    "                if status == 'AVAILABLE':\n",
    "                    print(f'Workgroup {REDSHIFT_WORKGROUP} is now available')\n",
    "                    return workgroup_response['workgroup']\n",
    "                else:\n",
    "                    print(f'Workgroup status: {status}, waiting...')\n",
    "                    time.sleep(10)\n",
    "            except Exception as e:\n",
    "                print(f'Error checking workgroup status: {str(e)}, retrying...')\n",
    "                time.sleep(10)\n",
    "        \n",
    "        print('Timeout waiting for workgroup, but proceeding...')\n",
    "        return response['workgroup']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error creating workgroup: {str(e)}')\n",
    "        raise\n",
    "\n",
    "# Create workgroup\n",
    "workgroup = create_redshift_workgroup()\n",
    "workgroup_arn = workgroup['workgroupArn']\n",
    "print(f\"Workgroup ARN: {workgroup_arn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a03caa",
   "metadata": {},
   "source": [
    "## Step 3: Create S3 Bucket and Load Sample Data\n",
    "\n",
    "We will create an S3 bucket to stage our sample e-commerce data before loading it into Redshift tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "231df9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9976c340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket sds-ecommerce-redshift-1122011 in us-east-1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_s3_bucket():\n",
    "    \"\"\"Create the staging S3 bucket in the same Region as this session.\"\"\"\n",
    "    # Build a region-pinned client so the endpoint and LocationConstraint match.\n",
    "    session       = boto3.session.Session()\n",
    "    bucket_region = session.region_name or \"us-east-1\"\n",
    "    s3            = boto3.client(\"s3\", region_name=bucket_region)\n",
    "\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=S3_BUCKET)\n",
    "        print(f\"Bucket {S3_BUCKET} already exists\")\n",
    "    except s3.exceptions.ClientError as e:\n",
    "        # Any 404/NoSuchBucket falls through to creation; re-raise everything else.\n",
    "        if e.response[\"Error\"][\"Code\"] not in (\"404\", \"NoSuchBucket\"):\n",
    "            raise\n",
    "\n",
    "        # For us-east-1 the API requires *no* LocationConstraint.\n",
    "        cfg = (\n",
    "            {}\n",
    "            if bucket_region == \"us-east-1\"\n",
    "            else {\"LocationConstraint\": bucket_region}\n",
    "        )\n",
    "\n",
    "        s3.create_bucket(\n",
    "            Bucket=S3_BUCKET,\n",
    "            **({\"CreateBucketConfiguration\": cfg} if cfg else {})\n",
    "        )\n",
    "        print(f\"Created bucket {S3_BUCKET} in {bucket_region}\")\n",
    "\n",
    "# Create S3 bucket\n",
    "create_s3_bucket()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1be47dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading sample data files to S3...\n",
      "Uploaded orders.csv (1.8 MB) to S3\n",
      "Uploaded order_items.csv (1.3 MB) to S3\n",
      "Uploaded payments.csv (0.8 MB) to S3\n",
      "Uploaded reviews.csv (0.5 MB) to S3\n",
      "\n",
      "Successfully uploaded all 4 data files to S3\n"
     ]
    }
   ],
   "source": [
    "def upload_sample_data():\n",
    "    \"\"\"Upload sample CSV files to S3\"\"\"\n",
    "    data_files = ['orders.csv', 'order_items.csv', 'payments.csv', 'reviews.csv']\n",
    "    sds_directory = 'sample_data'\n",
    "    \n",
    "    print(\"Uploading sample data files to S3...\")\n",
    "    files_found = 0\n",
    "    \n",
    "    for file_name in data_files:\n",
    "        local_path = os.path.join(sds_directory, file_name)\n",
    "        if os.path.exists(local_path):\n",
    "            # Get file size for informational purposes\n",
    "            file_size = os.path.getsize(local_path)\n",
    "            file_size_mb = file_size / (1024 * 1024)\n",
    "            \n",
    "            s3_client.upload_file(local_path, S3_BUCKET, file_name)\n",
    "            print(f'Uploaded {file_name} ({file_size_mb:.1f} MB) to S3')\n",
    "            files_found += 1\n",
    "        else:\n",
    "            print(f'Warning: {local_path} not found')\n",
    "    \n",
    "    if files_found == len(data_files):\n",
    "        print(f\"\\nSuccessfully uploaded all {files_found} data files to S3\")\n",
    "    else:\n",
    "        print(f\"\\nOnly {files_found} out of {len(data_files)} files were found and uploaded\")\n",
    "\n",
    "# Upload sample data\n",
    "upload_sample_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd94c8",
   "metadata": {},
   "source": [
    "## Step 4: Create Redshift Tables and Load Data\n",
    "\n",
    "Now we will create the database tables in Redshift and load our sample e-commerce data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a4b8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_statement(statement_id):\n",
    "    \"\"\"Wait for a Redshift Data API statement to complete\"\"\"\n",
    "    max_attempts = 30\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            response = redshift_data_client.describe_statement(Id=statement_id)\n",
    "            status = response['Status']\n",
    "            if status == 'FINISHED':\n",
    "                return response\n",
    "            elif status == 'FAILED':\n",
    "                raise Exception(f\"Statement failed: {response.get('Error', 'Unknown error')}\")\n",
    "            elif status == 'CANCELLED':\n",
    "                raise Exception(\"Statement was cancelled\")\n",
    "            else:\n",
    "                print(f\"Statement status: {status}, waiting...\")\n",
    "                time.sleep(5)\n",
    "        except Exception as e:\n",
    "            if 'Statement failed' in str(e) or 'cancelled' in str(e):\n",
    "                raise\n",
    "            print(f\"Error checking statement status: {str(e)}, retrying...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    raise Exception(\"Timeout waiting for statement to complete\")\n",
    "\n",
    "def run_redshift_statement(sql_statement):\n",
    "    \"\"\"Execute a SQL statement in Redshift\"\"\"\n",
    "    try:\n",
    "        response = redshift_data_client.execute_statement(\n",
    "            WorkgroupName=REDSHIFT_WORKGROUP,\n",
    "            Database=REDSHIFT_DATABASE,\n",
    "            Sql=sql_statement\n",
    "        )\n",
    "        statement_id = response['Id']\n",
    "        print(f\"Executing statement: {statement_id}\")\n",
    "        result = wait_for_statement(statement_id)\n",
    "        print(f\"Statement completed successfully\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing statement: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c7d3205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table: orders\n",
      "Executing statement: eb4da616-d8a7-4c3c-8c7d-2e66cc02eb79\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Created table: orders\n",
      "-------------\n",
      "Creating table: order_items\n",
      "Executing statement: e05be4a8-bc21-4e6d-b54b-4b217c11de5d\n",
      "Statement status: STARTED, waiting...\n",
      "Statement completed successfully\n",
      "Created table: order_items\n",
      "-------------\n",
      "Creating table: payments\n",
      "Executing statement: 841c158b-228f-4dec-a1b9-ac465fc02ff2\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Created table: payments\n",
      "-------------\n",
      "Creating table: reviews\n",
      "Executing statement: 26e7bc07-674b-4a7b-8ca4-9542a595af87\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Created table: reviews\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# Create tables in Redshift\n",
    "def create_tables():\n",
    "    \"\"\"Create all necessary tables in Redshift\"\"\"\n",
    "    \n",
    "    # Orders table\n",
    "    orders_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS orders (\n",
    "        order_id VARCHAR(255) PRIMARY KEY,\n",
    "        customer_id VARCHAR(255),\n",
    "        order_total DECIMAL(10,2),\n",
    "        order_status VARCHAR(50),\n",
    "        payment_method VARCHAR(50),\n",
    "        shipping_address TEXT,\n",
    "        created_at TIMESTAMP,\n",
    "        updated_at TIMESTAMP\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Order Items table\n",
    "    order_items_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS order_items (\n",
    "        order_item_id VARCHAR(255) PRIMARY KEY,\n",
    "        order_id VARCHAR(255),\n",
    "        product_id VARCHAR(255),\n",
    "        quantity INTEGER,\n",
    "        price DECIMAL(10,2)\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Payments table\n",
    "    payments_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS payments (\n",
    "        payment_id VARCHAR(255) PRIMARY KEY,\n",
    "        order_id VARCHAR(255),\n",
    "        customer_id VARCHAR(255),\n",
    "        amount DECIMAL(10,2),\n",
    "        payment_method VARCHAR(50),\n",
    "        payment_status VARCHAR(50),\n",
    "        created_at DATE\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reviews table\n",
    "    reviews_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS reviews (\n",
    "        review_id VARCHAR(255) PRIMARY KEY,\n",
    "        product_id VARCHAR(255),\n",
    "        customer_id VARCHAR(255),\n",
    "        rating INTEGER,\n",
    "        created_at DATE\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    tables = {\n",
    "        'orders': orders_sql,\n",
    "        'order_items': order_items_sql,\n",
    "        'payments': payments_sql,\n",
    "        'reviews': reviews_sql\n",
    "    }\n",
    "    \n",
    "    for table_name, sql in tables.items():\n",
    "        print(f\"Creating table: {table_name}\")\n",
    "        run_redshift_statement(sql)\n",
    "        print(f\"Created table: {table_name}\")\n",
    "        print(\"-------------\")\n",
    "\n",
    "# Create tables\n",
    "create_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b248290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into orders from orders.csv\n",
      "Executing statement: c39d78c0-2eb8-41f3-81f1-f8a1e3ad4d1d\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into orders\n",
      "Loading data into order_items from order_items.csv\n",
      "Executing statement: 436ee9a3-3272-4d7e-8454-bec447a85734\n",
      "Statement status: STARTED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into order_items\n",
      "Loading data into payments from payments.csv\n",
      "Executing statement: 33098b36-5634-4d00-b3ba-243fc5d43caf\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into payments\n",
      "Loading data into reviews from reviews.csv\n",
      "Executing statement: 5637e1eb-2a53-4c2f-a2d2-94332ca109ce\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into reviews\n"
     ]
    }
   ],
   "source": [
    "# Load data from S3 into Redshift tables\n",
    "def load_data_from_s3():\n",
    "    \"\"\"Load data from S3 CSV files into Redshift tables\"\"\"\n",
    "    \n",
    "    tables_and_files = {\n",
    "        'orders': 'orders.csv',\n",
    "        'order_items': 'order_items.csv',\n",
    "        'payments': 'payments.csv',\n",
    "        'reviews': 'reviews.csv'\n",
    "    }\n",
    "    \n",
    "    for table_name, file_name in tables_and_files.items():\n",
    "        print(f\"Loading data into {table_name} from {file_name}\")\n",
    "        \n",
    "        copy_sql = f\"\"\"\n",
    "        COPY {table_name}\n",
    "        FROM 's3://{S3_BUCKET}/{file_name}'\n",
    "        IAM_ROLE '{redshift_role_arn}'\n",
    "        CSV\n",
    "        IGNOREHEADER 1\n",
    "        DELIMITER ','\n",
    "        REGION '{region}';\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            run_redshift_statement(copy_sql)\n",
    "            print(f\"Loaded data into {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data into {table_name}: {str(e)}\")\n",
    "\n",
    "# Load data from S3\n",
    "load_data_from_s3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e51221",
   "metadata": {},
   "source": [
    "## Step 5: Verify Data Load\n",
    "\n",
    "Let's verify that our data has been loaded correctly by running some sample queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea721a6",
   "metadata": {},
   "source": [
    "## Step 6: Create Bedrock Knowledge Base with Redshift Data Source\n",
    "\n",
    "Now we'll create the Bedrock Knowledge Base configured to use our Redshift data as a structured data source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7891a188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base Name: redshift-structured-kb-1122011\n"
     ]
    }
   ],
   "source": [
    "# Configure Knowledge Base parameters\n",
    "kb_name = f\"redshift-structured-kb-{suffix}\"\n",
    "kb_description = \"Structured Knowledge Base for e-commerce data queries using Redshift\"\n",
    "generation_model = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "print(f\"Knowledge Base Name: {kb_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7f9bc",
   "metadata": {},
   "source": [
    "Amazon Bedrock Knowledge Bases uses a service role to connect knowledge bases to structured data stores, retrieve data from these data stores, and generate SQL queries based on user queries and the structure of the data stores. There are several access patterns based on if you're using Redshift Serverless vs Redshift Provisioned Cluster. In this notebook, let's use `IAM Role + Redshift Serverless WorkGroup` access pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65137e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Knowledge Base parameters for Redshift Serverless with IAM authentication\n",
    "kb_config_param = {\n",
    "    \"type\": \"SQL\",\n",
    "    \"sqlKnowledgeBaseConfiguration\": {\n",
    "        \"type\": \"REDSHIFT\",\n",
    "        \"redshiftConfiguration\": {\n",
    "            \"storageConfigurations\": [{\n",
    "                \"type\": \"REDSHIFT\",\n",
    "                \"redshiftConfiguration\": {\n",
    "                    \"databaseName\": REDSHIFT_DATABASE\n",
    "                }\n",
    "            }],\n",
    "            \"queryEngineConfiguration\": {\n",
    "                \"type\": \"SERVERLESS\",\n",
    "                \"serverlessConfiguration\": {\n",
    "                    \"workgroupArn\": workgroup_arn,\n",
    "                    \"authConfiguration\": {}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e2fb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_config_param['sqlKnowledgeBaseConfiguration']['redshiftConfiguration']['queryEngineConfiguration']['serverlessConfiguration']['authConfiguration']['type'] = \"IAM\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "999301c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'SQL',\n",
       " 'sqlKnowledgeBaseConfiguration': {'type': 'REDSHIFT',\n",
       "  'redshiftConfiguration': {'storageConfigurations': [{'type': 'REDSHIFT',\n",
       "     'redshiftConfiguration': {'databaseName': 'sds-ecommerce'}}],\n",
       "   'queryEngineConfiguration': {'type': 'SERVERLESS',\n",
       "    'serverlessConfiguration': {'workgroupArn': 'arn:aws:redshift-serverless:us-east-1:533267284022:workgroup/229deaab-1493-4fae-be7f-e08d65b05aa3',\n",
       "     'authConfiguration': {'type': 'IAM'}}}}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_config_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "932cac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "Step 1 - Creating Knowledge Base Execution Role (AmazonBedrockExecutionRoleForStructuredKnowledgeBase_1122011) and Policies\n",
      "========================================================================================\n",
      "Step 2 - Creating Knowledge Base\n",
      "{ 'createdAt': datetime.datetime(2025, 6, 21, 19, 22, 15, 496706, tzinfo=tzutc()),\n",
      "  'description': 'Structured Knowledge Base for e-commerce data queries using '\n",
      "                 'Redshift',\n",
      "  'knowledgeBaseArn': 'arn:aws:bedrock:us-east-1:533267284022:knowledge-base/MERU0MPC8X',\n",
      "  'knowledgeBaseConfiguration': { 'sqlKnowledgeBaseConfiguration': { 'redshiftConfiguration': { 'queryEngineConfiguration': { 'serverlessConfiguration': { 'authConfiguration': { 'type': 'IAM'},\n",
      "                                                                                                                                                           'workgroupArn': 'arn:aws:redshift-serverless:us-east-1:533267284022:workgroup/229deaab-1493-4fae-be7f-e08d65b05aa3'},\n",
      "                                                                                                                              'type': 'SERVERLESS'},\n",
      "                                                                                                'storageConfigurations': [ { 'redshiftConfiguration': { 'databaseName': 'sds-ecommerce'},\n",
      "                                                                                                                             'type': 'REDSHIFT'}]},\n",
      "                                                                     'type': 'REDSHIFT'},\n",
      "                                  'type': 'SQL'},\n",
      "  'knowledgeBaseId': 'MERU0MPC8X',\n",
      "  'name': 'redshift-structured-kb-1122011',\n",
      "  'roleArn': 'arn:aws:iam::533267284022:role/AmazonBedrockExecutionRoleForStructuredKnowledgeBase_1122011',\n",
      "  'status': 'CREATING',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 21, 19, 22, 15, 496706, tzinfo=tzutc())}\n",
      "Creating Data Sources aka query engine\n",
      "{ 'createdAt': datetime.datetime(2025, 6, 21, 19, 22, 15, 704823, tzinfo=tzutc()),\n",
      "  'dataSourceConfiguration': {'type': 'REDSHIFT_METADATA'},\n",
      "  'dataSourceId': 'QA6E9AZQBO',\n",
      "  'description': 'Query engine',\n",
      "  'knowledgeBaseId': 'MERU0MPC8X',\n",
      "  'name': 'redshift-structured-kb-1122011-ds',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 21, 19, 22, 15, 704823, tzinfo=tzutc())}\n",
      "========================================================================================\n",
      "Knowledge Base created successfully!\n",
      "'MERU0MPC8X'\n",
      "Knowledge Base ID: MERU0MPC8X\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    structured_kb = BedrockStructuredKnowledgeBase(\n",
    "        kb_name=kb_name,\n",
    "        kb_description=kb_description,\n",
    "        workgroup_arn=workgroup_arn,\n",
    "        kbConfigParam=kb_config_param,\n",
    "        generation_model=generation_model,\n",
    "        suffix=suffix\n",
    "    )\n",
    "    \n",
    "    print(\"Knowledge Base created successfully!\")\n",
    "    kb_id = structured_kb.get_knowledge_base_id()\n",
    "    print(f\"Knowledge Base ID: {kb_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating Knowledge Base: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22c74e",
   "metadata": {},
   "source": [
    "## Step 6: Database Access Configuration for IAM Role + Redshift Serverless WorkGroup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00549fb8",
   "metadata": {},
   "source": [
    "For the IAM Role + Redshift Serverless WorkGroup access pattern, you must configure database-level permissions for the IAM role used by Bedrock Knowledge Base.\n",
    "\n",
    "1. **Create IAM-based database user**: Map the IAM role to a database user in Redshift\n",
    "2. **Grant appropriate permissions**: Provide SELECT access to the relevant schemas and tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "260dbdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracted Role Name: AmazonBedrockExecutionRoleForStructuredKnowledgeBase_1122011\n"
     ]
    }
   ],
   "source": [
    "# Extract the IAM role name from the ARN for database user creation\n",
    "kb_details = structured_kb.knowledge_base\n",
    "\n",
    "bedrock_role_arn = kb_details['roleArn']\n",
    "bedrock_role_name = bedrock_role_arn.split('/')[-1]\n",
    "print(f\"   Extracted Role Name: {bedrock_role_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a7e47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating user: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_1122011\n",
      "Executing statement: 5a7182d8-ece4-4b25-8f5b-025b9c86a530\n",
      "Statement status: STARTED, waiting...\n",
      "Statement completed successfully\n",
      "IAM user created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the IAM user in Redshift (this is the critical missing step!)\n",
    "create_user_sql = f'CREATE USER \"IAMR:{bedrock_role_name}\" WITH PASSWORD DISABLE;'\n",
    "\n",
    "try:\n",
    "    print(f\"Creating user: IAMR:{bedrock_role_name}\")\n",
    "    run_redshift_statement(create_user_sql)\n",
    "    print(\"IAM user created successfully!\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(\"User already exists, continuing...\")\n",
    "    else:\n",
    "        print(f\"Error creating user: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53ab5fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granting SELECT permissions to: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_1122011\n",
      "Executing statement: a9003ac8-6fb3-432c-b02e-daa123ef237d\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "SELECT permissions granted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Grant SELECT on all tables in public schema\n",
    "grant_select_sql = f'GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"IAMR:{bedrock_role_name}\";'\n",
    "\n",
    "try:\n",
    "    print(f\"Granting SELECT permissions to: IAMR:{bedrock_role_name}\")\n",
    "    run_redshift_statement(grant_select_sql)\n",
    "    print(\"SELECT permissions granted successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error granting permissions: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3bd6df",
   "metadata": {},
   "source": [
    "## Step 7: Start Ingestion Job\n",
    "\n",
    "Now that the database permissions are properly configured, let's start the ingestion job to sync the data from the Redshift database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0097a5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job  started successfully\n",
      "\n",
      "{ 'dataSourceId': 'QA6E9AZQBO',\n",
      "  'ingestionJobId': 'HXSAVT8YXC',\n",
      "  'knowledgeBaseId': 'MERU0MPC8X',\n",
      "  'startedAt': datetime.datetime(2025, 6, 21, 19, 23, 27, 971321, tzinfo=tzutc()),\n",
      "  'status': 'FAILED',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 21, 19, 23, 31, 156217, tzinfo=tzutc())}\n",
      ".....\r"
     ]
    }
   ],
   "source": [
    "# Wait a bit for the Knowledge Base to be fully ready\n",
    "time.sleep(60)\n",
    "structured_kb.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "552d4c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for querying the Knowledge Base\n",
    "\n",
    "def query_with_retrieve_and_generate(kb_id, query):\n",
    "    \"\"\"Query using retrieve_and_generate API - returns natural language response\"\"\"\n",
    "    try:\n",
    "        response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "            input={\"text\": query},\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                \"type\": \"KNOWLEDGE_BASE\",\n",
    "                \"knowledgeBaseConfiguration\": {\n",
    "                    'knowledgeBaseId': kb_id,\n",
    "                    \"modelArn\": f\"arn:aws:bedrock:{region}::foundation-model/{generation_model}\",\n",
    "                    \"retrievalConfiguration\": {\n",
    "                        \"vectorSearchConfiguration\": {\n",
    "                            \"numberOfResults\": 5\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        return response['output']['text']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def generate_sql_query(kb_arn, query):\n",
    "    \"\"\"Generate SQL query from natural language\"\"\"\n",
    "    try:\n",
    "        response = bedrock_agent_runtime_client.generate_query(\n",
    "            queryGenerationInput={\n",
    "                \"text\": query,\n",
    "                \"type\": \"TEXT\"\n",
    "            },\n",
    "            transformationConfiguration={\n",
    "                \"mode\": \"TEXT_TO_SQL\",\n",
    "                \"textToSqlConfiguration\": {\n",
    "                    \"type\": \"KNOWLEDGE_BASE\",\n",
    "                    \"knowledgeBaseConfiguration\": {\n",
    "                        \"knowledgeBaseArn\": kb_details['knowledgeBaseArn']\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.get('queries') and len(response['queries']) > 0:\n",
    "            return response['queries'][0]['sql']\n",
    "        else:\n",
    "            return \"No SQL generated\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"✅ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6ad67a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Knowledge Base with sample queries\n",
      "============================================================\n",
      "\n",
      "📝 Query 1: How many orders are in the database?\n",
      "--------------------------------------------------\n",
      "🤖 Natural Language Response:\n",
      "   Error: An error occurred (ValidationException) when calling the RetrieveAndGenerate operation: No metadata found. Please trigger an ingestion and try again. (Service: BedrockAgentRuntime, Status Code: 400, Request ID: 603fd599-45ce-4e19-abd1-b18220015471) (SDK Attempt Count: 1)\n",
      "\n",
      "🔧 Generated SQL:\n",
      "   SELECT COUNT(\"order_id\") AS \"total_orders\" FROM public.orders;\n",
      "\n",
      "==================================================\n",
      "\n",
      "📝 Query 2: What is the average order total?\n",
      "--------------------------------------------------\n",
      "🤖 Natural Language Response:\n",
      "   Error: An error occurred (ValidationException) when calling the RetrieveAndGenerate operation: Invocation of model ID anthropic.claude-3-5-haiku-20241022-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model. (Service: BedrockRuntime, Status Code: 400, Request ID: bd182ec8-5076-4fc2-a62e-1a77762f291a) (SDK Attempt Count: 1)\n",
      "\n",
      "🔧 Generated SQL:\n",
      "   SELECT AVG(\"order_total\") AS \"average_order_total\" FROM public.orders WHERE \"created_at\" <= '2025-06-21';\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"How many orders are in the database?\",\n",
    "    \"What is the average order total?\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing Knowledge Base with sample queries\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n📝 Query {i}: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get natural language response\n",
    "    print(\"🤖 Natural Language Response:\")\n",
    "    nl_response = query_with_retrieve_and_generate(kb_id, query)\n",
    "    print(f\"   {nl_response}\")\n",
    "    \n",
    "    # Get generated SQL\n",
    "    print(\"\\n🔧 Generated SQL:\")\n",
    "    sql_query = generate_sql_query(kb_details['knowledgeBaseArn'], query)\n",
    "    print(f\"   {sql_query}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c65ba69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data-source QA6E9AZQBO (redshift-structured-kb-1122011-ds) ===\n",
      "\n",
      "  Job HXSAVT8YXC  |  status: FAILED\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from pprint import pprint\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Set your knowledge-base Id here (10-char alpha-numeric string)\n",
    "# ------------------------------------------------------------------\n",
    "KB_ID = kb_id\n",
    "\n",
    "bedrock = boto3.client(\"bedrock-agent\")\n",
    "\n",
    "def get_ingestion_jobs(kb_id: str):\n",
    "    \"\"\"\n",
    "    Prints the detail of every ingestion job that was ever started\n",
    "    for every data-source attached to the given knowledge-base.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) find all data-sources for this KB\n",
    "    sources = bedrock.list_data_sources(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        maxResults=100\n",
    "    )[\"dataSourceSummaries\"]\n",
    "\n",
    "    if not sources:\n",
    "        print(\"No data-sources found for KB:\", kb_id)\n",
    "        return\n",
    "\n",
    "    for src in sources:\n",
    "        ds_id = src[\"dataSourceId\"]\n",
    "        print(f\"\\n=== Data-source {ds_id} ({src['name']}) ===\")\n",
    "\n",
    "        # 2) list all ingestion jobs (newest first)\n",
    "        jobs = bedrock.list_ingestion_jobs(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            dataSourceId=ds_id,\n",
    "            sortBy={\"attribute\": \"STARTED_AT\", \"order\": \"DESCENDING\"},\n",
    "            maxResults=100\n",
    "        )[\"ingestionJobSummaries\"]\n",
    "\n",
    "        if not jobs:\n",
    "            print(\"  (no ingestion jobs yet)\")\n",
    "            continue\n",
    "\n",
    "        # 3) fetch & print the full record for every job found\n",
    "        for j in jobs:\n",
    "            job_id = j[\"ingestionJobId\"]\n",
    "            detail = bedrock.get_ingestion_job(\n",
    "                knowledgeBaseId=kb_id,\n",
    "                dataSourceId=ds_id,\n",
    "                ingestionJobId=job_id\n",
    "            )[\"ingestionJob\"]\n",
    "\n",
    "            print(f\"\\n  Job {job_id}  |  status: {detail['status']}\")\n",
    "            if detail.get(\"failureReasons\"):\n",
    "                print(\"     failureReasons:\", detail[\"failureReasons\"])\n",
    "            if detail.get(\"statistics\"):\n",
    "                print(\"     statistics:\")\n",
    "                pprint(detail[\"statistics\"], indent=10)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Run it\n",
    "# ------------------------------------------------------------------\n",
    "get_ingestion_jobs(KB_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d7153",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "Please make sure to uncomment and run the below section to delete all the resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2299290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Data source deleted =========\n",
      "======== Knowledge base deleted =========\n",
      "======== All IAM roles and policies deleted =========\n"
     ]
    }
   ],
   "source": [
    "# # Delete resources\n",
    "# print(\"===============================Deleteing resources ==============================\\n\")\n",
    "structured_kb.delete_kb( delete_iam_roles_and_policies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c2089dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Redshift environment cleanup...\n",
      "============================================================\n",
      "Step 1: Deleting Redshift workgroup sds-ecommerce-wg-1122011\n",
      "  Workgroup deletion initiated\n",
      "  Waiting for workgroup sds-ecommerce-wg-1122011 to be deleted...\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup status: DELETING\n",
      "    Workgroup deleted successfully\n",
      "\n",
      "Step 2: Deleting Redshift namespace sds-ecommerce-1122011\n",
      "  Namespace deletion initiated\n",
      "  Waiting for namespace sds-ecommerce-1122011 to be deleted...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace still exists, waiting...\n",
      "    Namespace deleted successfully\n",
      "\n",
      "Step 3: Deleting S3 bucket sds-ecommerce-redshift-1122011\n",
      "  Emptying bucket contents...\n",
      "    Deleted 4 objects\n",
      "  Deleting bucket...\n",
      "  S3 bucket deleted successfully\n",
      "\n",
      "Step 4: Deleting IAM role RedshiftS3AccessRole-1122011\n",
      "  Detaching managed policies...\n",
      "    Detached policy: AmazonS3ReadOnlyAccess\n",
      "  Deleting inline policies...\n",
      "  IAM role deleted successfully\n",
      "\n",
      "============================================================\n",
      "Redshift environment cleanup completed\n",
      "\n",
      "Summary of deleted resources:\n",
      "  - Redshift Workgroup: sds-ecommerce-wg-1122011\n",
      "  - Redshift Namespace: sds-ecommerce-1122011\n",
      "  - S3 Bucket: sds-ecommerce-redshift-1122011\n",
      "  - IAM Role: RedshiftS3AccessRole-1122011\n"
     ]
    }
   ],
   "source": [
    "def cleanup_redshift_environment():\n",
    "    \"\"\"\n",
    "    Delete all Redshift-related resources including workgroup, namespace, S3 bucket, and IAM role.\n",
    "    Uses the existing variables defined in the notebook.\n",
    "    \"\"\"\n",
    "    import boto3\n",
    "    import time\n",
    "    \n",
    "    # Initialize clients\n",
    "    session = boto3.session.Session()\n",
    "    region = session.region_name\n",
    "    redshift_client = boto3.client('redshift-serverless', region_name=region)\n",
    "    iam_client = boto3.client('iam')\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    def wait_for_workgroup_deleted(name, poll_interval=10, max_attempts=60):\n",
    "        \"\"\"Wait until workgroup is completely deleted\"\"\"\n",
    "        print(f\"  Waiting for workgroup {name} to be deleted...\")\n",
    "        attempts = 0\n",
    "        while attempts < max_attempts:\n",
    "            try:\n",
    "                wg = redshift_client.get_workgroup(workgroupName=name)[\"workgroup\"]\n",
    "                status = wg[\"status\"]\n",
    "                print(f\"    Workgroup status: {status}\")\n",
    "                if status == \"DELETED\":\n",
    "                    break\n",
    "                time.sleep(poll_interval)\n",
    "                attempts += 1\n",
    "            except redshift_client.exceptions.ResourceNotFoundException:\n",
    "                print(\"    Workgroup deleted successfully\")\n",
    "                return\n",
    "        \n",
    "        if attempts >= max_attempts:\n",
    "            print(f\"    Warning: Timeout waiting for workgroup deletion after {max_attempts * poll_interval} seconds\")\n",
    "    \n",
    "    def wait_for_namespace_deleted(name, poll_interval=10, max_attempts=60):\n",
    "        \"\"\"Wait until namespace is completely deleted\"\"\"\n",
    "        print(f\"  Waiting for namespace {name} to be deleted...\")\n",
    "        attempts = 0\n",
    "        while attempts < max_attempts:\n",
    "            try:\n",
    "                redshift_client.get_namespace(namespaceName=name)\n",
    "                print(f\"    Namespace still exists, waiting...\")\n",
    "                time.sleep(poll_interval)\n",
    "                attempts += 1\n",
    "            except redshift_client.exceptions.ResourceNotFoundException:\n",
    "                print(\"    Namespace deleted successfully\")\n",
    "                return\n",
    "        \n",
    "        if attempts >= max_attempts:\n",
    "            print(f\"    Warning: Timeout waiting for namespace deletion after {max_attempts * poll_interval} seconds\")\n",
    "    \n",
    "    print(\"Starting Redshift environment cleanup...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Delete Redshift workgroup first\n",
    "    print(f\"Step 1: Deleting Redshift workgroup {REDSHIFT_WORKGROUP}\")\n",
    "    try:\n",
    "        redshift_client.delete_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
    "        print(\"  Workgroup deletion initiated\")\n",
    "        wait_for_workgroup_deleted(REDSHIFT_WORKGROUP)\n",
    "    except redshift_client.exceptions.ResourceNotFoundException:\n",
    "        print(\"  Workgroup already deleted or does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error deleting workgroup: {str(e)}\")\n",
    "    \n",
    "    # 2. Delete Redshift namespace\n",
    "    print(f\"\\nStep 2: Deleting Redshift namespace {REDSHIFT_NAMESPACE}\")\n",
    "    try:\n",
    "        redshift_client.delete_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
    "        print(\"  Namespace deletion initiated\")\n",
    "        wait_for_namespace_deleted(REDSHIFT_NAMESPACE)\n",
    "    except redshift_client.exceptions.ResourceNotFoundException:\n",
    "        print(\"  Namespace already deleted or does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error deleting namespace: {str(e)}\")\n",
    "    \n",
    "    # 3. Empty and delete S3 bucket\n",
    "    print(f\"\\nStep 3: Deleting S3 bucket {S3_BUCKET}\")\n",
    "    try:\n",
    "        bucket = s3.Bucket(S3_BUCKET)\n",
    "        \n",
    "        # Check if bucket exists\n",
    "        s3_client.head_bucket(Bucket=S3_BUCKET)\n",
    "        \n",
    "        # Delete all objects in the bucket\n",
    "        print(\"  Emptying bucket contents...\")\n",
    "        objects_to_delete = []\n",
    "        for obj in bucket.objects.all():\n",
    "            objects_to_delete.append({'Key': obj.key})\n",
    "        \n",
    "        if objects_to_delete:\n",
    "            bucket.delete_objects(Delete={'Objects': objects_to_delete})\n",
    "            print(f\"    Deleted {len(objects_to_delete)} objects\")\n",
    "        else:\n",
    "            print(\"    Bucket was already empty\")\n",
    "        \n",
    "        # Delete the bucket\n",
    "        print(\"  Deleting bucket...\")\n",
    "        bucket.delete()\n",
    "        print(\"  S3 bucket deleted successfully\")\n",
    "        \n",
    "    except s3_client.exceptions.NoSuchBucket:\n",
    "        print(\"  S3 bucket already deleted or does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error deleting S3 bucket: {str(e)}\")\n",
    "    \n",
    "    # 4. Delete IAM role and policies\n",
    "    print(f\"\\nStep 4: Deleting IAM role {redshift_role_arn.split('/')[-1]}\")\n",
    "    role_name = redshift_role_arn.split('/')[-1]\n",
    "    try:\n",
    "        # Check if role exists\n",
    "        iam_client.get_role(RoleName=role_name)\n",
    "        \n",
    "        # Detach managed policies\n",
    "        print(\"  Detaching managed policies...\")\n",
    "        attached_policies = iam_client.list_attached_role_policies(RoleName=role_name)['AttachedPolicies']\n",
    "        for policy in attached_policies:\n",
    "            policy_arn = policy['PolicyArn']\n",
    "            iam_client.detach_role_policy(RoleName=role_name, PolicyArn=policy_arn)\n",
    "            print(f\"    Detached policy: {policy['PolicyName']}\")\n",
    "            \n",
    "            # Delete custom policies (not AWS managed)\n",
    "            if not policy_arn.startswith('arn:aws:iam::aws:policy/'):\n",
    "                try:\n",
    "                    iam_client.delete_policy(PolicyArn=policy_arn)\n",
    "                    print(f\"    Deleted custom policy: {policy['PolicyName']}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Could not delete policy {policy['PolicyName']}: {str(e)}\")\n",
    "        \n",
    "        # Delete inline policies\n",
    "        print(\"  Deleting inline policies...\")\n",
    "        inline_policies = iam_client.list_role_policies(RoleName=role_name)['PolicyNames']\n",
    "        for policy_name in inline_policies:\n",
    "            iam_client.delete_role_policy(RoleName=role_name, PolicyName=policy_name)\n",
    "            print(f\"    Deleted inline policy: {policy_name}\")\n",
    "        \n",
    "        # Delete the role\n",
    "        iam_client.delete_role(RoleName=role_name)\n",
    "        print(\"  IAM role deleted successfully\")\n",
    "        \n",
    "    except iam_client.exceptions.NoSuchEntityException:\n",
    "        print(\"  IAM role already deleted or does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error deleting IAM role: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Redshift environment cleanup completed\")\n",
    "    print(\"\\nSummary of deleted resources:\")\n",
    "    print(f\"  - Redshift Workgroup: {REDSHIFT_WORKGROUP}\")\n",
    "    print(f\"  - Redshift Namespace: {REDSHIFT_NAMESPACE}\")\n",
    "    print(f\"  - S3 Bucket: {S3_BUCKET}\")\n",
    "    print(f\"  - IAM Role: {role_name}\")\n",
    "\n",
    "# Usage:\n",
    "cleanup_redshift_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ad527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-on-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
