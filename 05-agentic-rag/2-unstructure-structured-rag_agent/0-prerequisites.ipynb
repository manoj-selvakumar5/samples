{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "725828ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import logging\n",
    "import requests\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a61171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS clients\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "sts_client = boto3.client('sts')\n",
    "redshift_client = boto3.client('redshift-serverless', region_name=region)\n",
    "redshift_data_client = boto3.client('redshift-data', region_name=region)\n",
    "iam_client = boto3.client('iam')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5d40a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using suffix: 0192129\n"
     ]
    }
   ],
   "source": [
    "# Generate unique suffix for resource names\n",
    "current_time = time.time()\n",
    "timestamp_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(current_time))[-7:]\n",
    "suffix = f\"{timestamp_str}\"\n",
    "\n",
    "print(f\"Using suffix: {suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d05e550",
   "metadata": {},
   "source": [
    "## Step 1: Download Bedrock Knowledge Base Utilities\n",
    "\n",
    "Lets download the structured knowledge base utility to help with Knowledge Base configuration and creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ab0fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded structured KB utils to utils/structured_knowledge_base.py\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/rag/knowledge-bases/features-examples/utils/structured_knowledge_base.py\"\n",
    "target_path = \"utils/structured_knowledge_base.py\"\n",
    "os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "response = requests.get(url)\n",
    "with open(target_path, \"w\") as f:\n",
    "    f.write(response.text)\n",
    "print(f\"Downloaded structured KB utils to {target_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f62900bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.structured_knowledge_base import BedrockStructuredKnowledgeBase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2ccb8",
   "metadata": {},
   "source": [
    "## Step 2: Set up Redshift Serverless Infrastructure\n",
    "\n",
    "Next we will create the necessary Redshift Serverless components: namespace and workgroup. This infrastructure will host our structured data that the Knowledge Base will query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7182df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift Namespace: sds-ecommerce-0192129\n",
      "Redshift Workgroup: sds-ecommerce-wg-0192129\n",
      "Database: sds-ecommerce\n",
      "S3 Bucket: sds-ecommerce-redshift-0192129\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Redshift resources\n",
    "REDSHIFT_NAMESPACE = f'sds-ecommerce-{suffix}'\n",
    "REDSHIFT_WORKGROUP = f'sds-ecommerce-wg-{suffix}'\n",
    "REDSHIFT_DATABASE = f'sds-ecommerce'\n",
    "S3_BUCKET = f'sds-ecommerce-redshift-{suffix}'\n",
    "\n",
    "print(f\"Redshift Namespace: {REDSHIFT_NAMESPACE}\")\n",
    "print(f\"Redshift Workgroup: {REDSHIFT_WORKGROUP}\")\n",
    "print(f\"Database: {REDSHIFT_DATABASE}\")\n",
    "print(f\"S3 Bucket: {S3_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "109b83d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created role RedshiftS3AccessRole-0192129\n",
      "Redshift IAM Role ARN: arn:aws:iam::533267284022:role/RedshiftS3AccessRole-0192129\n"
     ]
    }
   ],
   "source": [
    "def create_iam_role_for_redshift():\n",
    "    \"\"\"Create IAM role for Redshift to access S3\"\"\"\n",
    "    try:\n",
    "        # Get account ID\n",
    "        account_id = sts_client.get_caller_identity()['Account']\n",
    "        \n",
    "        # Create IAM role if it doesn't exist\n",
    "        role_name = f'RedshiftS3AccessRole-{suffix}'\n",
    "        try:\n",
    "            role_response = iam_client.get_role(RoleName=role_name)\n",
    "            print(f'Role {role_name} already exists')\n",
    "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
    "        except iam_client.exceptions.NoSuchEntityException:\n",
    "            trust_policy = {\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"redshift.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            iam_client.create_role(\n",
    "                RoleName=role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    "            )\n",
    "            \n",
    "            # Attach necessary policies\n",
    "            iam_client.attach_role_policy(\n",
    "                RoleName=role_name,\n",
    "                PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n",
    "            )\n",
    "            \n",
    "            print(f'Created role {role_name}')\n",
    "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error creating IAM role: {str(e)}')\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "redshift_role_arn = create_iam_role_for_redshift()\n",
    "print(f\"Redshift IAM Role ARN: {redshift_role_arn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e59245e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating namespace sds-ecommerce-0192129...\n",
      "Created namespace sds-ecommerce-0192129\n",
      "Waiting for namespace to be available...\n",
      "Namespace sds-ecommerce-0192129 is now available\n"
     ]
    }
   ],
   "source": [
    "def create_redshift_namespace():\n",
    "    \"\"\"Create Redshift Serverless namespace\"\"\"\n",
    "    try:\n",
    "        # Check if namespace already exists\n",
    "        try:\n",
    "            response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
    "            print(f'Namespace {REDSHIFT_NAMESPACE} already exists')\n",
    "            return response['namespace']\n",
    "        except redshift_client.exceptions.ResourceNotFoundException:\n",
    "            print(f'Creating namespace {REDSHIFT_NAMESPACE}...')\n",
    "        \n",
    "        # Create the namespace\n",
    "        response = redshift_client.create_namespace(\n",
    "            namespaceName=REDSHIFT_NAMESPACE,\n",
    "            adminUsername='admin',\n",
    "            adminUserPassword='TempPassword123!',  # Change this in production\n",
    "            dbName=REDSHIFT_DATABASE,\n",
    "            defaultIamRoleArn=redshift_role_arn,\n",
    "            iamRoles=[redshift_role_arn]\n",
    "        )\n",
    "        \n",
    "        print(f'Created namespace {REDSHIFT_NAMESPACE}')\n",
    "        \n",
    "        # Wait for namespace to be available\n",
    "        print('Waiting for namespace to be available...')\n",
    "        max_attempts = 30\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                namespace_response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
    "                status = namespace_response['namespace']['status']\n",
    "                if status == 'AVAILABLE':\n",
    "                    print(f'Namespace {REDSHIFT_NAMESPACE} is now available')\n",
    "                    return namespace_response['namespace']\n",
    "                else:\n",
    "                    print(f'Namespace status: {status}, waiting...')\n",
    "                    time.sleep(10)\n",
    "            except Exception as e:\n",
    "                print(f'Error checking namespace status: {str(e)}, retrying...')\n",
    "                time.sleep(10)\n",
    "        \n",
    "        print('Timeout waiting for namespace, but proceeding...')\n",
    "        return response['namespace']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error creating namespace: {str(e)}')\n",
    "        raise\n",
    "\n",
    "# Create namespace\n",
    "namespace = create_redshift_namespace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "372e2dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating workgroup sds-ecommerce-wg-0192129...\n",
      "Created workgroup sds-ecommerce-wg-0192129\n",
      "Waiting for workgroup to be available...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup sds-ecommerce-wg-0192129 is now available\n",
      "Workgroup ARN: arn:aws:redshift-serverless:us-west-2:533267284022:workgroup/f4a40c1d-5bba-4443-b7db-068abe39ef9b\n"
     ]
    }
   ],
   "source": [
    "def create_redshift_workgroup():\n",
    "    \"\"\"Create Redshift Serverless workgroup\"\"\"\n",
    "    try:\n",
    "        # Check if workgroup already exists\n",
    "        try:\n",
    "            response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
    "            print(f'Workgroup {REDSHIFT_WORKGROUP} already exists')\n",
    "            return response['workgroup']\n",
    "        except redshift_client.exceptions.ResourceNotFoundException:\n",
    "            print(f'Creating workgroup {REDSHIFT_WORKGROUP}...')\n",
    "        \n",
    "        # Create the workgroup\n",
    "        response = redshift_client.create_workgroup(\n",
    "            workgroupName=REDSHIFT_WORKGROUP,\n",
    "            namespaceName=REDSHIFT_NAMESPACE,\n",
    "            baseCapacity=8,  # Minimum base capacity\n",
    "            enhancedVpcRouting=False,\n",
    "            publiclyAccessible=True,\n",
    "            configParameters=[\n",
    "                {\n",
    "                    'parameterKey': 'enable_user_activity_logging',\n",
    "                    'parameterValue': 'true'\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f'Created workgroup {REDSHIFT_WORKGROUP}')\n",
    "        \n",
    "        # Wait for workgroup to be available\n",
    "        print('Waiting for workgroup to be available...')\n",
    "        max_attempts = 30\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                workgroup_response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
    "                status = workgroup_response['workgroup']['status']\n",
    "                if status == 'AVAILABLE':\n",
    "                    print(f'Workgroup {REDSHIFT_WORKGROUP} is now available')\n",
    "                    return workgroup_response['workgroup']\n",
    "                else:\n",
    "                    print(f'Workgroup status: {status}, waiting...')\n",
    "                    time.sleep(10)\n",
    "            except Exception as e:\n",
    "                print(f'Error checking workgroup status: {str(e)}, retrying...')\n",
    "                time.sleep(10)\n",
    "        \n",
    "        print('Timeout waiting for workgroup, but proceeding...')\n",
    "        return response['workgroup']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error creating workgroup: {str(e)}')\n",
    "        raise\n",
    "\n",
    "# Create workgroup\n",
    "workgroup = create_redshift_workgroup()\n",
    "workgroup_arn = workgroup['workgroupArn']\n",
    "print(f\"Workgroup ARN: {workgroup_arn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a03caa",
   "metadata": {},
   "source": [
    "## Step 3: Create S3 Bucket and Load Sample Data\n",
    "\n",
    "We will create an S3 bucket to stage our sample e-commerce data before loading it into Redshift tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9976c340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket sds-ecommerce-redshift-0192129\n"
     ]
    }
   ],
   "source": [
    "def create_s3_bucket():\n",
    "    \"\"\"Create S3 bucket for data staging\"\"\"\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=S3_BUCKET)\n",
    "        print(f'Bucket {S3_BUCKET} already exists')\n",
    "    except:\n",
    "        try:\n",
    "            if region == 'us-east-1':\n",
    "                s3_client.create_bucket(Bucket=S3_BUCKET)\n",
    "            else:\n",
    "                s3_client.create_bucket(\n",
    "                    Bucket=S3_BUCKET,\n",
    "                    CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                )\n",
    "            print(f'Created bucket {S3_BUCKET}')\n",
    "        except Exception as e:\n",
    "            print(f'Error creating bucket: {str(e)}')\n",
    "            raise\n",
    "\n",
    "# Create S3 bucket\n",
    "create_s3_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1be47dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading sample data files to S3...\n",
      "Uploaded orders.csv (1.8 MB) to S3\n",
      "Uploaded order_items.csv (1.3 MB) to S3\n",
      "Uploaded payments.csv (0.8 MB) to S3\n",
      "Uploaded reviews.csv (0.5 MB) to S3\n",
      "\n",
      "Successfully uploaded all 4 data files to S3\n"
     ]
    }
   ],
   "source": [
    "def upload_sample_data():\n",
    "    \"\"\"Upload sample CSV files to S3\"\"\"\n",
    "    data_files = ['orders.csv', 'order_items.csv', 'payments.csv', 'reviews.csv']\n",
    "    sds_directory = 'sample_data'\n",
    "    \n",
    "    print(\"Uploading sample data files to S3...\")\n",
    "    files_found = 0\n",
    "    \n",
    "    for file_name in data_files:\n",
    "        local_path = os.path.join(sds_directory, file_name)\n",
    "        if os.path.exists(local_path):\n",
    "            # Get file size for informational purposes\n",
    "            file_size = os.path.getsize(local_path)\n",
    "            file_size_mb = file_size / (1024 * 1024)\n",
    "            \n",
    "            s3_client.upload_file(local_path, S3_BUCKET, file_name)\n",
    "            print(f'Uploaded {file_name} ({file_size_mb:.1f} MB) to S3')\n",
    "            files_found += 1\n",
    "        else:\n",
    "            print(f'Warning: {local_path} not found')\n",
    "    \n",
    "    if files_found == len(data_files):\n",
    "        print(f\"\\nSuccessfully uploaded all {files_found} data files to S3\")\n",
    "    else:\n",
    "        print(f\"\\nOnly {files_found} out of {len(data_files)} files were found and uploaded\")\n",
    "\n",
    "# Upload sample data\n",
    "upload_sample_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd94c8",
   "metadata": {},
   "source": [
    "## Step 4: Create Redshift Tables and Load Data\n",
    "\n",
    "Now we will create the database tables in Redshift and load our sample e-commerce data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a4b8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_statement(statement_id):\n",
    "    \"\"\"Wait for a Redshift Data API statement to complete\"\"\"\n",
    "    max_attempts = 30\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            response = redshift_data_client.describe_statement(Id=statement_id)\n",
    "            status = response['Status']\n",
    "            if status == 'FINISHED':\n",
    "                return response\n",
    "            elif status == 'FAILED':\n",
    "                raise Exception(f\"Statement failed: {response.get('Error', 'Unknown error')}\")\n",
    "            elif status == 'CANCELLED':\n",
    "                raise Exception(\"Statement was cancelled\")\n",
    "            else:\n",
    "                print(f\"Statement status: {status}, waiting...\")\n",
    "                time.sleep(5)\n",
    "        except Exception as e:\n",
    "            if 'Statement failed' in str(e) or 'cancelled' in str(e):\n",
    "                raise\n",
    "            print(f\"Error checking statement status: {str(e)}, retrying...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    raise Exception(\"Timeout waiting for statement to complete\")\n",
    "\n",
    "def run_redshift_statement(sql_statement):\n",
    "    \"\"\"Execute a SQL statement in Redshift\"\"\"\n",
    "    try:\n",
    "        response = redshift_data_client.execute_statement(\n",
    "            WorkgroupName=REDSHIFT_WORKGROUP,\n",
    "            Database=REDSHIFT_DATABASE,\n",
    "            Sql=sql_statement\n",
    "        )\n",
    "        statement_id = response['Id']\n",
    "        print(f\"Executing statement: {statement_id}\")\n",
    "        result = wait_for_statement(statement_id)\n",
    "        print(f\"Statement completed successfully\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing statement: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c7d3205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table: orders\n",
      "Executing statement: ac7bdfd2-d9e8-4be4-bb17-314dc3cd0d32\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Created table: orders\n",
      "-------------\n",
      "Creating table: order_items\n",
      "Executing statement: 4d6dc980-3f5c-42a4-b132-738e95d4d3d2\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Created table: order_items\n",
      "-------------\n",
      "Creating table: payments\n",
      "Executing statement: 0cc66ba8-67cb-4484-855c-e2bceae6dc52\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Created table: payments\n",
      "-------------\n",
      "Creating table: reviews\n",
      "Executing statement: 4a6846a0-89d0-4709-82e1-79d10752baa8\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Created table: reviews\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# Create tables in Redshift\n",
    "def create_tables():\n",
    "    \"\"\"Create all necessary tables in Redshift\"\"\"\n",
    "    \n",
    "    # Orders table\n",
    "    orders_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS orders (\n",
    "        order_id VARCHAR(255) PRIMARY KEY,\n",
    "        customer_id VARCHAR(255),\n",
    "        order_total DECIMAL(10,2),\n",
    "        order_status VARCHAR(50),\n",
    "        payment_method VARCHAR(50),\n",
    "        shipping_address TEXT,\n",
    "        created_at TIMESTAMP,\n",
    "        updated_at TIMESTAMP\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Order Items table\n",
    "    order_items_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS order_items (\n",
    "        order_item_id VARCHAR(255) PRIMARY KEY,\n",
    "        order_id VARCHAR(255),\n",
    "        product_id VARCHAR(255),\n",
    "        quantity INTEGER,\n",
    "        price DECIMAL(10,2)\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Payments table\n",
    "    payments_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS payments (\n",
    "        payment_id VARCHAR(255) PRIMARY KEY,\n",
    "        order_id VARCHAR(255),\n",
    "        customer_id VARCHAR(255),\n",
    "        amount DECIMAL(10,2),\n",
    "        payment_method VARCHAR(50),\n",
    "        payment_status VARCHAR(50),\n",
    "        created_at DATE\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reviews table\n",
    "    reviews_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS reviews (\n",
    "        review_id VARCHAR(255) PRIMARY KEY,\n",
    "        product_id VARCHAR(255),\n",
    "        customer_id VARCHAR(255),\n",
    "        rating INTEGER,\n",
    "        created_at DATE\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    tables = {\n",
    "        'orders': orders_sql,\n",
    "        'order_items': order_items_sql,\n",
    "        'payments': payments_sql,\n",
    "        'reviews': reviews_sql\n",
    "    }\n",
    "    \n",
    "    for table_name, sql in tables.items():\n",
    "        print(f\"Creating table: {table_name}\")\n",
    "        run_redshift_statement(sql)\n",
    "        print(f\"Created table: {table_name}\")\n",
    "        print(\"-------------\")\n",
    "\n",
    "# Create tables\n",
    "create_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b248290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into orders from orders.csv\n",
      "Executing statement: 2528181a-ca4b-413a-a4b4-50848cf10b25\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into orders\n",
      "Loading data into order_items from order_items.csv\n",
      "Executing statement: 8b22aa4d-5e64-4c2a-accc-45e31311bf6c\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into order_items\n",
      "Loading data into payments from payments.csv\n",
      "Executing statement: f3442630-aa5d-4897-961a-1ca33870f0d6\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into payments\n",
      "Loading data into reviews from reviews.csv\n",
      "Executing statement: 6277e764-30d5-4974-8939-c5d4f75c10df\n",
      "Statement status: STARTED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into reviews\n"
     ]
    }
   ],
   "source": [
    "# Load data from S3 into Redshift tables\n",
    "def load_data_from_s3():\n",
    "    \"\"\"Load data from S3 CSV files into Redshift tables\"\"\"\n",
    "    \n",
    "    tables_and_files = {\n",
    "        'orders': 'orders.csv',\n",
    "        'order_items': 'order_items.csv',\n",
    "        'payments': 'payments.csv',\n",
    "        'reviews': 'reviews.csv'\n",
    "    }\n",
    "    \n",
    "    for table_name, file_name in tables_and_files.items():\n",
    "        print(f\"Loading data into {table_name} from {file_name}\")\n",
    "        \n",
    "        copy_sql = f\"\"\"\n",
    "        COPY {table_name}\n",
    "        FROM 's3://{S3_BUCKET}/{file_name}'\n",
    "        IAM_ROLE '{redshift_role_arn}'\n",
    "        CSV\n",
    "        IGNOREHEADER 1\n",
    "        DELIMITER ','\n",
    "        REGION '{region}';\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            run_redshift_statement(copy_sql)\n",
    "            print(f\"Loaded data into {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data into {table_name}: {str(e)}\")\n",
    "\n",
    "# Load data from S3\n",
    "load_data_from_s3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e51221",
   "metadata": {},
   "source": [
    "## Step 5: Verify Data Load\n",
    "\n",
    "Let's verify that our data has been loaded correctly by running some sample queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea721a6",
   "metadata": {},
   "source": [
    "## Step 6: Create Bedrock Knowledge Base with Redshift Data Source\n",
    "\n",
    "Now we'll create the Bedrock Knowledge Base configured to use our Redshift data as a structured data source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7891a188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base Name: redshift-structured-kb-0192129\n"
     ]
    }
   ],
   "source": [
    "# Configure Knowledge Base parameters\n",
    "kb_name = f\"redshift-structured-kb-{suffix}\"\n",
    "kb_description = \"Structured Knowledge Base for e-commerce data queries using Redshift\"\n",
    "generation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "print(f\"Knowledge Base Name: {kb_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7f9bc",
   "metadata": {},
   "source": [
    "Amazon Bedrock Knowledge Bases uses a service role to connect knowledge bases to structured data stores, retrieve data from these data stores, and generate SQL queries based on user queries and the structure of the data stores. There are several access patterns based on if you're using Redshift Serverless vs Redshift Provisioned Cluster. In this notebook, let's use `IAM Role + Redshift Serverless WorkGroup` access pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65137e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Knowledge Base parameters for Redshift Serverless with IAM authentication\n",
    "kb_config_param = {\n",
    "    \"type\": \"SQL\",\n",
    "    \"sqlKnowledgeBaseConfiguration\": {\n",
    "        \"type\": \"REDSHIFT\",\n",
    "        \"redshiftConfiguration\": {\n",
    "            \"storageConfigurations\": [{\n",
    "                \"type\": \"REDSHIFT\",\n",
    "                \"redshiftConfiguration\": {\n",
    "                    \"databaseName\": REDSHIFT_DATABASE\n",
    "                }\n",
    "            }],\n",
    "            \"queryEngineConfiguration\": {\n",
    "                \"type\": \"SERVERLESS\",\n",
    "                \"serverlessConfiguration\": {\n",
    "                    \"workgroupArn\": workgroup_arn,\n",
    "                    \"authConfiguration\": {\n",
    "                        \"type\": \"IAM\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "932cac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "Step 1 - Creating Knowledge Base Execution Role (AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0192129) and Policies\n",
      "========================================================================================\n",
      "Step 2 - Creating Knowledge Base\n",
      "{ 'createdAt': datetime.datetime(2025, 6, 21, 2, 23, 3, 383984, tzinfo=tzutc()),\n",
      "  'description': 'Structured Knowledge Base for e-commerce data queries using '\n",
      "                 'Redshift',\n",
      "  'knowledgeBaseArn': 'arn:aws:bedrock:us-west-2:533267284022:knowledge-base/NDFEP5C9NZ',\n",
      "  'knowledgeBaseConfiguration': { 'sqlKnowledgeBaseConfiguration': { 'redshiftConfiguration': { 'queryEngineConfiguration': { 'serverlessConfiguration': { 'authConfiguration': { 'type': 'IAM'},\n",
      "                                                                                                                                                           'workgroupArn': 'arn:aws:redshift-serverless:us-west-2:533267284022:workgroup/f4a40c1d-5bba-4443-b7db-068abe39ef9b'},\n",
      "                                                                                                                              'type': 'SERVERLESS'},\n",
      "                                                                                                'storageConfigurations': [ { 'redshiftConfiguration': { 'databaseName': 'sds-ecommerce'},\n",
      "                                                                                                                             'type': 'REDSHIFT'}]},\n",
      "                                                                     'type': 'REDSHIFT'},\n",
      "                                  'type': 'SQL'},\n",
      "  'knowledgeBaseId': 'NDFEP5C9NZ',\n",
      "  'name': 'redshift-structured-kb-0192129',\n",
      "  'roleArn': 'arn:aws:iam::533267284022:role/AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0192129',\n",
      "  'status': 'CREATING',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 21, 2, 23, 3, 383984, tzinfo=tzutc())}\n",
      "Creating Data Sources aka query engine\n",
      "{ 'createdAt': datetime.datetime(2025, 6, 21, 2, 23, 3, 499580, tzinfo=tzutc()),\n",
      "  'dataSourceConfiguration': {'type': 'REDSHIFT_METADATA'},\n",
      "  'dataSourceId': 'RAYVKWOCOQ',\n",
      "  'description': 'Query engine',\n",
      "  'knowledgeBaseId': 'NDFEP5C9NZ',\n",
      "  'name': 'redshift-structured-kb-0192129-ds',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 21, 2, 23, 3, 499580, tzinfo=tzutc())}\n",
      "========================================================================================\n",
      "Knowledge Base created successfully!\n",
      "'NDFEP5C9NZ'\n",
      "Knowledge Base ID: NDFEP5C9NZ\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    structured_kb = BedrockStructuredKnowledgeBase(\n",
    "        kb_name=kb_name,\n",
    "        kb_description=kb_description,\n",
    "        workgroup_arn=workgroup_arn,\n",
    "        kbConfigParam=kb_config_param,\n",
    "        generation_model=generation_model,\n",
    "        suffix=suffix\n",
    "    )\n",
    "    \n",
    "    print(\"Knowledge Base created successfully!\")\n",
    "    kb_id = structured_kb.get_knowledge_base_id()\n",
    "    print(f\"Knowledge Base ID: {kb_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating Knowledge Base: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be9c6b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion Job Details:\n",
      "==================================================\n",
      "Status: FAILED\n",
      "\n",
      "Full job details:\n",
      "{'dataSourceId': 'RAYVKWOCOQ',\n",
      " 'ingestionJobId': '54IIOZYXUG',\n",
      " 'knowledgeBaseId': 'NDFEP5C9NZ',\n",
      " 'startedAt': datetime.datetime(2025, 6, 21, 2, 23, 41, 328701, tzinfo=tzutc()),\n",
      " 'status': 'FAILED',\n",
      " 'updatedAt': datetime.datetime(2025, 6, 21, 2, 23, 44, 589892, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "# Let's check the ingestion job details to see the specific error\n",
    "def get_ingestion_job_details(kb_id, data_source_id, job_id):\n",
    "    \"\"\"Get detailed information about the ingestion job including error messages\"\"\"\n",
    "    try:\n",
    "        response = bedrock_agent_client.get_ingestion_job(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            dataSourceId=data_source_id,\n",
    "            ingestionJobId=job_id\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting ingestion job details: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Get the job details\n",
    "job_details = get_ingestion_job_details(kb_id, 'RAYVKWOCOQ', '54IIOZYXUG')\n",
    "if job_details:\n",
    "    print(\"Ingestion Job Details:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Status: {job_details['ingestionJob']['status']}\")\n",
    "    if 'failureReasons' in job_details['ingestionJob']:\n",
    "        print(f\"Failure Reasons: {job_details['ingestionJob']['failureReasons']}\")\n",
    "    if 'statistics' in job_details['ingestionJob']:\n",
    "        print(f\"Statistics: {job_details['ingestionJob']['statistics']}\")\n",
    "    print(\"\\nFull job details:\")\n",
    "    import pprint\n",
    "    pprint.pprint(job_details['ingestionJob'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "515d4855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granting additional system catalog permissions...\n",
      "Granting permission 1/6: information_schema.tables\n",
      "Executing statement: fca4913a-fc7d-44f7-b18a-575dd94ba13c\n",
      "Statement status: SUBMITTED, waiting...\n",
      "Statement completed successfully\n",
      "✅ Permission 1 granted successfully\n",
      "Granting permission 2/6: information_schema.columns\n",
      "Executing statement: 418ef2d1-7aea-4c13-958a-b7e6272d72e4\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "✅ Permission 2 granted successfully\n",
      "Granting permission 3/6: pg_catalog.pg_tables\n",
      "Executing statement: d0bb719a-794f-48ca-ad48-1e56c93ff33b\n",
      "Statement status: SUBMITTED, waiting...\n",
      "Statement completed successfully\n",
      "✅ Permission 3 granted successfully\n",
      "Granting permission 4/6: pg_catalog.pg_class\n",
      "Executing statement: 706a9b37-5b29-46cb-a2ae-e4ed73276a74\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "✅ Permission 4 granted successfully\n",
      "Granting permission 5/6: pg_catalog.pg_attribute\n",
      "Executing statement: 640abdea-49d1-42b0-b01e-3177ce282f2f\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "✅ Permission 5 granted successfully\n",
      "Granting permission 6/6: pg_catalog.pg_namespace\n",
      "Executing statement: bbb71ba1-54be-44c9-8e24-6bc7f20656f2\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "✅ Permission 6 granted successfully\n",
      "\n",
      "All additional permissions processed!\n"
     ]
    }
   ],
   "source": [
    "# Grant additional permissions that might be needed for Bedrock Knowledge Base\n",
    "additional_permissions = [\n",
    "    f'GRANT SELECT ON information_schema.tables TO \"IAMR:{bedrock_role_name}\";',\n",
    "    f'GRANT SELECT ON information_schema.columns TO \"IAMR:{bedrock_role_name}\";',\n",
    "    f'GRANT SELECT ON pg_catalog.pg_tables TO \"IAMR:{bedrock_role_name}\";',\n",
    "    f'GRANT SELECT ON pg_catalog.pg_class TO \"IAMR:{bedrock_role_name}\";',\n",
    "    f'GRANT SELECT ON pg_catalog.pg_attribute TO \"IAMR:{bedrock_role_name}\";',\n",
    "    f'GRANT SELECT ON pg_catalog.pg_namespace TO \"IAMR:{bedrock_role_name}\";'\n",
    "]\n",
    "\n",
    "print(\"Granting additional system catalog permissions...\")\n",
    "for i, permission_sql in enumerate(additional_permissions, 1):\n",
    "    try:\n",
    "        print(f\"Granting permission {i}/{len(additional_permissions)}: {permission_sql.split('ON')[1].split('TO')[0].strip()}\")\n",
    "        run_redshift_statement(permission_sql)\n",
    "        print(f\"✅ Permission {i} granted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Permission {i} failed (this might be expected): {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nAll additional permissions processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfb492b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying ingestion job with additional permissions...\n",
      "job  started successfully\n",
      "\n",
      "{ 'dataSourceId': 'RAYVKWOCOQ',\n",
      "  'ingestionJobId': '96AFW4F8KE',\n",
      "  'knowledgeBaseId': 'NDFEP5C9NZ',\n",
      "  'startedAt': datetime.datetime(2025, 6, 21, 2, 26, 4, 251941, tzinfo=tzutc()),\n",
      "  'status': 'FAILED',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 21, 2, 26, 7, 684605, tzinfo=tzutc())}\n",
      "✅ Ingestion job started successfully!\n"
     ]
    }
   ],
   "source": [
    "# Now let's retry the ingestion job with the additional permissions\n",
    "print(\"Retrying ingestion job with additional permissions...\")\n",
    "time.sleep(10)  # Wait a bit for permissions to propagate\n",
    "\n",
    "try:\n",
    "    structured_kb.start_ingestion_job()\n",
    "    print(\"✅ Ingestion job started successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error starting ingestion job: {str(e)}\")\n",
    "    \n",
    "    # If it still fails, let's try to get more details about the Knowledge Base status\n",
    "    try:\n",
    "        kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=kb_id)\n",
    "        print(f\"\\nKnowledge Base Status: {kb_response['knowledgeBase']['status']}\")\n",
    "        if kb_response['knowledgeBase']['status'] == 'FAILED':\n",
    "            print(\"Knowledge Base itself is in FAILED state\")\n",
    "    except Exception as kb_error:\n",
    "        print(f\"Error getting KB status: {str(kb_error)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22c74e",
   "metadata": {},
   "source": [
    "## Step 6: Database Access Configuration for IAM Role + Redshift Serverless WorkGroup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1aa7ec5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Ingestion Job Details:\n",
      "==================================================\n",
      "Status: FAILED\n",
      "\n",
      "Full latest job details:\n",
      "{'dataSourceId': 'RAYVKWOCOQ',\n",
      " 'ingestionJobId': '96AFW4F8KE',\n",
      " 'knowledgeBaseId': 'NDFEP5C9NZ',\n",
      " 'startedAt': datetime.datetime(2025, 6, 21, 2, 26, 4, 251941, tzinfo=tzutc()),\n",
      " 'status': 'FAILED',\n",
      " 'updatedAt': datetime.datetime(2025, 6, 21, 2, 26, 7, 684605, tzinfo=tzutc())}\n",
      "\n",
      "📊 Knowledge Base Status: ACTIVE\n",
      "\n",
      "🔍 Testing database connectivity...\n",
      "Testing query as Bedrock user: SELECT current_user, session_user;\n",
      "Statement status: STARTED, waiting...\n",
      "✅ Database connection test successful\n",
      "Query results:\n",
      "{'ColumnMetadata': [{'isCaseSensitive': True,\n",
      "                     'isCurrency': False,\n",
      "                     'isSigned': False,\n",
      "                     'label': 'current_user',\n",
      "                     'length': 0,\n",
      "                     'name': 'current_user',\n",
      "                     'nullable': 1,\n",
      "                     'precision': 127,\n",
      "                     'scale': 0,\n",
      "                     'schemaName': '',\n",
      "                     'tableName': '',\n",
      "                     'typeName': 'bpchar'},\n",
      "                    {'isCaseSensitive': True,\n",
      "                     'isCurrency': False,\n",
      "                     'isSigned': False,\n",
      "                     'label': 'session_user',\n",
      "                     'length': 0,\n",
      "                     'name': 'session_user',\n",
      "                     'nullable': 1,\n",
      "                     'precision': 64,\n",
      "                     'scale': 0,\n",
      "                     'schemaName': '',\n",
      "                     'tableName': '',\n",
      "                     'typeName': 'name'}],\n",
      " 'Records': [[{'stringValue': 'IAM:manoj-developer-admin'},\n",
      "              {'stringValue': 'IAM:manoj-developer-admin'}]],\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '550',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Sat, 21 Jun 2025 02:35:40 GMT',\n",
      "                                      'x-amzn-requestid': '5718099f-3ef2-4fe9-a743-02139569326e'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '5718099f-3ef2-4fe9-a743-02139569326e',\n",
      "                      'RetryAttempts': 0},\n",
      " 'TotalNumRows': 1}\n",
      "\n",
      "🔍 Testing table access...\n",
      "Testing table access: SELECT COUNT(*) FROM orders;\n",
      "Statement status: PICKED, waiting...\n",
      "✅ Table access test successful\n"
     ]
    }
   ],
   "source": [
    "# Check the latest ingestion job details\n",
    "latest_job_details = get_ingestion_job_details(kb_id, 'RAYVKWOCOQ', '96AFW4F8KE')\n",
    "if latest_job_details:\n",
    "    print(\"Latest Ingestion Job Details:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Status: {latest_job_details['ingestionJob']['status']}\")\n",
    "    if 'failureReasons' in latest_job_details['ingestionJob']:\n",
    "        print(f\"Failure Reasons: {latest_job_details['ingestionJob']['failureReasons']}\")\n",
    "    if 'statistics' in latest_job_details['ingestionJob']:\n",
    "        print(f\"Statistics: {latest_job_details['ingestionJob']['statistics']}\")\n",
    "    print(\"\\nFull latest job details:\")\n",
    "    import pprint\n",
    "    pprint.pprint(latest_job_details['ingestionJob'])\n",
    "\n",
    "# Let's also check if there are any other issues with the Knowledge Base itself\n",
    "try:\n",
    "    kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=kb_id)\n",
    "    print(f\"\\n📊 Knowledge Base Status: {kb_response['knowledgeBase']['status']}\")\n",
    "    if kb_response['knowledgeBase']['status'] != 'ACTIVE':\n",
    "        print(\"⚠️ Knowledge Base is not in ACTIVE state\")\n",
    "        print(\"Full KB details:\")\n",
    "        pprint.pprint(kb_response['knowledgeBase'])\n",
    "except Exception as kb_error:\n",
    "    print(f\"❌ Error getting KB status: {str(kb_error)}\")\n",
    "\n",
    "# Let's also verify our database permissions by testing a simple query\n",
    "print(\"\\n🔍 Testing database connectivity...\")\n",
    "try:\n",
    "    test_query = f'SELECT current_user, session_user;'\n",
    "    print(f\"Testing query as Bedrock user: {test_query}\")\n",
    "    \n",
    "    # This will fail if our permissions aren't set up correctly\n",
    "    response = redshift_data_client.execute_statement(\n",
    "        WorkgroupName=REDSHIFT_WORKGROUP,\n",
    "        Database=REDSHIFT_DATABASE,\n",
    "        Sql=test_query\n",
    "    )\n",
    "    \n",
    "    statement_id = response['Id']\n",
    "    result = wait_for_statement(statement_id)\n",
    "    \n",
    "    # Get the results\n",
    "    results = redshift_data_client.get_statement_result(Id=statement_id)\n",
    "    print(\"✅ Database connection test successful\")\n",
    "    print(\"Query results:\")\n",
    "    pprint.pprint(results)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Database connection test failed: {str(e)}\")\n",
    "    print(\"This might indicate permission issues\")\n",
    "\n",
    "# Check if tables are accessible\n",
    "print(\"\\n🔍 Testing table access...\")\n",
    "try:\n",
    "    test_query = f'SELECT COUNT(*) FROM orders;'\n",
    "    print(f\"Testing table access: {test_query}\")\n",
    "    \n",
    "    response = redshift_data_client.execute_statement(\n",
    "        WorkgroupName=REDSHIFT_WORKGROUP,\n",
    "        Database=REDSHIFT_DATABASE,\n",
    "        Sql=test_query\n",
    "    )\n",
    "    \n",
    "    statement_id = response['Id']\n",
    "    result = wait_for_statement(statement_id)\n",
    "    \n",
    "    results = redshift_data_client.get_statement_result(Id=statement_id)\n",
    "    print(\"✅ Table access test successful\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Table access test failed: {str(e)}\")\n",
    "    print(\"This indicates the Bedrock role cannot access the tables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8723e2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking Bedrock IAM user in database...\n",
      "Checking IAM users in database: SELECT usename FROM pg_user WHERE usename LIKE 'IAMR:%';\n",
      "Statement status: SUBMITTED, waiting...\n",
      "✅ IAM users in database:\n",
      "  - IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0192129\n",
      "✅ Bedrock user IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0192129 exists in database\n",
      "\n",
      "🔍 Checking permissions for IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0192129...\n",
      "Statement status: SUBMITTED, waiting...\n",
      "❌ Error checking schema permissions: Statement failed: ERROR: column \"schemaname\" does not exist in pg_user, pg_namespace\n",
      "Statement status: PICKED, waiting...\n",
      "\n",
      "🔍 Table permissions for IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0192129:\n",
      "  Table 'orders': SELECT permission: True\n",
      "  Table 'order_items': SELECT permission: True\n",
      "  Table 'payments': SELECT permission: True\n",
      "  Table 'reviews': SELECT permission: True\n",
      "\n",
      "============================================================\n",
      "🔍 DIAGNOSIS SUMMARY:\n",
      "============================================================\n",
      "Based on the tests above, the ingestion failure is likely due to:\n",
      "1. The Bedrock IAM user not existing in the Redshift database, OR\n",
      "2. The Bedrock IAM user not having proper permissions\n",
      "3. The Knowledge Base is trying to use the Bedrock role but can't authenticate\n",
      "\n",
      "The fact that our personal IAM user works but the ingestion fails\n",
      "confirms this is a Bedrock-specific authentication/permission issue.\n"
     ]
    }
   ],
   "source": [
    "# The issue is that we need to test with the actual Bedrock role, not our personal IAM user\n",
    "# Let's check if the Bedrock IAM user exists and has proper permissions\n",
    "\n",
    "print(\"🔍 Checking Bedrock IAM user in database...\")\n",
    "\n",
    "# First, let's see what users exist in the database\n",
    "try:\n",
    "    list_users_query = \"SELECT usename FROM pg_user WHERE usename LIKE 'IAMR:%';\"\n",
    "    print(f\"Checking IAM users in database: {list_users_query}\")\n",
    "    \n",
    "    response = redshift_data_client.execute_statement(\n",
    "        WorkgroupName=REDSHIFT_WORKGROUP,\n",
    "        Database=REDSHIFT_DATABASE,\n",
    "        Sql=list_users_query\n",
    "    )\n",
    "    \n",
    "    statement_id = response['Id']\n",
    "    result = wait_for_statement(statement_id)\n",
    "    \n",
    "    results = redshift_data_client.get_statement_result(Id=statement_id)\n",
    "    print(\"✅ IAM users in database:\")\n",
    "    for record in results.get('Records', []):\n",
    "        username = record[0]['stringValue']\n",
    "        print(f\"  - {username}\")\n",
    "    \n",
    "    # Check if our specific Bedrock user exists\n",
    "    bedrock_user_exists = any(\n",
    "        record[0]['stringValue'] == f\"IAMR:{bedrock_role_name}\" \n",
    "        for record in results.get('Records', [])\n",
    "    )\n",
    "    \n",
    "    if bedrock_user_exists:\n",
    "        print(f\"✅ Bedrock user IAMR:{bedrock_role_name} exists in database\")\n",
    "    else:\n",
    "        print(f\"❌ Bedrock user IAMR:{bedrock_role_name} NOT found in database\")\n",
    "        print(\"This is likely why the ingestion is failing!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error checking IAM users: {str(e)}\")\n",
    "\n",
    "# Let's also check what permissions the Bedrock user has\n",
    "try:\n",
    "    print(f\"\\n🔍 Checking permissions for IAMR:{bedrock_role_name}...\")\n",
    "    \n",
    "    # Check schema permissions\n",
    "    schema_perms_query = f\"\"\"\n",
    "    SELECT \n",
    "        schemaname,\n",
    "        usename,\n",
    "        has_schema_privilege(usename, schemaname, 'USAGE') as has_usage\n",
    "    FROM pg_user, pg_namespace \n",
    "    WHERE nspname = 'public' \n",
    "    AND usename = 'IAMR:{bedrock_role_name}';\n",
    "    \"\"\"\n",
    "    \n",
    "    response = redshift_data_client.execute_statement(\n",
    "        WorkgroupName=REDSHIFT_WORKGROUP,\n",
    "        Database=REDSHIFT_DATABASE,\n",
    "        Sql=schema_perms_query\n",
    "    )\n",
    "    \n",
    "    statement_id = response['Id']\n",
    "    result = wait_for_statement(statement_id)\n",
    "    \n",
    "    results = redshift_data_client.get_statement_result(Id=statement_id)\n",
    "    if results.get('Records'):\n",
    "        for record in results['Records']:\n",
    "            schema = record[0]['stringValue']\n",
    "            user = record[1]['stringValue'] \n",
    "            has_usage = record[2]['booleanValue']\n",
    "            print(f\"  Schema '{schema}': User '{user}' has USAGE permission: {has_usage}\")\n",
    "    else:\n",
    "        print(\"  No schema permissions found - this could be the issue!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error checking schema permissions: {str(e)}\")\n",
    "\n",
    "# Check table permissions\n",
    "try:\n",
    "    table_perms_query = f\"\"\"\n",
    "    SELECT \n",
    "        tablename,\n",
    "        has_table_privilege('IAMR:{bedrock_role_name}', 'public.' || tablename, 'SELECT') as has_select\n",
    "    FROM pg_tables \n",
    "    WHERE schemaname = 'public'\n",
    "    AND tablename IN ('orders', 'order_items', 'payments', 'reviews');\n",
    "    \"\"\"\n",
    "    \n",
    "    response = redshift_data_client.execute_statement(\n",
    "        WorkgroupName=REDSHIFT_WORKGROUP,\n",
    "        Database=REDSHIFT_DATABASE,\n",
    "        Sql=table_perms_query\n",
    "    )\n",
    "    \n",
    "    statement_id = response['Id']\n",
    "    result = wait_for_statement(statement_id)\n",
    "    \n",
    "    results = redshift_data_client.get_statement_result(Id=statement_id)\n",
    "    print(f\"\\n🔍 Table permissions for IAMR:{bedrock_role_name}:\")\n",
    "    for record in results.get('Records', []):\n",
    "        table = record[0]['stringValue']\n",
    "        has_select = record[1]['booleanValue'] if len(record) > 1 else False\n",
    "        print(f\"  Table '{table}': SELECT permission: {has_select}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error checking table permissions: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔍 DIAGNOSIS SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Based on the tests above, the ingestion failure is likely due to:\")\n",
    "print(\"1. The Bedrock IAM user not existing in the Redshift database, OR\")\n",
    "print(\"2. The Bedrock IAM user not having proper permissions\")\n",
    "print(\"3. The Knowledge Base is trying to use the Bedrock role but can't authenticate\")\n",
    "print(\"\\nThe fact that our personal IAM user works but the ingestion fails\")\n",
    "print(\"confirms this is a Bedrock-specific authentication/permission issue.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "702ad3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Granting specific system catalog permissions for Bedrock Knowledge Base...\n",
      "Granting 7 critical system permissions...\n",
      "  1/7: pg_catalog.pg_class\n",
      "Executing statement: 54653f59-2be8-4712-a9f3-3596091b81eb\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "    ✅ Success\n",
      "  2/7: pg_catalog.pg_attribute\n",
      "Executing statement: 0aa063b3-addb-4bbf-842d-678d0714d491\n",
      "Statement status: SUBMITTED, waiting...\n",
      "Statement completed successfully\n",
      "    ✅ Success\n",
      "  3/7: pg_catalog.pg_namespace\n",
      "Executing statement: 62cc8875-1fdc-47cc-8a0e-36abccc00174\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "    ✅ Success\n",
      "  4/7: pg_catalog.pg_type\n",
      "Executing statement: 270bbe6a-bea9-425b-9866-663323e7eefc\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "    ✅ Success\n",
      "  5/7: information_schema.tables\n",
      "Executing statement: c0a253ac-5c6e-4eb3-9ffd-07b23d601588\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "    ✅ Success\n",
      "  6/7: information_schema.columns\n",
      "Executing statement: a6ee54d5-937c-4820-aa36-44be85959745\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "    ✅ Success\n",
      "  7/7: information_schema.schemata\n",
      "Executing statement: 31b07e57-b21d-4d4d-a989-3e53cdde41b2\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "    ✅ Success\n",
      "✅ System catalog permissions granted!\n",
      "\n",
      "🔗 Granting CONNECT privilege on database...\n",
      "Executing statement: 52b850de-ad28-4ac0-aaae-1738c2d51c64\n",
      "Statement status: PICKED, waiting...\n",
      "Error executing statement: Statement failed: ERROR: syntax error at or near \"CONNECT\" in context \"GRANT CONNECT\", at line 1\n",
      "  Position: 7\n",
      "⚠️ CONNECT privilege failed (might already exist): Statement failed: ERROR: syntax error at or near \"CONNECT\" in context \"GRANT CONNECT\", at line 1\n",
      "  Position: 7\n",
      "\n",
      "🎯 All critical permissions have been applied!\n",
      "The Knowledge Base should now be able to introspect the database schema properly.\n"
     ]
    }
   ],
   "source": [
    "# Grant specific system catalog permissions that Bedrock Knowledge Base requires\n",
    "print(\"🔧 Granting specific system catalog permissions for Bedrock Knowledge Base...\")\n",
    "\n",
    "# These are the critical permissions that Bedrock needs to introspect the database schema\n",
    "critical_permissions = [\n",
    "    f'GRANT SELECT ON pg_catalog.pg_class TO \"IAMR:{bedrock_role_name}\";',\n",
    "    f'GRANT SELECT ON pg_catalog.pg_attribute TO \"IAMR:{bedrock_role_name}\";', \n",
    "    f'GRANT SELECT ON pg_catalog.pg_namespace TO \"IAMR:{bedrock_role_name}\";',\n",
    "    f'GRANT SELECT ON pg_catalog.pg_type TO \"IAMR:{bedrock_role_name}\";',\n",
    "    f'GRANT SELECT ON information_schema.tables TO \"IAMR:{bedrock_role_name}\";',\n",
    "    f'GRANT SELECT ON information_schema.columns TO \"IAMR:{bedrock_role_name}\";',\n",
    "    f'GRANT SELECT ON information_schema.schemata TO \"IAMR:{bedrock_role_name}\";'\n",
    "]\n",
    "\n",
    "print(f\"Granting {len(critical_permissions)} critical system permissions...\")\n",
    "for i, permission_sql in enumerate(critical_permissions, 1):\n",
    "    try:\n",
    "        table_name = permission_sql.split('ON')[1].split('TO')[0].strip()\n",
    "        print(f\"  {i}/{len(critical_permissions)}: {table_name}\")\n",
    "        run_redshift_statement(permission_sql)\n",
    "        print(f\"    ✅ Success\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ⚠️ Failed (might be expected): {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"✅ System catalog permissions granted!\")\n",
    "\n",
    "# Also ensure the user has CONNECT privilege on the database\n",
    "try:\n",
    "    print(f\"\\n🔗 Granting CONNECT privilege on database...\")\n",
    "    connect_sql = f'GRANT CONNECT ON DATABASE \"{REDSHIFT_DATABASE}\" TO \"IAMR:{bedrock_role_name}\";'\n",
    "    run_redshift_statement(connect_sql)\n",
    "    print(\"✅ CONNECT privilege granted!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ CONNECT privilege failed (might already exist): {str(e)}\")\n",
    "\n",
    "print(\"\\n🎯 All critical permissions have been applied!\")\n",
    "print(\"The Knowledge Base should now be able to introspect the database schema properly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5601a4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Retrying Knowledge Base ingestion with complete permissions...\n",
      "Waiting 15 seconds for permissions to propagate...\n",
      "job  started successfully\n",
      "\n",
      "{ 'dataSourceId': 'RAYVKWOCOQ',\n",
      "  'ingestionJobId': 'Q2V8GATT7S',\n",
      "  'knowledgeBaseId': 'NDFEP5C9NZ',\n",
      "  'startedAt': datetime.datetime(2025, 6, 21, 2, 40, 38, 190353, tzinfo=tzutc()),\n",
      "  'status': 'FAILED',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 21, 2, 40, 41, 344483, tzinfo=tzutc())}\n",
      "✅ Ingestion job started successfully!\n"
     ]
    }
   ],
   "source": [
    "# Now retry the ingestion with the comprehensive permissions\n",
    "print(\"🚀 Retrying Knowledge Base ingestion with complete permissions...\")\n",
    "print(\"Waiting 15 seconds for permissions to propagate...\")\n",
    "time.sleep(15)\n",
    "\n",
    "try:\n",
    "    # Start the ingestion job\n",
    "    ingestion_response = structured_kb.start_ingestion_job()\n",
    "    print(\"✅ Ingestion job started successfully!\")\n",
    "    \n",
    "    # Wait and monitor the ingestion job\n",
    "    if ingestion_response and 'ingestionJobId' in ingestion_response:\n",
    "        job_id = ingestion_response['ingestionJobId']\n",
    "        data_source_id = ingestion_response['dataSourceId']\n",
    "        \n",
    "        print(f\"📊 Monitoring ingestion job: {job_id}\")\n",
    "        print(\"This may take a few minutes...\")\n",
    "        \n",
    "        # Monitor the job status\n",
    "        max_wait_time = 600  # 10 minutes\n",
    "        check_interval = 30  # 30 seconds\n",
    "        elapsed_time = 0\n",
    "        \n",
    "        while elapsed_time < max_wait_time:\n",
    "            try:\n",
    "                job_details = get_ingestion_job_details(kb_id, data_source_id, job_id)\n",
    "                if job_details:\n",
    "                    status = job_details['ingestionJob']['status']\n",
    "                    print(f\"⏱️ Status: {status} (elapsed: {elapsed_time}s)\")\n",
    "                    \n",
    "                    if status == 'COMPLETE':\n",
    "                        print(\"🎉 Ingestion completed successfully!\")\n",
    "                        if 'statistics' in job_details['ingestionJob']:\n",
    "                            stats = job_details['ingestionJob']['statistics']\n",
    "                            print(f\"📈 Statistics: {stats}\")\n",
    "                        break\n",
    "                    elif status == 'FAILED':\n",
    "                        print(\"❌ Ingestion failed!\")\n",
    "                        if 'failureReasons' in job_details['ingestionJob']:\n",
    "                            print(f\"💥 Failure reasons: {job_details['ingestionJob']['failureReasons']}\")\n",
    "                        break\n",
    "                    elif status in ['IN_PROGRESS', 'STARTED']:\n",
    "                        print(f\"⏳ Ingestion in progress...\")\n",
    "                        time.sleep(check_interval)\n",
    "                        elapsed_time += check_interval\n",
    "                    else:\n",
    "                        print(f\"🤔 Unknown status: {status}\")\n",
    "                        time.sleep(check_interval) \n",
    "                        elapsed_time += check_interval\n",
    "                else:\n",
    "                    print(\"⚠️ Could not get job details\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error checking job status: {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        if elapsed_time >= max_wait_time:\n",
    "            print(\"⏰ Timeout waiting for ingestion to complete\")\n",
    "            print(\"You can check the status later in the AWS Console\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error starting ingestion job: {str(e)}\")\n",
    "    print(\"Please check the AWS Console for more details\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5c2a2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Fixing IAM role policy for Bedrock Knowledge Base...\n",
      "Current attached policies for AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0192129:\n",
      "  - AmazonBedrockRedshiftPolicyForKnowledgeBase_0192129: arn:aws:iam::533267284022:policy/AmazonBedrockRedshiftPolicyForKnowledgeBase_0192129\n",
      "✅ Created new policy: BedrockRedshiftAccessPolicy-0192129\n",
      "✅ Attached policy to role: AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0192129\n",
      "🎯 IAM role policy has been updated with comprehensive Redshift permissions!\n"
     ]
    }
   ],
   "source": [
    "# Fix the IAM role policy - the key missing piece!\n",
    "print(\"🔧 Fixing IAM role policy for Bedrock Knowledge Base...\")\n",
    "\n",
    "# Get the current IAM role policy and update it with the missing permissions\n",
    "try:\n",
    "    # First, let's check the current policies attached to the role\n",
    "    response = iam_client.list_attached_role_policies(RoleName=bedrock_role_name)\n",
    "    print(f\"Current attached policies for {bedrock_role_name}:\")\n",
    "    for policy in response['AttachedPolicies']:\n",
    "        print(f\"  - {policy['PolicyName']}: {policy['PolicyArn']}\")\n",
    "    \n",
    "    # The issue is likely that the Bedrock role doesn't have the right permissions\n",
    "    # Let's create a comprehensive policy for Redshift access\n",
    "    redshift_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"RedshiftDataAPIStatementPermissions\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"redshift-data:GetStatementResult\",\n",
    "                    \"redshift-data:DescribeStatement\",\n",
    "                    \"redshift-data:CancelStatement\"\n",
    "                ],\n",
    "                \"Resource\": \"*\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"redshift-data:statement-owner-iam-userid\": \"${aws:userid}\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"Sid\": \"RedshiftDataAPIExecutePermissions\", \n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"redshift-data:ExecuteStatement\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    workgroup_arn\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"Sid\": \"RedshiftServerlessGetCredentials\",\n",
    "                \"Effect\": \"Allow\", \n",
    "                \"Action\": \"redshift-serverless:GetCredentials\",\n",
    "                \"Resource\": [\n",
    "                    workgroup_arn\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"Sid\": \"SqlWorkbenchAccess\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"sqlworkbench:GetSqlRecommendations\",\n",
    "                    \"sqlworkbench:PutSqlGenerationContext\", \n",
    "                    \"sqlworkbench:GetSqlGenerationContext\",\n",
    "                    \"sqlworkbench:DeleteSqlGenerationContext\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            },\n",
    "            {\n",
    "                \"Sid\": \"KbAccess\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:GenerateQuery\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create a new policy with the correct permissions\n",
    "    policy_name = f'BedrockRedshiftAccessPolicy-{suffix}'\n",
    "    \n",
    "    try:\n",
    "        # Try to create the policy\n",
    "        policy_response = iam_client.create_policy(\n",
    "            PolicyName=policy_name,\n",
    "            PolicyDocument=json.dumps(redshift_policy_document),\n",
    "            Description='Comprehensive Redshift access policy for Bedrock Knowledge Base'\n",
    "        )\n",
    "        policy_arn = policy_response['Policy']['Arn']\n",
    "        print(f\"✅ Created new policy: {policy_name}\")\n",
    "        \n",
    "        # Attach the new policy to the role\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=bedrock_role_name,\n",
    "            PolicyArn=policy_arn\n",
    "        )\n",
    "        print(f\"✅ Attached policy to role: {bedrock_role_name}\")\n",
    "        \n",
    "    except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "        # Policy already exists, get its ARN and attach it\n",
    "        account_id = sts_client.get_caller_identity()['Account']\n",
    "        policy_arn = f\"arn:aws:iam::{account_id}:policy/{policy_name}\"\n",
    "        \n",
    "        try:\n",
    "            iam_client.attach_role_policy(\n",
    "                RoleName=bedrock_role_name,\n",
    "                PolicyArn=policy_arn\n",
    "            )\n",
    "            print(f\"✅ Attached existing policy to role: {bedrock_role_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Policy might already be attached: {str(e)}\")\n",
    "    \n",
    "    print(\"🎯 IAM role policy has been updated with comprehensive Redshift permissions!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error updating IAM role policy: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "118c9f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 FINAL ATTEMPT: Starting Knowledge Base ingestion with all fixes applied...\n",
      "Waiting 30 seconds for IAM policy changes to propagate...\n",
      "📊 Starting ingestion job...\n",
      "job  started successfully\n",
      "\n",
      "{ 'dataSourceId': 'RAYVKWOCOQ',\n",
      "  'ingestionJobId': 'YVRM98QKTC',\n",
      "  'knowledgeBaseId': 'NDFEP5C9NZ',\n",
      "  'startedAt': datetime.datetime(2025, 6, 21, 2, 44, 37, 547815, tzinfo=tzutc()),\n",
      "  'status': 'FAILED',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 21, 2, 44, 40, 697034, tzinfo=tzutc())}\n",
      "❌ Failed to get ingestion job details\n"
     ]
    }
   ],
   "source": [
    "# Final retry with comprehensive monitoring and proper IAM permissions\n",
    "print(\"🚀 FINAL ATTEMPT: Starting Knowledge Base ingestion with all fixes applied...\")\n",
    "print(\"Waiting 30 seconds for IAM policy changes to propagate...\")\n",
    "time.sleep(30)\n",
    "\n",
    "try:\n",
    "    # Start the ingestion job\n",
    "    print(\"📊 Starting ingestion job...\")\n",
    "    ingestion_response = structured_kb.start_ingestion_job()\n",
    "    \n",
    "    if ingestion_response and 'ingestionJobId' in ingestion_response:\n",
    "        job_id = ingestion_response['ingestionJobId']\n",
    "        data_source_id = ingestion_response['dataSourceId']\n",
    "        \n",
    "        print(f\"✅ Ingestion job started successfully!\")\n",
    "        print(f\"📊 Job ID: {job_id}\")\n",
    "        print(f\"📊 Data Source ID: {data_source_id}\")\n",
    "        print(\"🔄 Monitoring job progress...\")\n",
    "        \n",
    "        # Enhanced monitoring with detailed status reporting\n",
    "        max_wait_time = 900  # 15 minutes\n",
    "        check_interval = 15  # 15 seconds\n",
    "        elapsed_time = 0\n",
    "        last_status = None\n",
    "        \n",
    "        while elapsed_time < max_wait_time:\n",
    "            try:\n",
    "                job_details = get_ingestion_job_details(kb_id, data_source_id, job_id)\n",
    "                if job_details:\n",
    "                    status = job_details['ingestionJob']['status']\n",
    "                    \n",
    "                    # Only print status changes to reduce noise\n",
    "                    if status != last_status:\n",
    "                        print(f\"⏱️ Status changed: {status} (elapsed: {elapsed_time}s)\")\n",
    "                        last_status = status\n",
    "                        \n",
    "                        # Print additional details for certain statuses\n",
    "                        if status == 'IN_PROGRESS':\n",
    "                            print(\"   📈 Processing database metadata...\")\n",
    "                        elif status == 'STARTED':\n",
    "                            print(\"   🔄 Initializing ingestion...\")\n",
    "                    \n",
    "                    if status == 'COMPLETE':\n",
    "                        print(\"🎉 INGESTION COMPLETED SUCCESSFULLY!\")\n",
    "                        if 'statistics' in job_details['ingestionJob']:\n",
    "                            stats = job_details['ingestionJob']['statistics']\n",
    "                            print(f\"📈 Final Statistics:\")\n",
    "                            for key, value in stats.items():\n",
    "                                print(f\"   • {key}: {value}\")\n",
    "                        \n",
    "                        print(\"\\\\n🎯 Your Knowledge Base is now ready to use!\")\n",
    "                        print(\"You can now query your structured data using natural language!\")\n",
    "                        break\n",
    "                        \n",
    "                    elif status == 'FAILED':\n",
    "                        print(\"❌ INGESTION FAILED!\")\n",
    "                        if 'failureReasons' in job_details['ingestionJob']:\n",
    "                            failure_reasons = job_details['ingestionJob']['failureReasons']\n",
    "                            print(f\"💥 Failure reasons:\")\n",
    "                            for reason in failure_reasons:\n",
    "                                print(f\"   • {reason}\")\n",
    "                        \n",
    "                        # Print full job details for debugging\n",
    "                        print(\"\\\\n🔍 Full job details for debugging:\")\n",
    "                        import pprint\n",
    "                        pprint.pprint(job_details['ingestionJob'])\n",
    "                        break\n",
    "                        \n",
    "                    elif status in ['IN_PROGRESS', 'STARTED']:\n",
    "                        time.sleep(check_interval)\n",
    "                        elapsed_time += check_interval\n",
    "                    else:\n",
    "                        print(f\"🤔 Unknown status: {status}\")\n",
    "                        time.sleep(check_interval)\n",
    "                        elapsed_time += check_interval\n",
    "                else:\n",
    "                    print(\"⚠️ Could not get job details\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error checking job status: {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        if elapsed_time >= max_wait_time:\n",
    "            print(\"⏰ Timeout waiting for ingestion to complete\")\n",
    "            print(\"The job may still be running. Check the AWS Console for updates.\")\n",
    "    else:\n",
    "        print(\"❌ Failed to get ingestion job details\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error starting ingestion job: {str(e)}\")\n",
    "    print(\"\\\\n🔍 Troubleshooting suggestions:\")\n",
    "    print(\"1. Check that the Redshift workgroup is running\")\n",
    "    print(\"2. Verify IAM permissions in the AWS Console\")\n",
    "    print(\"3. Check CloudWatch logs for detailed error messages\")\n",
    "    print(\"4. Ensure the database user and permissions are correctly set up\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 8: Resource Cleanup\n",
    "\n",
    "**⚠️ IMPORTANT: Run this section only when you want to delete all the resources created in this notebook to avoid ongoing charges.**\n",
    "\n",
    "The following cells will help you clean up all the AWS resources created during this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c6627d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Cleaning up Bedrock Knowledge Base resources...\n",
      "Deleting Knowledge Base: NDFEP5C9NZ\n",
      "✅ Knowledge Base deletion initiated\n",
      "Waiting for Knowledge Base deletion... (attempt 1)\n",
      "✅ Knowledge Base successfully deleted\n",
      "Deleting Bedrock IAM role: AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0192129\n",
      "Detached policy: AmazonBedrockRedshiftPolicyForKnowledgeBase_0192129\n",
      "Detached policy: BedrockRedshiftAccessPolicy-0192129\n",
      "✅ Bedrock IAM role deleted\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Delete Bedrock Knowledge Base and Data Source\n",
    "def cleanup_bedrock_resources():\n",
    "    \"\"\"Delete Bedrock Knowledge Base and associated resources\"\"\"\n",
    "    try:\n",
    "        print(\"🗑️ Cleaning up Bedrock Knowledge Base resources...\")\n",
    "        \n",
    "        # Delete Knowledge Base (this also deletes associated data sources)\n",
    "        try:\n",
    "            print(f\"Deleting Knowledge Base: {kb_id}\")\n",
    "            bedrock_agent_client.delete_knowledge_base(knowledgeBaseId=kb_id)\n",
    "            print(\"✅ Knowledge Base deletion initiated\")\n",
    "            \n",
    "            # Wait for deletion to complete\n",
    "            max_attempts = 30\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    bedrock_agent_client.get_knowledge_base(knowledgeBaseId=kb_id)\n",
    "                    print(f\"Waiting for Knowledge Base deletion... (attempt {attempt + 1})\")\n",
    "                    time.sleep(10)\n",
    "                except bedrock_agent_client.exceptions.ResourceNotFoundException:\n",
    "                    print(\"✅ Knowledge Base successfully deleted\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"⚠️ Timeout waiting for Knowledge Base deletion, but continuing...\")\n",
    "                \n",
    "        except bedrock_agent_client.exceptions.ResourceNotFoundException:\n",
    "            print(\"Knowledge Base already deleted or doesn't exist\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error deleting Knowledge Base: {str(e)}\")\n",
    "        \n",
    "        # Delete the Bedrock execution role\n",
    "        try:\n",
    "            print(f\"Deleting Bedrock IAM role: {bedrock_role_name}\")\n",
    "            \n",
    "            # Detach policies first\n",
    "            attached_policies = iam_client.list_attached_role_policies(RoleName=bedrock_role_name)\n",
    "            for policy in attached_policies['AttachedPolicies']:\n",
    "                iam_client.detach_role_policy(\n",
    "                    RoleName=bedrock_role_name,\n",
    "                    PolicyArn=policy['PolicyArn']\n",
    "                )\n",
    "                print(f\"Detached policy: {policy['PolicyName']}\")\n",
    "            \n",
    "            # Delete inline policies\n",
    "            inline_policies = iam_client.list_role_policies(RoleName=bedrock_role_name)\n",
    "            for policy_name in inline_policies['PolicyNames']:\n",
    "                iam_client.delete_role_policy(\n",
    "                    RoleName=bedrock_role_name,\n",
    "                    PolicyName=policy_name\n",
    "                )\n",
    "                print(f\"Deleted inline policy: {policy_name}\")\n",
    "            \n",
    "            # Delete the role\n",
    "            iam_client.delete_role(RoleName=bedrock_role_name)\n",
    "            print(\"✅ Bedrock IAM role deleted\")\n",
    "            \n",
    "        except iam_client.exceptions.NoSuchEntityException:\n",
    "            print(\"Bedrock IAM role already deleted or doesn't exist\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error deleting Bedrock IAM role: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in Bedrock cleanup: {str(e)}\")\n",
    "\n",
    "# Run Bedrock cleanup\n",
    "cleanup_bedrock_resources()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd52f49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Cleaning up Redshift Serverless resources...\n",
      "Deleting Redshift workgroup: sds-ecommerce-wg-0192129\n",
      "✅ Workgroup deletion initiated\n",
      "Workgroup status: DELETING, waiting for deletion... (attempt 1)\n",
      "Workgroup status: DELETING, waiting for deletion... (attempt 2)\n",
      "Workgroup status: DELETING, waiting for deletion... (attempt 3)\n",
      "Workgroup status: DELETING, waiting for deletion... (attempt 4)\n",
      "Workgroup status: DELETING, waiting for deletion... (attempt 5)\n",
      "Workgroup status: DELETING, waiting for deletion... (attempt 6)\n",
      "Workgroup status: DELETING, waiting for deletion... (attempt 7)\n",
      "✅ Workgroup successfully deleted\n",
      "Deleting Redshift namespace: sds-ecommerce-0192129\n",
      "✅ Namespace deletion initiated\n",
      "Namespace status: DELETING, waiting for deletion... (attempt 1)\n",
      "Namespace status: DELETING, waiting for deletion... (attempt 2)\n",
      "Namespace status: DELETING, waiting for deletion... (attempt 3)\n",
      "Namespace status: DELETING, waiting for deletion... (attempt 4)\n",
      "Namespace status: DELETING, waiting for deletion... (attempt 5)\n",
      "Namespace status: DELETING, waiting for deletion... (attempt 6)\n",
      "Namespace status: DELETING, waiting for deletion... (attempt 7)\n",
      "Namespace status: DELETING, waiting for deletion... (attempt 8)\n",
      "Namespace status: DELETING, waiting for deletion... (attempt 9)\n",
      "Namespace status: DELETING, waiting for deletion... (attempt 10)\n",
      "Namespace status: DELETING, waiting for deletion... (attempt 11)\n",
      "Namespace status: DELETING, waiting for deletion... (attempt 12)\n",
      "✅ Namespace successfully deleted\n",
      "Deleting Redshift IAM role: RedshiftS3AccessRole-0192129\n",
      "Detached policy: AmazonS3ReadOnlyAccess\n",
      "✅ Redshift IAM role deleted\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Delete Redshift Serverless resources\n",
    "def cleanup_redshift_resources():\n",
    "    \"\"\"Delete Redshift Serverless workgroup and namespace\"\"\"\n",
    "    try:\n",
    "        print(\"🗑️ Cleaning up Redshift Serverless resources...\")\n",
    "        \n",
    "        # Delete workgroup first\n",
    "        try:\n",
    "            print(f\"Deleting Redshift workgroup: {REDSHIFT_WORKGROUP}\")\n",
    "            redshift_client.delete_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
    "            print(\"✅ Workgroup deletion initiated\")\n",
    "            \n",
    "            # Wait for workgroup deletion\n",
    "            max_attempts = 30\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
    "                    status = response['workgroup']['status']\n",
    "                    print(f\"Workgroup status: {status}, waiting for deletion... (attempt {attempt + 1})\")\n",
    "                    time.sleep(10)\n",
    "                except redshift_client.exceptions.ResourceNotFoundException:\n",
    "                    print(\"✅ Workgroup successfully deleted\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"⚠️ Timeout waiting for workgroup deletion, but continuing...\")\n",
    "                \n",
    "        except redshift_client.exceptions.ResourceNotFoundException:\n",
    "            print(\"Workgroup already deleted or doesn't exist\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error deleting workgroup: {str(e)}\")\n",
    "        \n",
    "        # Delete namespace\n",
    "        try:\n",
    "            print(f\"Deleting Redshift namespace: {REDSHIFT_NAMESPACE}\")\n",
    "            redshift_client.delete_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
    "            print(\"✅ Namespace deletion initiated\")\n",
    "            \n",
    "            # Wait for namespace deletion\n",
    "            max_attempts = 30\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
    "                    status = response['namespace']['status']\n",
    "                    print(f\"Namespace status: {status}, waiting for deletion... (attempt {attempt + 1})\")\n",
    "                    time.sleep(10)\n",
    "                except redshift_client.exceptions.ResourceNotFoundException:\n",
    "                    print(\"✅ Namespace successfully deleted\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"⚠️ Timeout waiting for namespace deletion, but continuing...\")\n",
    "                \n",
    "        except redshift_client.exceptions.ResourceNotFoundException:\n",
    "            print(\"Namespace already deleted or doesn't exist\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error deleting namespace: {str(e)}\")\n",
    "            \n",
    "        # Delete Redshift IAM role\n",
    "        try:\n",
    "            redshift_role_name = f'RedshiftS3AccessRole-{suffix}'\n",
    "            print(f\"Deleting Redshift IAM role: {redshift_role_name}\")\n",
    "            \n",
    "            # Detach policies first\n",
    "            attached_policies = iam_client.list_attached_role_policies(RoleName=redshift_role_name)\n",
    "            for policy in attached_policies['AttachedPolicies']:\n",
    "                iam_client.detach_role_policy(\n",
    "                    RoleName=redshift_role_name,\n",
    "                    PolicyArn=policy['PolicyArn']\n",
    "                )\n",
    "                print(f\"Detached policy: {policy['PolicyName']}\")\n",
    "            \n",
    "            # Delete the role\n",
    "            iam_client.delete_role(RoleName=redshift_role_name)\n",
    "            print(\"✅ Redshift IAM role deleted\")\n",
    "            \n",
    "        except iam_client.exceptions.NoSuchEntityException:\n",
    "            print(\"Redshift IAM role already deleted or doesn't exist\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error deleting Redshift IAM role: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in Redshift cleanup: {str(e)}\")\n",
    "\n",
    "# Run Redshift cleanup\n",
    "cleanup_redshift_resources()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9aa0c956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Cleaning up S3 resources...\n",
      "Deleting all objects in bucket: sds-ecommerce-redshift-0192129\n",
      "✅ Deleted 4 objects from bucket\n",
      "Deleting S3 bucket: sds-ecommerce-redshift-0192129\n",
      "✅ S3 bucket deleted successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Delete S3 bucket and contents\n",
    "def cleanup_s3_resources():\n",
    "    \"\"\"Delete S3 bucket and all its contents\"\"\"\n",
    "    try:\n",
    "        print(\"🗑️ Cleaning up S3 resources...\")\n",
    "        \n",
    "        # First, delete all objects in the bucket\n",
    "        try:\n",
    "            print(f\"Deleting all objects in bucket: {S3_BUCKET}\")\n",
    "            \n",
    "            # List all objects\n",
    "            paginator = s3_client.get_paginator('list_objects_v2')\n",
    "            pages = paginator.paginate(Bucket=S3_BUCKET)\n",
    "            \n",
    "            objects_deleted = 0\n",
    "            for page in pages:\n",
    "                if 'Contents' in page:\n",
    "                    objects_to_delete = [{'Key': obj['Key']} for obj in page['Contents']]\n",
    "                    if objects_to_delete:\n",
    "                        s3_client.delete_objects(\n",
    "                            Bucket=S3_BUCKET,\n",
    "                            Delete={'Objects': objects_to_delete}\n",
    "                        )\n",
    "                        objects_deleted += len(objects_to_delete)\n",
    "            \n",
    "            print(f\"✅ Deleted {objects_deleted} objects from bucket\")\n",
    "            \n",
    "            # Delete all object versions if versioning is enabled\n",
    "            try:\n",
    "                versions_paginator = s3_client.get_paginator('list_object_versions')\n",
    "                version_pages = versions_paginator.paginate(Bucket=S3_BUCKET)\n",
    "                \n",
    "                versions_deleted = 0\n",
    "                for page in version_pages:\n",
    "                    versions_to_delete = []\n",
    "                    if 'Versions' in page:\n",
    "                        versions_to_delete.extend([\n",
    "                            {'Key': version['Key'], 'VersionId': version['VersionId']}\n",
    "                            for version in page['Versions']\n",
    "                        ])\n",
    "                    if 'DeleteMarkers' in page:\n",
    "                        versions_to_delete.extend([\n",
    "                            {'Key': marker['Key'], 'VersionId': marker['VersionId']}\n",
    "                            for marker in page['DeleteMarkers']\n",
    "                        ])\n",
    "                    \n",
    "                    if versions_to_delete:\n",
    "                        s3_client.delete_objects(\n",
    "                            Bucket=S3_BUCKET,\n",
    "                            Delete={'Objects': versions_to_delete}\n",
    "                        )\n",
    "                        versions_deleted += len(versions_to_delete)\n",
    "                \n",
    "                if versions_deleted > 0:\n",
    "                    print(f\"✅ Deleted {versions_deleted} object versions from bucket\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error deleting object versions (this is normal if versioning is not enabled): {str(e)}\")\n",
    "            \n",
    "            # Now delete the bucket\n",
    "            print(f\"Deleting S3 bucket: {S3_BUCKET}\")\n",
    "            s3_client.delete_bucket(Bucket=S3_BUCKET)\n",
    "            print(\"✅ S3 bucket deleted successfully\")\n",
    "            \n",
    "        except s3_client.exceptions.NoSuchBucket:\n",
    "            print(\"S3 bucket already deleted or doesn't exist\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error deleting S3 bucket: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in S3 cleanup: {str(e)}\")\n",
    "\n",
    "# Run S3 cleanup\n",
    "cleanup_s3_resources()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702561a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 4: Clean up local files\n",
    "# def cleanup_local_files():\n",
    "#     \"\"\"Delete local utility files and directories\"\"\"\n",
    "#     try:\n",
    "#         print(\"🗑️ Cleaning up local files...\")\n",
    "        \n",
    "#         # Delete the utils directory\n",
    "#         import shutil\n",
    "#         if os.path.exists('utils'):\n",
    "#             shutil.rmtree('utils')\n",
    "#             print(\"✅ Deleted utils directory\")\n",
    "#         else:\n",
    "#             print(\"Utils directory doesn't exist\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error cleaning up local files: {str(e)}\")\n",
    "\n",
    "# # Run local cleanup\n",
    "# cleanup_local_files()\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"🎉 CLEANUP COMPLETED!\")\n",
    "# print(\"=\"*60)\n",
    "# print(\"All AWS resources have been deleted. Please verify in the AWS Console:\")\n",
    "# print(\"• Bedrock Knowledge Base - should be deleted\")\n",
    "# print(\"• Redshift Serverless namespace and workgroup - should be deleted\") \n",
    "# print(\"• S3 bucket - should be deleted\")\n",
    "# print(\"• IAM roles - should be deleted\")\n",
    "# print(\"\\n⚠️ Note: Some resources may take a few minutes to fully delete.\")\n",
    "# print(\"💰 This should stop all ongoing charges for these resources.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00549fb8",
   "metadata": {},
   "source": [
    "For the IAM Role + Redshift Serverless WorkGroup access pattern, you must configure database-level permissions for the IAM role used by Bedrock Knowledge Base.\n",
    "\n",
    "1. **Create IAM-based database user**: Map the IAM role to a database user in Redshift\n",
    "2. **Grant appropriate permissions**: Provide SELECT access to the relevant schemas and tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "260dbdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracted Role Name: AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0192129\n"
     ]
    }
   ],
   "source": [
    "# Extract the IAM role name from the ARN for database user creation\n",
    "kb_details = structured_kb.knowledge_base\n",
    "\n",
    "bedrock_role_arn = kb_details['roleArn']\n",
    "bedrock_role_name = bedrock_role_arn.split('/')[-1]\n",
    "print(f\"   Extracted Role Name: {bedrock_role_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a7e47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating user: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0192129\n",
      "Executing statement: 2a8efcc7-222c-4fa6-8924-7667eeb6b44e\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "IAM user created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the IAM user in Redshift (this is the critical missing step!)\n",
    "create_user_sql = f'CREATE USER \"IAMR:{bedrock_role_name}\" WITH PASSWORD DISABLE;'\n",
    "\n",
    "try:\n",
    "    print(f\"Creating user: IAMR:{bedrock_role_name}\")\n",
    "    run_redshift_statement(create_user_sql)\n",
    "    print(\"IAM user created successfully!\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(\"User already exists, continuing...\")\n",
    "    else:\n",
    "        print(f\"Error creating user: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'CREATE USER \"IAMR:{knowledge_base.bedrock_kb_execution_role_name}\" WITH PASSWORD DISABLE;')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53ab5fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granting USAGE on schema public to: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0192129\n",
      "Error executing statement: An error occurred (ValidationException) when calling the ExecuteStatement operation: Serverless workgroup sds-ecommerce-wg-0192129 not found. (Service: AWSRedshiftServerless; Status Code: 400; Error Code: ResourceNotFoundException; Request ID: 7a4c8a1f-fdf9-41d6-bd59-a9d8f9273f49; Proxy: null)\n",
      "Error granting permissions: An error occurred (ValidationException) when calling the ExecuteStatement operation: Serverless workgroup sds-ecommerce-wg-0192129 not found. (Service: AWSRedshiftServerless; Status Code: 400; Error Code: ResourceNotFoundException; Request ID: 7a4c8a1f-fdf9-41d6-bd59-a9d8f9273f49; Proxy: null)\n"
     ]
    },
    {
     "ename": "ValidationException",
     "evalue": "An error occurred (ValidationException) when calling the ExecuteStatement operation: Serverless workgroup sds-ecommerce-wg-0192129 not found. (Service: AWSRedshiftServerless; Status Code: 400; Error Code: ResourceNotFoundException; Request ID: 7a4c8a1f-fdf9-41d6-bd59-a9d8f9273f49; Proxy: null)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGranting USAGE on schema public to: IAMR:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbedrock_role_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mrun_redshift_statement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrant_usage_sql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSAGE permission granted successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGranting SELECT permissions to: IAMR:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbedrock_role_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m, in \u001b[0;36mrun_redshift_statement\u001b[0;34m(sql_statement)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute a SQL statement in Redshift\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mredshift_data_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_statement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mWorkgroupName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREDSHIFT_WORKGROUP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREDSHIFT_DATABASE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mSql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_statement\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     statement_id \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting statement: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatement_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages/botocore/client.py:598\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    595\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    596\u001b[0m     )\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[1;32m    122\u001b[0m     hook()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/genai-on-aws/lib/python3.12/site-packages/botocore/client.py:1061\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1059\u001b[0m     )\n\u001b[1;32m   1060\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the ExecuteStatement operation: Serverless workgroup sds-ecommerce-wg-0192129 not found. (Service: AWSRedshiftServerless; Status Code: 400; Error Code: ResourceNotFoundException; Request ID: 7a4c8a1f-fdf9-41d6-bd59-a9d8f9273f49; Proxy: null)"
     ]
    }
   ],
   "source": [
    "# Grant USAGE on schema and SELECT on all tables in public schema\n",
    "grant_usage_sql = f'GRANT USAGE ON SCHEMA public TO \"IAMR:{bedrock_role_name}\" WITH PASSWORD DISABLE;'\n",
    "grant_select_sql = f'GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"IAMR:{bedrock_role_name}\";'\n",
    "\n",
    "try:\n",
    "    print(f\"Granting USAGE on schema public to: IAMR:{bedrock_role_name}\")\n",
    "    run_redshift_statement(grant_usage_sql)\n",
    "    print(\"USAGE permission granted successfully!\")\n",
    "    \n",
    "    print(f\"Granting SELECT permissions to: IAMR:{bedrock_role_name}\")\n",
    "    run_redshift_statement(grant_select_sql)\n",
    "    print(\"SELECT permissions granted successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error granting permissions: {str(e)}\")\n",
    "    raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "552d4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper functions for querying the Knowledge Base\n",
    "\n",
    "# def query_with_retrieve_and_generate(kb_id, query):\n",
    "#     \"\"\"Query using retrieve_and_generate API - returns natural language response\"\"\"\n",
    "#     try:\n",
    "#         response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "#             input={\"text\": query},\n",
    "#             retrieveAndGenerateConfiguration={\n",
    "#                 \"type\": \"KNOWLEDGE_BASE\",\n",
    "#                 \"knowledgeBaseConfiguration\": {\n",
    "#                     'knowledgeBaseId': kb_id,\n",
    "#                     \"modelArn\": f\"arn:aws:bedrock:{region}::foundation-model/{generation_model}\",\n",
    "#                     \"retrievalConfiguration\": {\n",
    "#                         \"vectorSearchConfiguration\": {\n",
    "#                             \"numberOfResults\": 5\n",
    "#                         }\n",
    "#                     }\n",
    "#                 }\n",
    "#             }\n",
    "#         )\n",
    "#         return response['output']['text']\n",
    "#     except Exception as e:\n",
    "#         return f\"Error: {str(e)}\"\n",
    "\n",
    "# def generate_sql_query(kb_arn, query):\n",
    "#     \"\"\"Generate SQL query from natural language\"\"\"\n",
    "#     try:\n",
    "#         response = bedrock_agent_runtime_client.generate_query(\n",
    "#             queryGenerationInput={\n",
    "#                 \"text\": query,\n",
    "#                 \"type\": \"TEXT\"\n",
    "#             },\n",
    "#             transformationConfiguration={\n",
    "#                 \"mode\": \"TEXT_TO_SQL\",\n",
    "#                 \"textToSqlConfiguration\": {\n",
    "#                     \"type\": \"KNOWLEDGE_BASE\",\n",
    "#                     \"knowledgeBaseConfiguration\": {\n",
    "#                         \"knowledgeBaseArn\": kb_details['knowledgeBaseArn']\n",
    "#                     }\n",
    "#                 }\n",
    "#             }\n",
    "#         )\n",
    "        \n",
    "#         if response.get('queries') and len(response['queries']) > 0:\n",
    "#             return response['queries'][0]['sql']\n",
    "#         else:\n",
    "#             return \"No SQL generated\"\n",
    "#     except Exception as e:\n",
    "#         return f\"Error: {str(e)}\"\n",
    "\n",
    "# print(\"✅ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6ad67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test queries\n",
    "# test_queries = [\n",
    "#     \"How many orders are in the database?\",\n",
    "#     \"What is the average order total?\"\n",
    "# ]\n",
    "\n",
    "# print(\"🧪 Testing Knowledge Base with sample queries\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# for i, query in enumerate(test_queries, 1):\n",
    "#     print(f\"\\n📝 Query {i}: {query}\")\n",
    "#     print(\"-\" * 50)\n",
    "    \n",
    "#     # Get natural language response\n",
    "#     print(\"🤖 Natural Language Response:\")\n",
    "#     nl_response = query_with_retrieve_and_generate(kb_id, query)\n",
    "#     print(f\"   {nl_response}\")\n",
    "    \n",
    "#     # Get generated SQL\n",
    "#     print(\"\\n🔧 Generated SQL:\")\n",
    "#     sql_query = generate_sql_query(kb_details['knowledgeBaseArn'], query)\n",
    "#     print(f\"   {sql_query}\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3bd6df",
   "metadata": {},
   "source": [
    "## Step 7: Start Ingestion Job\n",
    "\n",
    "Now that the database permissions are properly configured, let's start the ingestion job to sync the data from the Redshift database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0097a5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job  started successfully\n",
      "\n",
      "{ 'dataSourceId': 'RAYVKWOCOQ',\n",
      "  'ingestionJobId': '54IIOZYXUG',\n",
      "  'knowledgeBaseId': 'NDFEP5C9NZ',\n",
      "  'startedAt': datetime.datetime(2025, 6, 21, 2, 23, 41, 328701, tzinfo=tzutc()),\n",
      "  'status': 'FAILED',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 21, 2, 23, 44, 589892, tzinfo=tzutc())}\n",
      ".....\r"
     ]
    }
   ],
   "source": [
    "# Wait a bit for the Knowledge Base to be fully ready\n",
    "time.sleep(20)\n",
    "structured_kb.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ba69b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-on-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
